"use strict";(self.webpackChunkwener_website=self.webpackChunkwener_website||[]).push([["60823"],{17149:function(e,n,t){t.r(n),t.d(n,{frontMatter:()=>a,toc:()=>c,default:()=>u,metadata:()=>s,assets:()=>l,contentTitle:()=>i});var s=JSON.parse('{"id":"db/column/clickhouse/clickhouse-conf","title":"Clickhouse \u914D\u7F6E","description":"- /etc/clickhouse-server/config.xml","source":"@site/../notes/db/column/clickhouse/clickhouse-conf.md","sourceDirName":"db/column/clickhouse","slug":"/db/column/clickhouse/conf","permalink":"/notes/db/column/clickhouse/conf","draft":false,"unlisted":false,"editUrl":"https://github.com/wenerme/wener/edit/master/notes/../notes/db/column/clickhouse/clickhouse-conf.md","tags":[{"inline":true,"label":"Configuration","permalink":"/notes/tags/configuration"}],"version":"current","lastUpdatedBy":"wener","lastUpdatedAt":1686920980000,"frontMatter":{"tags":["Configuration"]},"sidebar":"docs","previous":{"title":"ClickHouse","permalink":"/notes/db/column/clickhouse/"},"next":{"title":"ClickHouse FAQ","permalink":"/notes/db/column/clickhouse/faq"}}'),r=t(86106),o=t(17776);let a={tags:["Configuration"]},i="Clickhouse \u914D\u7F6E",l={},c=[{value:"users.yaml",id:"usersyaml",level:2},{value:"REST API",id:"rest-api",level:2},{value:"Query",id:"query",level:2},{value:"docker_related_config.xml",id:"docker_related_configxml",level:2}];function d(e){let n={a:"a",code:"code",h1:"h1",h2:"h2",header:"header",li:"li",pre:"pre",ul:"ul",...(0,o.R)(),...e.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(n.header,{children:(0,r.jsx)(n.h1,{id:"clickhouse-\u914D\u7F6E",children:"Clickhouse \u914D\u7F6E"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"/etc/clickhouse-server/config.xml"}),"\n",(0,r.jsx)(n.li,{children:"/etc/clickhouse-server/users.xml"}),"\n",(0,r.jsx)(n.li,{children:"/etc/clickhouse-server/config.d"}),"\n",(0,r.jsxs)(n.li,{children:["/etc/clickhouse-server/users.d\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"XML, YAML"}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\u53C2\u8003\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:(0,r.jsx)(n.a,{href:"https://clickhouse.com/docs/en/operations/configuration-files/",children:"https://clickhouse.com/docs/en/operations/configuration-files/"})}),"\n",(0,r.jsx)(n.li,{children:(0,r.jsx)(n.a,{href:"https://github.com/ClickHouse/ClickHouse/blob/master/programs/server/config.yaml.example",children:"https://github.com/ClickHouse/ClickHouse/blob/master/programs/server/config.yaml.example"})}),"\n",(0,r.jsx)(n.li,{children:(0,r.jsx)(n.a,{href:"https://github.com/ClickHouse/ClickHouse/blob/master/programs/server/config.xml",children:"https://github.com/ClickHouse/ClickHouse/blob/master/programs/server/config.xml"})}),"\n",(0,r.jsx)(n.li,{children:(0,r.jsx)(n.a,{href:"https://github.com/ClickHouse/ClickHouse/blob/master/programs/server/users.yaml.example",children:"https://github.com/ClickHouse/ClickHouse/blob/master/programs/server/users.yaml.example"})}),"\n",(0,r.jsx)(n.li,{children:(0,r.jsx)(n.a,{href:"https://github.com/ClickHouse/ClickHouse/blob/master/programs/server/users.xml",children:"https://github.com/ClickHouse/ClickHouse/blob/master/programs/server/users.xml"})}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"# clickhouse-benchmark\n# \u4F1A\u547D\u4EE4\u884C\u542F\u52A8 clickhouse server\ncurl -LO https://raw.githubusercontent.com/ClickHouse/ClickBench/main/hardware/hardware.sh\nchmod a+x hardware.sh\n./hardware.sh\n"})}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-yaml",children:"# skip_check_for_incorrect_settings: 1 here.\n\n# users \u91CC\u53EF\u6839\u636E\u7528\u6237\u914D\u7F6E\n# logger \u5B9E\u73B0 https://github.com/pocoproject/poco/blob/poco-1.9.4-release/Foundation/include/Poco/Logger.h#L105-L114\nlogger:\n  # none fatal critical error warning notice information debug trace\n  level: trace\n  log: /var/log/clickhouse-server/clickhouse-server.log\n  errorlog: /var/log/clickhouse-server/clickhouse-server.err.log\n  # Rotation policy\n  # See https://github.com/pocoproject/poco/blob/poco-1.9.4-release/Foundation/include/Poco/FileChannel.h#L54-L85\n  size: 1000M\n  count: 10\n  # console: 1 # \u9ED8\u8BA4\u81EA\u52A8\u68C0\u6D4B\n\n  # Per level overrides (legacy):\n  # For example to suppress logging of the ConfigReloader you can use:\n  # NOTE: levels.logger is reserved, see below.\n  # levels:\n  #     ConfigReloader: none\n\n  # Per level overrides:\n  # For example to suppress logging of the RBAC for default user you can use:\n  # (But please note that the logger name maybe changed from version to version, even after minor upgrade)\n  # levels:\n  #     - logger:\n  #         name: 'ContextAccess (default)'\n  #         level: none\n  #     - logger:\n  #         name: 'DatabaseOrdinary (test)'\n  #         level: none\n\n# It is the name that will be shown in the clickhouse-client.\n# By default, anything with \"production\" will be highlighted in red in query prompt.\n# display_name: production\n\nhttp_port: 8123\n# https_port: 8443\n\n# native protocol\n# - clickhouse-client  - clickhouse-benchmark, clickhouse-copier\n# - clickhouse-server\n# - native protocol driver\ntcp_port: 9000\n# tcp_port_secure: 9440\n\nmysql_port: 9004\npostgresql_port: 9005\n\n# PROXYv1\n# tcp_with_proxy_port: 9011\n\n# \u5185\u90E8 replicas \u901A\u8BAF - data exchange\n# \u907F\u514D\u4E0D\u4FE1\u4EFB\u5BA2\u6237\u7AEF\u8BBF\u95EE\ninterserver_http_port: 9009\n# interserver_https_port: 9010\n# \u9ED8\u8BA4 hostname -f\n# interserver_http_host: example.clickhouse.com\n# avoid SSRF attacks\n# interserver_http_credentials:\n#     user: interserver\n#     password: ''\n\n# \u76D1\u542C\u5730\u5740\n# listen_host: '::'\n# listen_host: 0.0.0.0\n\n# \u76D1\u542C\u672C\u5730\n# listen_host: '::1'\n# listen_host: 127.0.0.1\n\n# listen_try: 0\n# \u76D1\u542C\u76F8\u540C\u7AEF\u53E3 - \u4E0D\u63A8\u8350\n# listen_reuse_port: 0\n# listen_backlog: 64\nmax_connections: 4096\n\n# HTTP 1.1 'Connection: keep-alive'\nkeep_alive_timeout: 3\n\n# src/Server/grpc_protos/clickhouse_grpc.proto\n# grpc_port: 9100\ngrpc:\n  enable_ssl: false\n\n  # The following two files are used only if enable_ssl=1\n  ssl_cert_file: /path/to/ssl_cert_file\n  ssl_key_file: /path/to/ssl_key_file\n\n  # Whether server will request client for a certificate\n  ssl_require_client_auth: false\n\n  # The following file is used only if ssl_require_client_auth=1\n  ssl_ca_cert_file: /path/to/ssl_ca_cert_file\n\n  # Default compression algorithm (applied if client doesn't specify another algorithm).\n  # Supported algorithms: none, deflate, gzip, stream_gzip\n  compression: deflate\n\n  # Default compression level (applied if client doesn't specify another level).\n  # Supported levels: none, low, medium, high\n  compression_level: medium\n\n  # Send/receive message size limits in bytes. -1 means unlimited\n  max_send_message_size: -1\n  max_receive_message_size: -1\n\n  # Enable if you want very detailed logs\n  verbose_logs: false\n\n# \u6240\u6709\u9009\u9879 https://github.com/ClickHouse-Extras/poco/blob/master/NetSSL_OpenSSL/include/Poco/Net/SSLManager.h#L71\nopenSSL:\n  server:\n    # Used for https server AND secure tcp port\n    # openssl req -subj \"/CN=localhost\" -new -newkey rsa:2048 -days 365 -nodes -x509 -keyout /etc/clickhouse-server/server.key -out /etc/clickhouse-server/server.crt\n    # certificateFile: /etc/clickhouse-server/server.crt\n    # privateKeyFile: /etc/clickhouse-server/server.key\n\n    # dhparams are optional. You can delete the dhParamsFile: element.\n    # To generate dhparams, use the following command:\n    # openssl dhparam -out /etc/clickhouse-server/dhparam.pem 4096\n    # Only file format with BEGIN DH PARAMETERS is supported.\n    dhParamsFile: /etc/clickhouse-server/dhparam.pem\n    verificationMode: none\n    loadDefaultCAFile: true\n    cacheSessions: true\n    disableProtocols: 'sslv2,sslv3'\n    preferServerCiphers: true\n  client:\n    # Used for connecting to https dictionary source and secured Zookeeper communication\n    loadDefaultCAFile: true\n    cacheSessions: true\n    disableProtocols: 'sslv2,sslv3'\n    preferServerCiphers: true\n\n    # Use for self-signed: verificationMode: none\n    invalidCertificateHandler:\n      # Use for self-signed: name: AcceptCertificateHandler\n      name: RejectCertificateHandler\n\n# https://github.com/tabixio/tabix\n# \u8BF7\u6C42 http://localhost:8123 \u65F6\u7684\u9ED8\u8BA4\u54CD\u5E94\n# http_server_default_response: |-\n#     <html ng-app=\"SMI2\"><head><base href=\"http://ui.tabix.io/\"></head><body><div ui-view=\"\" class=\"content-ui\"></div><script src=\"http://loader.tabix.io/master.js\"><\/script></body></html>\n\n# Maximum number of concurrent queries.\nmax_concurrent_queries: 100\n\n# Maximum memory usage (resident set size) for server process.\n# Zero value or unset means default. Default is \"max_server_memory_usage_to_ram_ratio\" of available physical RAM.\n# If the value is larger than \"max_server_memory_usage_to_ram_ratio\" of available physical RAM, it will be cut down.\n\n# The constraint is checked on query execution time.\n# If a query tries to allocate memory and the current memory usage plus allocation is greater\n# than specified threshold, exception will be thrown.\n\n# It is not practical to set this constraint to small values like just a few gigabytes,\n# because memory allocator will keep this amount of memory in caches and the server will deny service of queries.\nmax_server_memory_usage: 0\n\n# Maximum number of threads in the Global thread pool.\n# This will default to a maximum of 10000 threads if not specified.\n# This setting will be useful in scenarios where there are a large number\n# of distributed queries that are running concurrently but are idling most\n# of the time, in which case a higher number of threads might be required.\nmax_thread_pool_size: 10000\n\n# On memory constrained environments you may have to set this to value larger than 1.\nmax_server_memory_usage_to_ram_ratio: 0.9\n\n# Simple server-wide memory profiler. Collect a stack trace at every peak allocation step (in bytes).\n# Data will be stored in system.trace_log table with query_id = empty string.\n# Zero means disabled.\ntotal_memory_profiler_step: 4194304\n\n# Collect random allocations and deallocations and write them into system.trace_log with 'MemorySample' trace_type.\n# The probability is for every alloc/free regardless to the size of the allocation.\n# Note that sampling happens only when the amount of untracked memory exceeds the untracked memory limit,\n# which is 4 MiB by default but can be lowered if 'total_memory_profiler_step' is lowered.\n# You may want to set 'total_memory_profiler_step' to 1 for extra fine grained sampling.\ntotal_memory_tracker_sample_probability: 0\n\n# Set limit on number of open files (default: maximum). This setting makes sense on Mac OS X because getrlimit() fails to retrieve\n# correct maximum value.\n# max_open_files: 262144\n\n# Size of cache of uncompressed blocks of data, used in tables of MergeTree family.\n# In bytes. Cache is single for server. Memory is allocated only on demand.\n# Cache is used when 'use_uncompressed_cache' user setting turned on (off by default).\n# Uncompressed cache is advantageous only for very short queries and in rare cases.\n\n# Note: uncompressed cache can be pointless for lz4, because memory bandwidth\n# is slower than multi-core decompression on some server configurations.\n# Enabling it can sometimes paradoxically make queries slower.\nuncompressed_cache_size: 8589934592\n\n# Approximate size of mark cache, used in tables of MergeTree family.\n# In bytes. Cache is single for server. Memory is allocated only on demand.\n# You should not lower this value.\nmark_cache_size: 5368709120\n\n# If you enable the `min_bytes_to_use_mmap_io` setting,\n# the data in MergeTree tables can be read with mmap to avoid copying from kernel to userspace.\n# It makes sense only for large files and helps only if data reside in page cache.\n# To avoid frequent open/mmap/munmap/close calls (which are very expensive due to consequent page faults)\n# and to reuse mappings from several threads and queries,\n# the cache of mapped files is maintained. Its size is the number of mapped regions (usually equal to the number of mapped files).\n# The amount of data in mapped files can be monitored\n# in system.metrics, system.metric_log by the MMappedFiles, MMappedFileBytes metrics\n# and in system.asynchronous_metrics, system.asynchronous_metrics_log by the MMapCacheCells metric,\n# and also in system.events, system.processes, system.query_log, system.query_thread_log, system.query_views_log by the\n# CreatedReadBufferMMap, CreatedReadBufferMMapFailed, MMappedFileCacheHits, MMappedFileCacheMisses events.\n# Note that the amount of data in mapped files does not consume memory directly and is not accounted\n# in query or server memory usage - because this memory can be discarded similar to OS page cache.\n# The cache is dropped (the files are closed) automatically on removal of old parts in MergeTree,\n# also it can be dropped manually by the SYSTEM DROP MMAP CACHE query.\nmmap_cache_size: 1000\n\n# Cache size in bytes for compiled expressions.\ncompiled_expression_cache_size: 134217728\n\n# Cache size in elements for compiled expressions.\ncompiled_expression_cache_elements_size: 10000\n\n# \u6570\u636E\u76EE\u5F55\npath: /var/lib/clickhouse/\n# \u4E34\u65F6\u6570\u636E\u76EE\u5F55\ntmp_path: /var/lib/clickhouse/tmp/\n\n# Policy from the <storage_configuration> for the temporary files.\n# If not set <tmp_path> is used, otherwise <tmp_path> is ignored.\n\n# Notes:\n# - move_factor              is ignored\n# - keep_free_space_bytes    is ignored\n# - max_data_part_size_bytes is ignored\n# - you must have exactly one volume in that policy\n# tmp_policy: tmp\n\n# \u7528\u6237\u6587\u4EF6\u76EE\u5F55 - file \u51FD\u6570, SQLIte \u8BBF\u95EE\u7B49\nuser_files_path: /var/lib/clickhouse/user_files/\n\n# LDAP server definitions.\nldap_servers: ''\n\n# List LDAP servers with their connection parameters here to later 1) use them as authenticators for dedicated local users,\n# who have 'ldap' authentication mechanism specified instead of 'password', or to 2) use them as remote user directories.\n# Parameters:\n# host - LDAP server hostname or IP, this parameter is mandatory and cannot be empty.\n# port - LDAP server port, default is 636 if enable_tls is set to true, 389 otherwise.\n# bind_dn - template used to construct the DN to bind to.\n# The resulting DN will be constructed by replacing all '{user_name}' substrings of the template with the actual\n# user name during each authentication attempt.\n# user_dn_detection - section with LDAP search parameters for detecting the actual user DN of the bound user.\n# This is mainly used in search filters for further role mapping when the server is Active Directory. The\n# resulting user DN will be used when replacing '{user_dn}' substrings wherever they are allowed. By default,\n# user DN is set equal to bind DN, but once search is performed, it will be updated with to the actual detected\n# user DN value.\n# base_dn - template used to construct the base DN for the LDAP search.\n# The resulting DN will be constructed by replacing all '{user_name}' and '{bind_dn}' substrings\n# of the template with the actual user name and bind DN during the LDAP search.\n# scope - scope of the LDAP search.\n# Accepted values are: 'base', 'one_level', 'children', 'subtree' (the default).\n# search_filter - template used to construct the search filter for the LDAP search.\n# The resulting filter will be constructed by replacing all '{user_name}', '{bind_dn}', and '{base_dn}'\n# substrings of the template with the actual user name, bind DN, and base DN during the LDAP search.\n# Note, that the special characters must be escaped properly in XML.\n# verification_cooldown - a period of time, in seconds, after a successful bind attempt, during which a user will be assumed\n# to be successfully authenticated for all consecutive requests without contacting the LDAP server.\n# Specify 0 (the default) to disable caching and force contacting the LDAP server for each authentication request.\n# enable_tls - flag to trigger use of secure connection to the LDAP server.\n# Specify 'no' for plain text (ldap://) protocol (not recommended).\n# Specify 'yes' for LDAP over SSL/TLS (ldaps://) protocol (recommended, the default).\n# Specify 'starttls' for legacy StartTLS protocol (plain text (ldap://) protocol, upgraded to TLS).\n# tls_minimum_protocol_version - the minimum protocol version of SSL/TLS.\n# Accepted values are: 'ssl2', 'ssl3', 'tls1.0', 'tls1.1', 'tls1.2' (the default).\n# tls_require_cert - SSL/TLS peer certificate verification behavior.\n# Accepted values are: 'never', 'allow', 'try', 'demand' (the default).\n# tls_cert_file - path to certificate file.\n# tls_key_file - path to certificate key file.\n# tls_ca_cert_file - path to CA certificate file.\n# tls_ca_cert_dir - path to the directory containing CA certificates.\n# tls_cipher_suite - allowed cipher suite (in OpenSSL notation).\n# Example:\n# my_ldap_server:\n#     host: localhost\n#     port: 636\n#     bind_dn: 'uid={user_name},ou=users,dc=example,dc=com'\n#     verification_cooldown: 300\n#     enable_tls: yes\n#     tls_minimum_protocol_version: tls1.2\n#     tls_require_cert: demand\n#     tls_cert_file: /path/to/tls_cert_file\n#     tls_key_file: /path/to/tls_key_file\n#     tls_ca_cert_file: /path/to/tls_ca_cert_file\n#     tls_ca_cert_dir: /path/to/tls_ca_cert_dir\n#     tls_cipher_suite: ECDHE-ECDSA-AES256-GCM-SHA384:ECDHE-RSA-AES256-GCM-SHA384:AES256-GCM-SHA384\n\n# Example (typical Active Directory with configured user DN detection for further role mapping):\n# my_ad_server:\n#     host: localhost\n#     port: 389\n#     bind_dn: 'EXAMPLE\\{user_name}'\n#     user_dn_detection:\n#         base_dn: CN=Users,DC=example,DC=com\n#         search_filter: '(&amp;(objectClass=user)(sAMAccountName={user_name}))'\n#     enable_tls: no\n\n# To enable Kerberos authentication support for HTTP requests (GSS-SPNEGO), for those users who are explicitly configured\n# to authenticate via Kerberos, define a single 'kerberos' section here.\n# Parameters:\n# principal - canonical service principal name, that will be acquired and used when accepting security contexts.\n# This parameter is optional, if omitted, the default principal will be used.\n# This parameter cannot be specified together with 'realm' parameter.\n# realm - a realm, that will be used to restrict authentication to only those requests whose initiator's realm matches it.\n# This parameter is optional, if omitted, no additional filtering by realm will be applied.\n# This parameter cannot be specified together with 'principal' parameter.\n# Example:\n# kerberos: ''\n\n# Example:\n# kerberos:\n#     principal: HTTP/clickhouse.example.com@EXAMPLE.COM\n\n# Example:\n# kerberos:\n#     realm: EXAMPLE.COM\n\n# Sources to read users, roles, access rights, profiles of settings, quotas.\nuser_directories:\n  users_xml:\n    # Path to configuration file with predefined users.\n    path: users.yaml\n  local_directory:\n    # Path to folder where users created by SQL commands are stored.\n    path: /var/lib/clickhouse/access/\n\n#   # To add an LDAP server as a remote user directory of users that are not defined locally, define a single 'ldap' section\n#   # with the following parameters:\n#   # server - one of LDAP server names defined in 'ldap_servers' config section above.\n#   # This parameter is mandatory and cannot be empty.\n#   # roles - section with a list of locally defined roles that will be assigned to each user retrieved from the LDAP server.\n#   # If no roles are specified here or assigned during role mapping (below), user will not be able to perform any\n#   # actions after authentication.\n#   # role_mapping - section with LDAP search parameters and mapping rules.\n#   # When a user authenticates, while still bound to LDAP, an LDAP search is performed using search_filter and the\n#   # name of the logged in user. For each entry found during that search, the value of the specified attribute is\n#   # extracted. For each attribute value that has the specified prefix, the prefix is removed, and the rest of the\n#   # value becomes the name of a local role defined in ClickHouse, which is expected to be created beforehand by\n#   # CREATE ROLE command.\n#   # There can be multiple 'role_mapping' sections defined inside the same 'ldap' section. All of them will be\n#   # applied.\n#   # base_dn - template used to construct the base DN for the LDAP search.\n#   # The resulting DN will be constructed by replacing all '{user_name}', '{bind_dn}', and '{user_dn}'\n#   # substrings of the template with the actual user name, bind DN, and user DN during each LDAP search.\n#   # scope - scope of the LDAP search.\n#   # Accepted values are: 'base', 'one_level', 'children', 'subtree' (the default).\n#   # search_filter - template used to construct the search filter for the LDAP search.\n#   # The resulting filter will be constructed by replacing all '{user_name}', '{bind_dn}', '{user_dn}', and\n#   # '{base_dn}' substrings of the template with the actual user name, bind DN, user DN, and base DN during\n#   # each LDAP search.\n#   # Note, that the special characters must be escaped properly in XML.\n#   # attribute - attribute name whose values will be returned by the LDAP search. 'cn', by default.\n#   # prefix - prefix, that will be expected to be in front of each string in the original list of strings returned by\n#   # the LDAP search. Prefix will be removed from the original strings and resulting strings will be treated\n#   # as local role names. Empty, by default.\n#   # Example:\n#   # ldap:\n#   #     server: my_ldap_server\n#   #     roles:\n#   #         my_local_role1: ''\n#   #         my_local_role2: ''\n#   #     role_mapping:\n#   #         base_dn: 'ou=groups,dc=example,dc=com'\n#   #         scope: subtree\n#   #         search_filter: '(&amp;(objectClass=groupOfNames)(member={bind_dn}))'\n#   #         attribute: cn\n#   #         prefix: clickhouse_\n#   # Example (typical Active Directory with role mapping that relies on the detected user DN):\n#   # ldap:\n#   #     server: my_ad_server\n#   #     role_mapping:\n#   #         base_dn: 'CN=Users,DC=example,DC=com'\n#   #         attribute: CN\n#   #         scope: subtree\n#   #         search_filter: '(&amp;(objectClass=group)(member={user_dn}))'\n#   #         prefix: clickhouse_\n\n# \u9ED8\u8BA4\u914D\u7F6E\ndefault_profile: default\n\n# Comma-separated list of prefixes for user-defined settings.\n# custom_settings_prefixes: ''\n# system_profile: default\n# buffer_profile: default\n\n# \u9ED8\u8BA4\u6570\u636E\u5E93\ndefault_database: default\n\n# Server time zone could be set here.\n\n# Time zone is used when converting between String and DateTime types,\n# when printing DateTime in text formats and parsing DateTime from text,\n# it is used in date and time related functions, if specific time zone was not passed as an argument.\n\n# Time zone is specified as identifier from IANA time zone database, like UTC or Africa/Abidjan.\n# If not specified, system time zone at server startup is used.\n\n# Please note, that server could display time zone alias instead of specified name.\n# Example: Zulu is an alias for UTC.\n# timezone: UTC\n\n# You can specify umask here (see \"man umask\"). Server will apply it on startup.\n# Number is always parsed as octal. Default umask is 027 (other users cannot read logs, data files, etc; group can only read).\n# umask: 022\n\n# Perform mlockall after startup to lower first queries latency\n# and to prevent clickhouse executable from being paged out under high IO load.\n# Enabling this option is recommended but will lead to increased startup time for up to a few seconds.\nmlock_executable: true\n\n# Reallocate memory for machine code (\"text\") using huge pages. Highly experimental.\nremap_executable: false\n\n# Uncomment below in order to use JDBC table engine and function.\n# To install and run JDBC bridge in background:\n# * [Debian/Ubuntu]\n# export MVN_URL=https://repo1.maven.org/maven2/ru/yandex/clickhouse/clickhouse-jdbc-bridge\n# export PKG_VER=$(curl -sL $MVN_URL/maven-metadata.xml | grep '<release>' | sed -e 's|.*>\\(.*\\)<.*|\\1|')\n# wget https://github.com/ClickHouse/clickhouse-jdbc-bridge/releases/download/v$PKG_VER/clickhouse-jdbc-bridge_$PKG_VER-1_all.deb\n# apt install --no-install-recommends -f ./clickhouse-jdbc-bridge_$PKG_VER-1_all.deb\n# clickhouse-jdbc-bridge &\n# * [CentOS/RHEL]\n# export MVN_URL=https://repo1.maven.org/maven2/ru/yandex/clickhouse/clickhouse-jdbc-bridge\n# export PKG_VER=$(curl -sL $MVN_URL/maven-metadata.xml | grep '<release>' | sed -e 's|.*>\\(.*\\)<.*|\\1|')\n# wget https://github.com/ClickHouse/clickhouse-jdbc-bridge/releases/download/v$PKG_VER/clickhouse-jdbc-bridge-$PKG_VER-1.noarch.rpm\n# yum localinstall -y clickhouse-jdbc-bridge-$PKG_VER-1.noarch.rpm\n# clickhouse-jdbc-bridge &\n# Please refer to https://github.com/ClickHouse/clickhouse-jdbc-bridge#usage for more information.\n\n# jdbc_bridge:\n#     host: 127.0.0.1\n#     port: 9019\n\n# Configuration of clusters that could be used in Distributed tables.\n# https://clickhouse.com/docs/en/operations/table_engines/distributed/\nremote_servers:\n  # Test only shard config for testing distributed storage\n  test_shard_localhost:\n    # Inter-server per-cluster secret for Distributed queries\n    # default: no secret (no authentication will be performed)\n\n    # If set, then Distributed queries will be validated on shards, so at least:\n    # - such cluster should exist on the shard,\n    # - such cluster should have the same secret.\n\n    # And also (and which is more important), the initial_user will\n    # be used as current user for the query.\n\n    # Right now the protocol is pretty simple and it only takes into account:\n    # - cluster name\n    # - query\n\n    # Also it will be nice if the following will be implemented:\n    # - source hostname (see interserver_http_host), but then it will depends from DNS,\n    # it can use IP address instead, but then the you need to get correct on the initiator node.\n    # - target hostname / ip address (same notes as for source hostname)\n    # - time-based security tokens\n    # secret: ''\n    shard:\n      # Optional. Whether to write data to just one of the replicas. Default: false (write data to all replicas).\n      # internal_replication: false\n      # Optional. Shard weight when writing data. Default: 1.\n      # weight: 1\n      replica:\n        host: localhost\n        port: 9000\n        # Optional. Priority of the replica for load_balancing. Default: 1 (less value has more priority).\n        # priority: 1\n  test_cluster_two_shards_localhost:\n    shard:\n      - replica:\n          host: localhost\n          port: 9000\n      - replica:\n          host: localhost\n          port: 9000\n  test_cluster_two_shards:\n    shard:\n      - replica:\n          host: 127.0.0.1\n          port: 9000\n      - replica:\n          host: 127.0.0.2\n          port: 9000\n  test_cluster_two_shards_internal_replication:\n    shard:\n      - internal_replication: true\n        replica:\n          host: 127.0.0.1\n          port: 9000\n      - internal_replication: true\n        replica:\n          host: 127.0.0.2\n          port: 9000\n  test_shard_localhost_secure:\n    shard:\n      replica:\n        host: localhost\n        port: 9440\n        secure: 1\n  test_unavailable_shard:\n    shard:\n      - replica:\n          host: localhost\n          port: 9000\n      - replica:\n          host: localhost\n          port: 1\n\n# The list of hosts allowed to use in URL-related storage engines and table functions.\n# If this section is not present in configuration, all hosts are allowed.\n# remote_url_allow_hosts:\n\n# Host should be specified exactly as in URL. The name is checked before DNS resolution.\n# Example: \"clickhouse.com\", \"clickhouse.com.\" and \"www.clickhouse.com\" are different hosts.\n# If port is explicitly specified in URL, the host:port is checked as a whole.\n# If host specified here without port, any port with this host allowed.\n# \"clickhouse.com\" -> \"clickhouse.com:443\", \"clickhouse.com:80\" etc. is allowed, but \"clickhouse.com:80\" -> only \"clickhouse.com:80\" is allowed.\n# If the host is specified as IP address, it is checked as specified in URL. Example: \"[2a02:6b8:a::a]\".\n# If there are redirects and support for redirects is enabled, every redirect (the Location field) is checked.\n\n# Regular expression can be specified. RE2 engine is used for regexps.\n# Regexps are not aligned: don't forget to add ^ and $. Also don't forget to escape dot (.) metacharacter\n# (forgetting to do so is a common source of error).\n\n# If element has 'incl' attribute, then for it's value will be used corresponding substitution from another file.\n# By default, path to file with substitutions is /etc/metrika.xml. It could be changed in config in 'include_from' element.\n# Values for substitutions are specified in /clickhouse/name_of_substitution elements in that file.\n\n# ZooKeeper is used to store metadata about replicas, when using Replicated tables.\n# Optional. If you don't use replicated tables, you could omit that.\n# See https://clickhouse.com/docs/en/engines/table-engines/mergetree-family/replication/\n\n# zookeeper:\n#     - node:\n#         host: example1\n#         port: 2181\n#     - node:\n#         host: example2\n#         port: 2181\n#     - node:\n#         host: example3\n#         port: 2181\n\n# Substitutions for parameters of replicated tables.\n# Optional. If you don't use replicated tables, you could omit that.\n# See https://clickhouse.com/docs/en/engines/table-engines/mergetree-family/replication/#creating-replicated-tables\n# macros:\n#     shard: 01\n#     replica: example01-01-1\n\n# Reloading interval for embedded dictionaries, in seconds. Default: 3600.\nbuiltin_dictionaries_reload_interval: 3600\n\n# Maximum session timeout, in seconds. Default: 3600.\nmax_session_timeout: 3600\n\n# Default session timeout, in seconds. Default: 60.\ndefault_session_timeout: 60\n\n# Sending data to Graphite for monitoring. Several sections can be defined.\n# interval - send every X second\n# root_path - prefix for keys\n# hostname_in_path - append hostname to root_path (default = true)\n# metrics - send data from table system.metrics\n# events - send data from table system.events\n# asynchronous_metrics - send data from table system.asynchronous_metrics\n\n# graphite:\n#     host: localhost\n#     port: 42000\n#     timeout: 0.1\n#     interval: 60\n#     root_path: one_min\n#     hostname_in_path: true\n\n#     metrics: true\n#     events: true\n#     events_cumulative: false\n#     asynchronous_metrics: true\n\n# graphite:\n#     host: localhost\n#     port: 42000\n#     timeout: 0.1\n#     interval: 1\n#     root_path: one_sec\n\n#     metrics: true\n#     events: true\n#     events_cumulative: false\n#     asynchronous_metrics: false\n\n# Serve endpoint for Prometheus monitoring.\n# endpoint - mertics path (relative to root, statring with \"/\")\n# port - port to setup server. If not defined or 0 than http_port used\n# metrics - send data from table system.metrics\n# events - send data from table system.events\n# asynchronous_metrics - send data from table system.asynchronous_metrics\n# status_info - send data from different component from CH, ex: Dictionaries status\n\nprometheus:\n  endpoint: /metrics\n  port: 9363\n\n  metrics: true\n  events: true\n  asynchronous_metrics: true\n  status_info: true\n\n# \u67E5\u8BE2\u65E5\u5FD7\u914D\u7F6E - \u9700\u8981 log_queries = 1\nquery_log:\n  database: system\n  table: query_log\n\n  # PARTITION BY expr: https://clickhouse.com/docs/en/table_engines/mergetree-family/custom_partitioning_key/\n  # Example:\n  # event_date\n  # toMonday(event_date)\n  # toYYYYMM(event_date)\n  # toStartOfHour(event_time)\n  partition_by: toYYYYMM(event_date)\n\n  # Table TTL specification: https://clickhouse.com/docs/en/engines/table-engines/mergetree-family/mergetree/#mergetree-table-ttl\n  # Example:\n  # event_date + INTERVAL 1 WEEK\n  # event_date + INTERVAL 7 DAY DELETE\n  # event_date + INTERVAL 2 WEEK TO DISK 'bbb'\n\n  # ttl: 'event_date + INTERVAL 30 DAY DELETE'\n\n  # Instead of partition_by, you can provide full engine expression (starting with ENGINE = ) with parameters,\n  # Example: engine: 'ENGINE = MergeTree PARTITION BY toYYYYMM(event_date) ORDER BY (event_date, event_time) SETTINGS index_granularity = 1024'\n\n  # Interval of flushing data.\n  flush_interval_milliseconds: 7500\n\n# Trace log. Stores stack traces collected by query profilers.\n# See query_profiler_real_time_period_ns and query_profiler_cpu_time_period_ns settings.\ntrace_log:\n  database: system\n  table: trace_log\n  partition_by: toYYYYMM(event_date)\n  flush_interval_milliseconds: 7500\n\n# Query thread log. Has information about all threads participated in query execution.\n# Used only for queries with setting log_query_threads = 1.\nquery_thread_log:\n  database: system\n  table: query_thread_log\n  partition_by: toYYYYMM(event_date)\n  flush_interval_milliseconds: 7500\n\n# Query views log. Has information about all dependent views associated with a query.\n# Used only for queries with setting log_query_views = 1.\nquery_views_log:\n  database: system\n  table: query_views_log\n  partition_by: toYYYYMM(event_date)\n  flush_interval_milliseconds: 7500\n\n# Uncomment if use part log.\n# Part log contains information about all actions with parts in MergeTree tables (creation, deletion, merges, downloads).\npart_log:\n  database: system\n  table: part_log\n  partition_by: toYYYYMM(event_date)\n  flush_interval_milliseconds: 7500\n\n# Uncomment to write text log into table.\n# Text log contains all information from usual server log but stores it in structured and efficient way.\n# The level of the messages that goes to the table can be limited (<level>), if not specified all messages will go to the table.\n# text_log:\n#     database: system\n#     table: text_log\n#     flush_interval_milliseconds: 7500\n#     level: ''\n\n# Metric log contains rows with current values of ProfileEvents, CurrentMetrics collected with \"collect_interval_milliseconds\" interval.\nmetric_log:\n  database: system\n  table: metric_log\n  flush_interval_milliseconds: 7500\n  collect_interval_milliseconds: 1000\n\n# Asynchronous metric log contains values of metrics from\n# system.asynchronous_metrics.\nasynchronous_metric_log:\n  database: system\n  table: asynchronous_metric_log\n\n  # Asynchronous metrics are updated once a minute, so there is\n  # no need to flush more often.\n  flush_interval_milliseconds: 60000\n\n# OpenTelemetry log contains OpenTelemetry trace spans.\nopentelemetry_span_log:\n  # The default table creation code is insufficient, this <engine> spec\n  # is a workaround. There is no 'event_time' for this log, but two times,\n  # start and finish. It is sorted by finish time, to avoid inserting\n  # data too far away in the past (probably we can sometimes insert a span\n  # that is seconds earlier than the last span in the table, due to a race\n  # between several spans inserted in parallel). This gives the spans a\n  # global order that we can use to e.g. retry insertion into some external\n  # system.\n  engine: |-\n    engine MergeTree\n         partition by toYYYYMM(finish_date)\n         order by (finish_date, finish_time_us, trace_id)\n  database: system\n  table: opentelemetry_span_log\n  flush_interval_milliseconds: 7500\n\n# Crash log. Stores stack traces for fatal errors.\n# This table is normally empty.\ncrash_log:\n  database: system\n  table: crash_log\n  partition_by: ''\n  flush_interval_milliseconds: 1000\n\n# top_level_domains_path: /var/lib/clickhouse/top_level_domains/\n# Custom TLD lists.\n# Format: name: /path/to/file\n\n# Changes will not be applied w/o server restart.\n# Path to the list is under top_level_domains_path (see above).\ntop_level_domains_lists: ''\n\n# public_suffix_list: /path/to/public_suffix_list.dat\n\n# Configuration of external dictionaries. See:\n# https://clickhouse.com/docs/en/sql-reference/dictionaries/external-dictionaries/external-dicts\ndictionaries_config: '*_dictionary.xml'\n\n# Uncomment if you want data to be compressed 30-100% better.\n# Don't do that if you just started using ClickHouse.\n\n# compression:\n#     # Set of variants. Checked in order. Last matching case wins. If nothing matches, lz4 will be used.\n#     case:\n#         Conditions. All must be satisfied. Some conditions may be omitted.\n#         # min_part_size: 10000000000    # Min part size in bytes.\n#         # min_part_size_ratio: 0.01     # Min size of part relative to whole table size.\n#         # What compression method to use.\n#         method: zstd\n\n# Allow to execute distributed DDL queries (CREATE, DROP, ALTER, RENAME) on cluster.\n# Works only if ZooKeeper is enabled. Comment it if such functionality isn't required.\ndistributed_ddl:\n  # Path in ZooKeeper to queue with DDL queries\n  path: /clickhouse/task_queue/ddl\n\n  # Settings from this profile will be used to execute DDL queries\n  # profile: default\n\n  # Controls how much ON CLUSTER queries can be run simultaneously.\n  # pool_size: 1\n\n  # Cleanup settings (active tasks will not be removed)\n\n  # Controls task TTL (default 1 week)\n  # task_max_lifetime: 604800\n\n  # Controls how often cleanup should be performed (in seconds)\n  # cleanup_delay_period: 60\n\n  # Controls how many tasks could be in the queue\n  # max_tasks_in_queue: 1000\n\n# Settings to fine tune MergeTree tables. See documentation in source code, in MergeTreeSettings.h\n# merge_tree:\n#     max_suspicious_broken_parts: 5\n\n# Protection from accidental DROP.\n# If size of a MergeTree table is greater than max_table_size_to_drop (in bytes) than table could not be dropped with any DROP query.\n# If you want do delete one table and don't want to change clickhouse-server config, you could create special file <clickhouse-path>/flags/force_drop_table and make DROP once.\n# By default max_table_size_to_drop is 50GB; max_table_size_to_drop=0 allows to DROP any tables.\n# The same for max_partition_size_to_drop.\n# Uncomment to disable protection.\n\n# max_table_size_to_drop: 0\n# max_partition_size_to_drop: 0\n\n# Example of parameters for GraphiteMergeTree table engine\ngraphite_rollup_example:\n  pattern:\n    regexp: click_cost\n    function: any\n    retention:\n      - age: 0\n        precision: 3600\n      - age: 86400\n        precision: 60\n  default:\n    function: max\n    retention:\n      - age: 0\n        precision: 60\n      - age: 3600\n        precision: 300\n      - age: 86400\n        precision: 3600\n\nformat_schema_path: /var/lib/clickhouse/format_schemas/\n\n# Default query masking rules, matching lines would be replaced with something else in the logs\n# (both text logs and system.query_log).\n# name - name for the rule (optional)\n# regexp - RE2 compatible regular expression (mandatory)\n# replace - substitution string for sensitive data (optional, by default - six asterisks)\nquery_masking_rules:\n  rule:\n    name: hide encrypt/decrypt arguments\n    regexp: '((?:aes_)?(?:encrypt|decrypt)(?:_mysql)?)\\s*\\(\\s*(?:''(?:\\\\''|.)+''|.*?)\\s*\\)'\n    # or more secure, but also more invasive:\n    # (aes_\\w+)\\s*\\(.*\\)\n    replace: \\1(???)\n\n# Uncomment to use custom http handlers.\n# rules are checked from top to bottom, first match runs the handler\n# url - to match request URL, you can use 'regex:' prefix to use regex match(optional)\n# methods - to match request method, you can use commas to separate multiple method matches(optional)\n# headers - to match request headers, match each child element(child element name is header name), you can use 'regex:' prefix to use regex match(optional)\n# handler is request handler\n# type - supported types: static, dynamic_query_handler, predefined_query_handler\n# query - use with predefined_query_handler type, executes query when the handler is called\n# query_param_name - use with dynamic_query_handler type, extracts and executes the value corresponding to the <query_param_name> value in HTTP request params\n# status - use with static type, response status code\n# content_type - use with static type, response content-type\n# response_content - use with static type, Response content sent to client, when using the prefix 'file://' or 'config://', find the content from the file or configuration send to client.\n\n# http_handlers:\n#     - rule:\n#         url: /\n#         methods: POST,GET\n#         headers:\n#           pragma: no-cache\n#         handler:\n#           type: dynamic_query_handler\n#           query_param_name: query\n#     - rule:\n#         url: /predefined_query\n#         methods: POST,GET\n#         handler:\n#           type: predefined_query_handler\n#           query: 'SELECT * FROM system.settings'\n#     - rule:\n#         handler:\n#           type: static\n#           status: 200\n#           content_type: 'text/plain; charset=UTF-8'\n#           response_content: config://http_server_default_response\n\nsend_crash_reports:\n  enabled: false\n  anonymize: false\n  endpoint: 'https://6f33034cfe684dd7a3ab9875e57b1c8d@o388870.ingest.sentry.io/5226277'\n  # disable_internal_dns_cache: 1\n"})}),"\n",(0,r.jsx)(n.h2,{id:"usersyaml",children:"users.yaml"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"uuidgen | tee -a /dev/fd/2 | tr -d '\\r\\n' | sha256sum\n"})}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-yaml",children:"profiles:\n  default:\n    # 10G\n    max_memory_usage: 10000000000\n\n    # How to choose between replicas during distributed query processing.\n    # random - choose random replica from set of replicas with minimum number of errors\n    # nearest_hostname - from set of replicas with minimum number of errors, choose replica\n    # with minimum number of different symbols between replica's hostname and local hostname (Hamming distance).\n    # in_order - first live replica is chosen in specified order.\n    # first_or_random - if first replica one has higher number of errors, pick a random one from replicas with minimum number of errors.\n    load_balancing: random\n\n  readonly:\n    readonly: 1\n\nusers:\n  # \u5982\u679C\u672A\u7ED9 username \u5219\u540D\u5B57\u4E3A default\n  default:\n    # PASSWORD=$(base64 < /dev/urandom | head -c32); echo \"$PASSWORD\"; echo -n \"$PASSWORD\" | sha256sum | tr -d '-'\n    password_sha256_hex:\n    # PASSWORD=$(base64 < /dev/urandom | head -c32); echo \"$PASSWORD\"; echo -n \"$PASSWORD\" | sha1sum | tr -d '-' | xxd -r -p | sha1sum | tr -d '-'\n    password_double_sha1_hex:\n    password: ''\n\n    # LDAP\n    # server: my_ldap_server\n    # ldap:\n    kerberos:\n      realm: EXAMPLE.COM\n\n\n    # List of networks with open access.\n    #\n    # To open access from everywhere, specify:\n    #    - ip: '::/0'\n    #\n    # To open access only from localhost, specify:\n    #    - ip: '::1'\n    #    - ip: 127.0.0.1\n    #\n    # Each element of list has one of the following forms:\n    # ip: IP-address or network mask. Examples: 213.180.204.3 or 10.0.0.1/8 or 10.0.0.1/255.255.255.0\n    # 2a02:6b8::3 or 2a02:6b8::3/64 or 2a02:6b8::3/ffff:ffff:ffff:ffff::.\n    # host: Hostname. Example: server01.clickhouse.com.\n    # To check access, DNS query is performed, and all received addresses compared to peer address.\n    # host_regexp: Regular expression for host names. Example, ^server\\d\\d-\\d\\d-\\d\\.clickhouse\\.com$\n    # To check access, DNS PTR query is performed for peer address and then regexp is applied.\n    # Then, for result of PTR query, another DNS query is performed and all received addresses compared to peer address.\n    # Strongly recommended that regexp is ends with $ and take all expression in ''\n    # All results of DNS requests are cached till server restart.\n\n    networks:\n      ip: '::/0'\n\n    # Settings profile for user.\n    profile: default\n\n    # Quota for user.\n    quota: default\n\n    # User can create other users and grant rights to them.\n    # access_management: 1\n\n# Quotas.\nquotas:\n  # Name of quota.\n  default:\n    # Limits for time interval. You could specify many intervals with different limits.\n    interval:\n      # Length of interval.\n      duration: 3600\n\n      # No limits. Just calculate resource usage for time interval.\n      queries: 0\n      errors: 0\n      result_rows: 0\n      read_rows: 0\n      execution_time: 0\n"})}),"\n",(0,r.jsx)(n.h2,{id:"rest-api",children:"REST API"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"echo 'SELECT version()' | curl 'http://localhost:8123/' --data-binary @-\n\ncurl 'http://192.168.66.61:8123?query=select%20version()'\ncurl --get http://192.168.66.61:8123 --data-urlencode 'query=select version()'\n"})}),"\n",(0,r.jsx)(n.h2,{id:"query",children:"Query"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-sql",children:"select version();\n\nselect currentDatabase();\nselect currentProfiles();\nselect currentUser();\nselect currentRoles();\n"})}),"\n",(0,r.jsx)(n.h2,{id:"docker_related_configxml",children:"docker_related_config.xml"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-xml",children:"<clickhouse>\n     \x3c!-- Listen wildcard address to allow accepting connections from other containers and host network. --\x3e\n    <listen_host>::</listen_host>\n    <listen_host>0.0.0.0</listen_host>\n    <listen_try>1</listen_try>\n\n    \x3c!--\n    <logger>\n        <console>1</console>\n    </logger>\n    --\x3e\n</clickhouse>\n"})})]})}function u(e={}){let{wrapper:n}={...(0,o.R)(),...e.components};return n?(0,r.jsx)(n,{...e,children:(0,r.jsx)(d,{...e})}):d(e)}},17776:function(e,n,t){t.d(n,{R:()=>a,x:()=>i});var s=t(7378);let r={},o=s.createContext(r);function a(e){let n=s.useContext(o);return s.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function i(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(r):e.components||r:a(e.components),s.createElement(o.Provider,{value:n},e.children)}}}]);