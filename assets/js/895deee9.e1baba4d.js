"use strict";(self.webpackChunkwener_website=self.webpackChunkwener_website||[]).push([["16290"],{74810:function(e,t,r){r.r(t),r.d(t,{frontMatter:()=>a,toc:()=>c,default:()=>m,metadata:()=>n,assets:()=>l,contentTitle:()=>i});var n=JSON.parse('{"id":"ai/ocr/tatr","title":"Table Transformer","description":"- microsoft/table-transformer","source":"@site/../notes/ai/ocr/tatr.md","sourceDirName":"ai/ocr","slug":"/ai/ocr/tatr","permalink":"/notes/ai/ocr/tatr","draft":false,"unlisted":false,"editUrl":"https://github.com/wenerme/wener/edit/master/notes/../notes/ai/ocr/tatr.md","tags":[{"inline":true,"label":"Model","permalink":"/notes/tags/model"}],"version":"current","lastUpdatedBy":"wener","lastUpdatedAt":1741072966000,"frontMatter":{"tags":["Model"]},"sidebar":"docs","previous":{"title":"surya","permalink":"/notes/ai/ocr/surya"},"next":{"title":"\u7CBE\u5EA6","permalink":"/notes/ai/precision"}}'),s=r(86106),o=r(17776);let a={tags:["Model"]},i="Table Transformer",l={},c=[];function u(e){let t={a:"a",code:"code",h1:"h1",header:"header",li:"li",pre:"pre",ul:"ul",...(0,o.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(t.header,{children:(0,s.jsx)(t.h1,{id:"table-transformer",children:"Table Transformer"})}),"\n",(0,s.jsxs)(t.ul,{children:["\n",(0,s.jsxs)(t.li,{children:[(0,s.jsx)(t.a,{href:"https://github.com/microsoft/table-transformer",children:"microsoft/table-transformer"}),"\n",(0,s.jsxs)(t.ul,{children:["\n",(0,s.jsx)(t.li,{children:"TATR - Table Transformer"}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(t.li,{children:["labels\n",(0,s.jsxs)(t.ul,{children:["\n",(0,s.jsx)(t.li,{children:"table"}),"\n",(0,s.jsx)(t.li,{children:"table column"}),"\n",(0,s.jsx)(t.li,{children:"table row"}),"\n",(0,s.jsx)(t.li,{children:"table column header"}),"\n",(0,s.jsx)(t.li,{children:"table projected row header"}),"\n",(0,s.jsx)(t.li,{children:"table spanning cell"}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(t.li,{children:["\u53C2\u8003\n",(0,s.jsxs)(t.ul,{children:["\n",(0,s.jsx)(t.li,{children:(0,s.jsx)(t.a,{href:"https://huggingface.co/microsoft/table-transformer-structure-recognition",children:"https://huggingface.co/microsoft/table-transformer-structure-recognition"})}),"\n",(0,s.jsx)(t.li,{children:(0,s.jsx)(t.a,{href:"https://huggingface.co/microsoft/table-transformer-structure-recognition-v1.1-all",children:"microsoft/table-transformer-structure-recognition-v1.1-all"})}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(t.pre,{children:(0,s.jsx)(t.code,{className:"language-py",children:"from transformers import TableTransformerForObjectDetection\nimport torch\n\ntatr = TableTransformerForObjectDetection.from_pretrained(\"microsoft/table-structure-recognition-v1.1-all\")\ndevice = torch.device((\"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\"))\ntatr = tatr.to(device)\ntatr.config.id2label\n#%%\nfrom torchvision import transforms\n\nclass MaxResize(object):\n  def __init__(self, max_size=800):\n    self.max_size = max_size\n\n  def __call__(self, image):\n    width, height = image.size\n    current_max_size = max(width, height)\n    scale = self.max_size / current_max_size\n    resized_image = image.resize((int(round(scale * width)), int(round(scale * height))))\n    return resized_image\n\n\nstructure_transform = transforms.Compose([\n  MaxResize(1000),\n  transforms.ToTensor(),\n  transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n])\n\nfrom PIL import Image\n\nimg = Image.open(\"./inputs/table.png\").convert(\"RGB\")\npixel_values = structure_transform(img).unsqueeze(0).to(device)\n\nwith torch.no_grad():\n  structure_outputs = tatr(pixel_values)\n\nstructure_outputs\n#%%\ndef box_cxcywh_to_xyxy(x):\n  x_c, y_c, w, h = x.unbind(-1)\n  b = [(x_c - 0.5 * w), (y_c - 0.5 * h), (x_c + 0.5 * w), (y_c + 0.5 * h)]\n  return torch.stack(b, dim=1)\n\ndef rescale_bboxes(out_bbox, size):\n  img_w, img_h = size\n  b = box_cxcywh_to_xyxy(out_bbox)\n  b = b * torch.tensor([img_w, img_h, img_w, img_h], dtype=torch.float32)\n  return b\n\ndef outputs_to_objects(outputs, img_size, class_idx2name):\n  m = outputs['logits'].softmax(-1).max(-1)\n  pred_labels = list(m.indices.detach().cpu().numpy())[0]\n  pred_scores = list(m.values.detach().cpu().numpy())[0]\n  pred_bboxes = outputs['pred_boxes'].detach().cpu()[0]\n  pred_bboxes = [elem.tolist() for elem in rescale_bboxes(pred_bboxes, img_size)]\n\n  objects = []\n  for label, score, bbox in zip(pred_labels, pred_scores, pred_bboxes):\n    class_label = class_idx2name[int(label)]\n    if not class_label == 'no object':\n      objects.append({'label': class_label, 'score': float(score), 'bbox': [float(elem) for elem in bbox]})\n\n  return objects\n\nimport copy\n\nstructure_id2label = copy.deepcopy(tatr.config.id2label)\nstructure_id2label[len(structure_id2label)] = \"no object\"\n\nstructure_objects = outputs_to_objects(structure_outputs, img.size, structure_id2label)\nstructure_objects  # [{label,score,bbox}]\n#%%\nfrom PIL import ImageDraw\n\ndef draw_bboxes(bboxes, page_image, color='red'):\n  page_image = page_image.copy()\n  draw = ImageDraw.Draw(page_image)\n\n  for bbox in bboxes:\n    draw.rectangle(bbox, outline=color)\n\n  return page_image\n\n\nobjects = [x for x in structure_objects if x['score'] > 0.5]\ndraw_bboxes([x['bbox'] for x in objects], img)\n"})})]})}function m(e={}){let{wrapper:t}={...(0,o.R)(),...e.components};return t?(0,s.jsx)(t,{...e,children:(0,s.jsx)(u,{...e})}):u(e)}},17776:function(e,t,r){r.d(t,{R:()=>a,x:()=>i});var n=r(7378);let s={},o=n.createContext(s);function a(e){let t=n.useContext(o);return n.useMemo(function(){return"function"==typeof e?e(t):{...t,...e}},[t,e])}function i(e){let t;return t=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:a(e.components),n.createElement(o.Provider,{value:t},e.children)}}}]);