"use strict";(self.webpackChunkwener_website=self.webpackChunkwener_website||[]).push([["44745"],{88893:function(e,n,i){i.r(n),i.d(n,{frontMatter:()=>t,toc:()=>o,default:()=>d,metadata:()=>l,assets:()=>c,contentTitle:()=>h});var l=JSON.parse('{"id":"ai/inference/inference-awesome","title":"Inference Awesome","description":"- ollama","source":"@site/../notes/ai/inference/inference-awesome.md","sourceDirName":"ai/inference","slug":"/ai/inference/awesome","permalink":"/notes/ai/inference/awesome","draft":false,"unlisted":false,"editUrl":"https://github.com/wenerme/wener/edit/master/notes/../notes/ai/inference/inference-awesome.md","tags":[{"inline":true,"label":"Awesome","permalink":"/notes/tags/awesome"}],"version":"current","lastUpdatedBy":"wener","lastUpdatedAt":1758158840000,"frontMatter":{"tags":["Awesome"]},"sidebar":"docs","previous":{"title":"\u63A8\u7406","permalink":"/notes/ai/inference/"},"next":{"title":"LLM","permalink":"/notes/ai/llm/"}}'),r=i(86106),s=i(17776);let t={tags:["Awesome"]},h="Inference Awesome",c={},o=[];function a(e){let n={a:"a",h1:"h1",header:"header",li:"li",ul:"ul",...(0,s.R)(),...e.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(n.header,{children:(0,r.jsx)(n.h1,{id:"inference-awesome",children:"Inference Awesome"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"ollama"}),"\n",(0,r.jsx)(n.li,{children:"toruchrun"}),"\n",(0,r.jsxs)(n.li,{children:["vLLM Virtual Large Language Model\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"PagedAttention"}),"\n"]}),"\n"]}),"\n",(0,r.jsx)(n.li,{children:"SGLang"}),"\n",(0,r.jsx)(n.li,{children:"localai"}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.a,{href:"https://github.com/NVIDIA/TensorRT-LLM",children:"NVIDIA/TensorRT-LLM"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Apache-2.0, C++, Python"}),"\n",(0,r.jsx)(n.li,{children:"trt"}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.a,{href:"https://github.com/huggingface/text-generation-inference",children:"huggingface/text-generation-inference"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Apache-2.0, Python, Rust"}),"\n",(0,r.jsx)(n.li,{children:"HF TGI"}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.a,{href:"https://github.com/triton-inference-server/server",children:"triton-inference-server/server"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"BSD-3, Python, C++"}),"\n",(0,r.jsx)(n.li,{children:"NVIDIA Triton"}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.a,{href:"https://github.com/bentoml/BentoML",children:"bentoml/BentoML"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Apache-2.0, Python"}),"\n",(0,r.jsx)(n.li,{children:"Build Model Inference APIs, Job queues, LLM apps, Multi-model pipelines"}),"\n",(0,r.jsx)(n.li,{children:(0,r.jsx)(n.a,{href:"https://github.com/bentoml/BentoDiffusion",children:"bentoml/BentoDiffusion"})}),"\n",(0,r.jsx)(n.li,{children:(0,r.jsx)(n.a,{href:"https://github.com/bentoml/OpenLLM",children:"bentoml/OpenLLM"})}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["Image\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"ComfyUI"}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.a,{href:"https://github.com/AUTOMATIC1111/stable-diffusion-webui",children:"AUTOMATIC1111/stable-diffusion-webui"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"A1111"}),"\n"]}),"\n"]}),"\n",(0,r.jsx)(n.li,{children:"SD"}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["Audio\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Whisper"}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["Embeddings\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.a,{href:"https://github.com/michaelfeil/infinity",children:"michaelfeil/infinity"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"MIT, Python"}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.a,{href:"https://github.com/exo-explore/exo",children:"exo-explore/exo"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"GPLv3, Python"}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.a,{href:"https://github.com/Tencent/ncnn",children:"Tencent/ncnn"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"BSD-3, C/C++"}),"\n",(0,r.jsx)(n.li,{children:"neural network inference framework optimized for the mobile platform"}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.a,{href:"https://github.com/InternLM/lmdeploy",children:"InternLM/lmdeploy"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Apache-2.0, Python, C++"}),"\n",(0,r.jsxs)(n.li,{children:["based on ",(0,r.jsx)(n.a,{href:"https://github.com/open-mmlab/mmrazor",children:"MMRazor"})," and ",(0,r.jsx)(n.a,{href:"https://github.com/open-mmlab/mmdeploy",children:"MMDeploy"})]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.a,{href:"https://github.com/mit-han-lab/nunchaku",children:"mit-han-lab/nunchaku"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Apache-2.0, Python, C++"}),"\n",(0,r.jsx)(n.li,{children:"Nunchaku is a high-performance inference engine optimized for 4-bit neural networks"}),"\n",(0,r.jsx)(n.li,{children:"SVDQuant: Absorbing Outliers by Low-Rank Components for 4-Bit Diffusion Models"}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.a,{href:"https://github.com/theroyallab/tabbyAPI",children:"theroyallab/tabbyAPI"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"AGPLv3, Python"}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.a,{href:"https://github.com/turboderp-org/exllamav2",children:"turboderp-org/exllamav2"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"MIT, Python"}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.a,{href:"https://github.com/turboderp-org/exllamav3",children:"turboderp-org/exllamav3"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"MIT, Python"}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["Reading\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:(0,r.jsx)(n.a,{href:"https://github.com/bentoml/llm-inference-handbook",children:"https://github.com/bentoml/llm-inference-handbook"})}),"\n"]}),"\n"]}),"\n"]})]})}function d(e={}){let{wrapper:n}={...(0,s.R)(),...e.components};return n?(0,r.jsx)(n,{...e,children:(0,r.jsx)(a,{...e})}):a(e)}},17776:function(e,n,i){i.d(n,{R:()=>t,x:()=>h});var l=i(7378);let r={},s=l.createContext(r);function t(e){let n=l.useContext(s);return l.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function h(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(r):e.components||r:t(e.components),l.createElement(s.Provider,{value:n},e.children)}}}]);