"use strict";(self.webpackChunkwener_website=self.webpackChunkwener_website||[]).push([["95116"],{80006:function(n,e,t){t.r(e),t.d(e,{frontMatter:()=>a,toc:()=>c,default:()=>h,metadata:()=>r,assets:()=>l,contentTitle:()=>i});var r=JSON.parse('{"id":"ai/ml/pytorch/pytorch-learning","title":"PyTorch Learning","description":"- \u4EE5\u4E0B\u5212\u7EBF\u7ED3\u5C3E\u7684\u64CD\u4F5C\uFF08\u5982 xxx\\\\_\uFF09\u4F1A\u539F\u5730\u6267\u884C\uFF0C\u907F\u514D\u4E0D\u5FC5\u8981\u7684\u5185\u5B58\u62F7\u8D1D","source":"@site/../notes/ai/ml/pytorch/pytorch-learning.md","sourceDirName":"ai/ml/pytorch","slug":"/ai/ml/pytorch/learning","permalink":"/notes/ai/ml/pytorch/learning","draft":false,"unlisted":false,"editUrl":"https://github.com/wenerme/wener/edit/master/notes/../notes/ai/ml/pytorch/pytorch-learning.md","tags":[{"inline":true,"label":"Learning","permalink":"/notes/tags/learning"}],"version":"current","lastUpdatedBy":"wener","lastUpdatedAt":1756019924000,"frontMatter":{"tags":["Learning"]},"sidebar":"docs","previous":{"title":"Hub","permalink":"/notes/ai/ml/pytorch/hub"},"next":{"title":"Lightning","permalink":"/notes/ai/ml/pytorch/lightning"}}'),s=t(86106),o=t(17776);let a={tags:["Learning"]},i="PyTorch Learning",l={},c=[];function d(n){let e={a:"a",code:"code",h1:"h1",header:"header",hr:"hr",li:"li",pre:"pre",ul:"ul",...(0,o.R)(),...n.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(e.header,{children:(0,s.jsx)(e.h1,{id:"pytorch-learning",children:"PyTorch Learning"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsx)(e.li,{children:"\u4EE5\u4E0B\u5212\u7EBF\u7ED3\u5C3E\u7684\u64CD\u4F5C\uFF08\u5982 xxx_\uFF09\u4F1A\u539F\u5730\u6267\u884C\uFF0C\u907F\u514D\u4E0D\u5FC5\u8981\u7684\u5185\u5B58\u62F7\u8D1D"}),"\n",(0,s.jsxs)(e.li,{children:["\u53C2\u8003\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsx)(e.li,{children:(0,s.jsx)(e.a,{href:"https://sebastianraschka.com/teaching/pytorch-1h/",children:"https://sebastianraschka.com/teaching/pytorch-1h/"})}),"\n",(0,s.jsxs)(e.li,{children:["\u300ABuild a Large Language Model\u300B\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsx)(e.li,{children:(0,s.jsx)(e.a,{href:"https://github.com/rasbt/LLMs-from-scratch",children:"https://github.com/rasbt/LLMs-from-scratch"})}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-py",children:'import torch\nimport torch.nn as nn\n\n\n# \u5B9A\u4E49\u6A21\u578B\nclass RegressionModel(nn.Module):\n	def __init__(self):\n		super().__init__()\n		self.a = nn.Parameter(torch.randn(1, requires_grad=True, dtype=torch.float))\n		self.b = nn.Parameter(torch.randn(1, requires_grad=True, dtype=torch.float))\n		self.c = nn.Parameter(torch.randn(1, requires_grad=True, dtype=torch.float))\n\n	def forward(self, x: torch.Tensor) -> torch.Tensor:\n		return self.a * x ** 2 + self.b * x + self.c\n\n\nmodel = RegressionModel()\n\n# \u52A0\u8F7D\u6A21\u578B\n# model.load_state_dict(torch.load("model.pth"))\n# model.load_state_dict(safetensors.torch.load_file("model.safetensors"))\n\n# \u9009\u62E9\u635F\u5931\u51FD\u6570\nloss_fn = nn.L1Loss()\n\n# \u5B9A\u4E49\u4F18\u5316\u5668\u51FD\u6570 - \u6709\u8BB8\u591A\u51FD\u6570\u53EF\u7528\uFF0C\u4F46\u5E38\u89C1\u7684\u6709\n# optim.SGD(), optim.Adam()\noptimizer = torch.optim.SGD(params=model.parameters(), lr=0.01)\n\n# \u6D4B\u8BD5\u6570\u636E\n\n# \u771F\u5B9E\u53C2\u6570, \u6A21\u578B\u8981\u5B66\u4E60\u7684\na, b, c = 0.2, 0.3, -0.1\nX = torch.arange(-1, 1, 0.001, dtype=torch.float32).unsqueeze(1)  # \u751F\u6210\u533A\u95F4[-1,1)\u7684\u81EA\u53D8\u91CF\uFF0C\u5E76\u589E\u52A0\u4E00\u7EF4\uFF08\u5F62\u72B6[N]\u2192[N,1]\uFF09\uFF0C\u907F\u514D\u540E\u7EED\u5F62\u72B6\u4E0D\u5339\u914D\ny = a * X ** 2 + b * X + c  # \u6309\u4E8C\u6B21\u51FD\u6570\u5173\u7CFB\u751F\u6210\u6807\u7B7E\n\nfrom sklearn.model_selection import train_test_split\n\n# \u5212\u5206\u8BAD\u7EC3\u96C6\u548C\u6D4B\u8BD5\u96C6\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# epoch \u8868\u793A\u5B8C\u6574\u904D\u5386\u8BAD\u7EC3\u96C6\u7684\u6B21\u6570\n# \u8FD9\u662F\u4E00\u4E2A\u8D85\u53C2\u6570\uFF0C\u4F1A\u5F71\u54CD\u8BAD\u7EC3\u65F6\u957F\u4E0E\u6536\u655B\u6548\u679C\nepochs = 1000\nfor epoch in range(epochs):\n	model.train()  # \u5207\u6362\u5230\u8BAD\u7EC3\u6A21\u5F0F\n\n	# 1. \u524D\u5411\u4F20\u64AD\uFF08\u8BAD\u7EC3\u96C6\uFF09\n	y_pred_train = model(X_train)\n	# 2. \u8BA1\u7B97\u8BAD\u7EC3\u96C6\u635F\u5931\n	loss = loss_fn(y_pred_train, y_train)\n	# 3. \u6E05\u7A7A\u4E0A\u4E00\u8F6E\u7684\u68AF\u5EA6\n	optimizer.zero_grad()\n	# 4. \u53CD\u5411\u4F20\u64AD\u8BA1\u7B97\u68AF\u5EA6\n	loss.backward()\n	# 5. \u4F7F\u7528\u4F18\u5316\u5668\u66F4\u65B0\u6A21\u578B\u53C2\u6570\n	optimizer.step()\n\n	# \u6D4B\u8BD5\u73AF\u8282\uFF1A\u76D1\u63A7\u662F\u5426\u8FC7\u62DF\u5408\n	model.eval()  # \u5207\u6362\u5230\u8BC4\u4F30\u6A21\u5F0F\n	with torch.inference_mode():\n		# 1. \u524D\u5411\u4F20\u64AD\uFF08\u6D4B\u8BD5\u96C6\uFF09\n		test_preds = model(X_test)\n		# 2. \u8BA1\u7B97\u6D4B\u8BD5\u96C6\u635F\u5931\n		test_loss = loss_fn(test_preds, y_test)\n\n	if epoch % 100 == 0:\n		print(f"Epoch: {epoch} | Train loss: {loss} | Test Loss: {test_loss}")\n\n# \u4FDD\u5B58\u6A21\u578B\ntorch.save(model.state_dict(), "model.pth")\n# \u4FDD\u5B58 safetensor\nfrom safetensors.torch import save_file\n\nsave_file(model.state_dict(), "model.safetensors")\n'})}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-py",children:"model = RegressionModel()\nmodel.load_state_dict(torch.load('model.pth'))\n\n# \u9884\u6D4B 0.5\nx_new = torch.tensor([[0.5]], dtype=torch.float32)\nmodel.eval()  # \u5207\u6362\u5230\u8BC4\u4F30\u6A21\u5F0F\nwith torch.no_grad():  # \u8BC4\u4F30\u65F6\u4E0D\u9700\u8981\u8BA1\u7B97\u68AF\u5EA6\uFF0C\u8282\u7701\u5185\u5B58\n	y_new = model(x_new)\nprint(f'\u9884\u6D4B\u8F93\u5165 {x_new.item()} \u65F6\uFF0C\u6A21\u578B\u8F93\u51FA\u4E3A {y_new.item()}')\n"})}),"\n",(0,s.jsx)(e.hr,{}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-py",children:'#%%\nimport torch\n\n\nclass NeuralNetwork(torch.nn.Module):\n  def __init__(self, num_inputs, num_outputs):\n    super().__init__()\n\n    self.layers = torch.nn.Sequential(\n\n      # 1st hidden layer\n      torch.nn.Linear(num_inputs, 30),\n      torch.nn.ReLU(),\n\n      # 2nd hidden layer\n      torch.nn.Linear(30, 20),\n      torch.nn.ReLU(),\n\n      # output layer\n      torch.nn.Linear(20, num_outputs),\n    )\n\n  def forward(self, x):\n    logits = self.layers(x)\n    return logits\n#%%\nmodel = NeuralNetwork(num_inputs=50, num_outputs=3)\nprint(model)\n#%%\nnum_params = sum(\n    p.numel() for p in model.parameters() if p.requires_grad\n)\nprint("Total number of trainable model parameters:", num_params)\n'})}),"\n",(0,s.jsx)(e.hr,{}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-py",children:'#%%\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\n#%%\n\n# \u8BA1\u7B97\u8BBE\u5907\ndevice = "cuda" if torch.cuda.is_available() else "mps" if torch.backends.mps.is_available() else "cpu"\ntorch.set_default_device(device)\n'})})]})}function h(n={}){let{wrapper:e}={...(0,o.R)(),...n.components};return e?(0,s.jsx)(e,{...n,children:(0,s.jsx)(d,{...n})}):d(n)}},17776:function(n,e,t){t.d(e,{R:()=>a,x:()=>i});var r=t(7378);let s={},o=r.createContext(s);function a(n){let e=r.useContext(o);return r.useMemo(function(){return"function"==typeof n?n(e):{...e,...n}},[e,n])}function i(n){let e;return e=n.disableParentContext?"function"==typeof n.components?n.components(s):n.components||s:a(n.components),r.createElement(o.Provider,{value:e},n.children)}}}]);