"use strict";(self.webpackChunkwener_website=self.webpackChunkwener_website||[]).push([["415556"],{859338:function(e,o,n){n.r(o),n.d(o,{frontMatter:()=>c,toc:()=>a,default:()=>p,metadata:()=>s,assets:()=>l,contentTitle:()=>r});var s=JSON.parse('{"id":"ai/model/cosyvoice","title":"CosyVoice","description":"- FunAudioLLM/CosyVoice","source":"@site/../notes/ai/model/cosyvoice.md","sourceDirName":"ai/model","slug":"/ai/model/cosyvoice","permalink":"/notes/ai/model/cosyvoice","draft":false,"unlisted":false,"editUrl":"https://github.com/wenerme/wener/edit/master/notes/../notes/ai/model/cosyvoice.md","tags":[],"version":"current","lastUpdatedBy":"wener","lastUpdatedAt":1748756432000,"frontMatter":{"title":"CosyVoice"},"sidebar":"docs","previous":{"title":"BiRefNet","permalink":"/notes/ai/model/birefnet"},"next":{"title":"DINO","permalink":"/notes/ai/model/dino/"}}'),i=n(486106),t=n(917776);let c={title:"CosyVoice"},r="CosyVoice",l={},a=[];function d(e){let o={a:"a",code:"code",h1:"h1",header:"header",li:"li",pre:"pre",ul:"ul",...(0,t.R)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(o.header,{children:(0,i.jsx)(o.h1,{id:"cosyvoice",children:"CosyVoice"})}),"\n",(0,i.jsxs)(o.ul,{children:["\n",(0,i.jsxs)(o.li,{children:[(0,i.jsx)(o.a,{href:"https://github.com/FunAudioLLM/CosyVoice",children:"FunAudioLLM/CosyVoice"}),"\n",(0,i.jsxs)(o.ul,{children:["\n",(0,i.jsx)(o.li,{children:"\u4E2D\u6587\u3001\u82F1\u6587\u3001\u65E5\u6587\u3001\u97E9\u6587\u3001\u4E2D\u6587\u65B9\u8A00\uFF08\u7CA4\u8BED\u3001\u56DB\u5DDD\u8BDD\u3001\u4E0A\u6D77\u8BDD\u3001\u5929\u6D25\u8BDD\u3001\u6B66\u6C49\u8BDD\u7B49\uFF09"}),"\n",(0,i.jsxs)(o.li,{children:["hf ",(0,i.jsx)(o.a,{href:"https://huggingface.co/FunAudioLLM/CosyVoice2-0.5B",children:"FunAudioLLM/CosyVoice2-0.5B"})]}),"\n"]}),"\n"]}),"\n",(0,i.jsx)(o.li,{children:"\u26A0\uFE0F \u6CE8\u610F \u590D\u523B\u6709\u7535\u97F3\u95EE\u9898"}),"\n",(0,i.jsxs)(o.li,{children:["\u63A8\u7406\u6A21\u5F0F - \u4E0D\u540C\u7684\u6A21\u578B\u652F\u6301\u4E0D\u540C\u7684\u6A21\u5F0F\n",(0,i.jsxs)(o.ul,{children:["\n",(0,i.jsx)(o.li,{children:"\u9884\u8BAD\u7EC3\u97F3\u8272"}),"\n",(0,i.jsx)(o.li,{children:"\u8DE8\u8BED\u79CD\u590D\u523B"}),"\n",(0,i.jsxs)(o.li,{children:["\u81EA\u7136\u8BED\u8A00\u63A7\u5236 CosyVoice-300M-Instruct\n",(0,i.jsxs)(o.ul,{children:["\n",(0,i.jsx)(o.li,{children:"\u652F\u6301\u4F7F\u7528 instruct \u63A7\u5236\u8BED\u97F3"}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(o.li,{children:["\u590D\u523B\n",(0,i.jsxs)(o.ul,{children:["\n",(0,i.jsx)(o.li,{children:"\u6587\u672C+\u97F3\u9891 \u751F\u6210\u4E00\u4E2A speaker"}),"\n",(0,i.jsx)(o.li,{children:"\u4F7F\u7528\u8FD9\u4E2A speaker \u751F\u6210\u97F3\u9891"}),"\n",(0,i.jsx)(o.li,{children:"\u6837\u672C\u4E0D\u8D85\u8FC7 30s"}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,i.jsx)(o.li,{children:(0,i.jsx)(o.code,{children:"<|zh|><|en|><|jp|><|yue|><|ko|>"})}),"\n",(0,i.jsxs)(o.li,{children:["instruct ",(0,i.jsx)(o.code,{children:"<laughter></laughter><strong></strong>[laughter][breath]"})]}),"\n",(0,i.jsxs)(o.li,{children:["\u53C2\u8003\n",(0,i.jsxs)(o.ul,{children:["\n",(0,i.jsx)(o.li,{children:(0,i.jsx)(o.a,{href:"https://www.modelscope.cn/models/iic/CosyVoice2-0.5B/summary",children:"https://www.modelscope.cn/models/iic/CosyVoice2-0.5B/summary"})}),"\n",(0,i.jsx)(o.li,{children:(0,i.jsx)(o.a,{href:"https://funaudiollm.github.io/cosyvoice2",children:"https://funaudiollm.github.io/cosyvoice2"})}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,i.jsx)(o.pre,{children:(0,i.jsx)(o.code,{className:"language-bash",children:"git clone https://github.com/FunAudioLLM/CosyVoice\ncd CosyVoice\ngit submodule update --init --recursive\n\nuv venv --python 3.10\nuv pip install pynini==2.1.5\nuv pip install protobuf==4.2\n# uv pip install -r requirements.txt -i https://mirrors.aliyun.com/pypi/simple/ --trusted-host=mirrors.aliyun.com\n#  --extra-index-url https://pypi.nvidia.com\nuv pip install -r requirements.txt\n\nsudo apt install sox libsox-dev\n\n# https://www.modelscope.cn/iic/CosyVoice2-0.5B\n# iic/CosyVoice2-0.5B\nuvx --from huggingface_hub huggingface-cli download FunAudioLLM/CosyVoice2-0.5B\nls ~/.cache/huggingface/hub/models--FunAudioLLM--CosyVoice2-0.5B/snapshots/*/\n\nffmpeg input.mp3 -ar 16000 output.wav\n\nuv run ./dl.py\nuv run ./gen.py\n"})}),"\n",(0,i.jsx)(o.pre,{children:(0,i.jsx)(o.code,{className:"language-py",metastring:'title="dl.py"',children:"# uv run ./dl.py\nfrom modelscope import snapshot_download\nsnapshot_download('iic/CosyVoice2-0.5B', local_dir='pretrained_models/CosyVoice2-0.5B')\nsnapshot_download('iic/CosyVoice-300M', local_dir='iic/CosyVoice-300M')\nsnapshot_download('iic/CosyVoice-300M-25Hz', local_dir='pretrained_models/CosyVoice-300M-25Hz')\nsnapshot_download('iic/CosyVoice-300M-SFT', local_dir='pretrained_models/CosyVoice-300M-SFT')\nsnapshot_download('iic/CosyVoice-300M-Instruct', local_dir='pretrained_models/CosyVoice-300M-Instruct')\nsnapshot_download('iic/CosyVoice-ttsfrd', local_dir='pretrained_models/CosyVoice-ttsfrd')\n"})}),"\n",(0,i.jsx)(o.pre,{children:(0,i.jsx)(o.code,{className:"language-bash",children:"# \u6216\u8005 git \u4E0B\u8F7D \u6A21\u578B\napt install git-lfs\nmkdir -p pretrained_models\ngit clone https://www.modelscope.cn/iic/CosyVoice2-0.5B.git pretrained_models/CosyVoice2-0.5B\ngit clone https://www.modelscope.cn/iic/CosyVoice-300M.git pretrained_models/CosyVoice-300M\ngit clone https://www.modelscope.cn/iic/CosyVoice-300M-25Hz.git pretrained_models/CosyVoice-300M-25Hz\ngit clone https://www.modelscope.cn/iic/CosyVoice-300M-SFT.git pretrained_models/CosyVoice-300M-SFT\ngit clone https://www.modelscope.cn/iic/CosyVoice-300M-Instruct.git pretrained_models/CosyVoice-300M-Instruct\ngit clone https://www.modelscope.cn/iic/CosyVoice-ttsfrd.git pretrained_models/CosyVoice-ttsfrd\n"})}),"\n",(0,i.jsx)(o.pre,{children:(0,i.jsx)(o.code,{className:"language-py",metastring:'title="gen.py"',children:"import sys\nsys.path.append('third_party/Matcha-TTS')\nfrom cosyvoice.cli.cosyvoice import CosyVoice, CosyVoice2\nfrom cosyvoice.utils.file_utils import load_wav\nimport torchaudio\nfrom os.path import expanduser\n\ncosyvoice = CosyVoice2(expanduser('./pretrained_models/CosyVoice2-0.5B/'), load_jit=False, load_trt=False, fp16=False)\n\n# NOTE if you want to reproduce the results on https://funaudiollm.github.io/cosyvoice2, please add text_frontend=False during inference\n# zero_shot usage\nprompt_speech_16k = load_wav('./data/input.wav', 16000)\nfor i, j in enumerate(cosyvoice.inference_zero_shot('\u6536\u5230\u597D\u53CB\u4ECE\u8FDC\u65B9\u5BC4\u6765\u7684\u751F\u65E5\u793C\u7269\uFF0C\u90A3\u4EFD\u610F\u5916\u7684\u60CA\u559C\u4E0E\u6DF1\u6DF1\u7684\u795D\u798F\u8BA9\u6211\u5FC3\u4E2D\u5145\u6EE1\u4E86\u751C\u871C\u7684\u5FEB\u4E50\uFF0C\u7B11\u5BB9\u5982\u82B1\u513F\u822C\u7EFD\u653E\u3002', '\u5E0C\u671B\u4F60\u4EE5\u540E\u80FD\u591F\u505A\u7684\u6BD4\u6211\u8FD8\u597D\u5466\u3002', prompt_speech_16k, stream=False)):\n    torchaudio.save('zero_shot_{}.wav'.format(i), j['tts_speech'], cosyvoice.sample_rate)\n\n# save zero_shot spk for future usage\nassert cosyvoice.add_zero_shot_spk('\u5E0C\u671B\u4F60\u4EE5\u540E\u80FD\u591F\u505A\u7684\u6BD4\u6211\u8FD8\u597D\u5466\u3002', prompt_speech_16k, 'my_zero_shot_spk') is True\nfor i, j in enumerate(cosyvoice.inference_zero_shot('\u6536\u5230\u597D\u53CB\u4ECE\u8FDC\u65B9\u5BC4\u6765\u7684\u751F\u65E5\u793C\u7269\uFF0C\u90A3\u4EFD\u610F\u5916\u7684\u60CA\u559C\u4E0E\u6DF1\u6DF1\u7684\u795D\u798F\u8BA9\u6211\u5FC3\u4E2D\u5145\u6EE1\u4E86\u751C\u871C\u7684\u5FEB\u4E50\uFF0C\u7B11\u5BB9\u5982\u82B1\u513F\u822C\u7EFD\u653E\u3002', '', '', zero_shot_spk_id='my_zero_shot_spk', stream=False)):\n    torchaudio.save('zero_shot_{}.wav'.format(i), j['tts_speech'], cosyvoice.sample_rate)\ncosyvoice.save_spkinfo()\n\n# fine grained control, for supported control, check cosyvoice/tokenizer/tokenizer.py#L248\nfor i, j in enumerate(cosyvoice.inference_cross_lingual('\u5728\u4ED6\u8BB2\u8FF0\u90A3\u4E2A\u8352\u8BDE\u6545\u4E8B\u7684\u8FC7\u7A0B\u4E2D\uFF0C\u4ED6\u7A81\u7136[laughter]\u505C\u4E0B\u6765\uFF0C\u56E0\u4E3A\u4ED6\u81EA\u5DF1\u4E5F\u88AB\u9017\u7B11\u4E86[laughter]\u3002', prompt_speech_16k, stream=False)):\n    torchaudio.save('fine_grained_control_{}.wav'.format(i), j['tts_speech'], cosyvoice.sample_rate)\n\n# instruct usage\nfor i, j in enumerate(cosyvoice.inference_instruct2('\u6536\u5230\u597D\u53CB\u4ECE\u8FDC\u65B9\u5BC4\u6765\u7684\u751F\u65E5\u793C\u7269\uFF0C\u90A3\u4EFD\u610F\u5916\u7684\u60CA\u559C\u4E0E\u6DF1\u6DF1\u7684\u795D\u798F\u8BA9\u6211\u5FC3\u4E2D\u5145\u6EE1\u4E86\u751C\u871C\u7684\u5FEB\u4E50\uFF0C\u7B11\u5BB9\u5982\u82B1\u513F\u822C\u7EFD\u653E\u3002', '\u7528\u56DB\u5DDD\u8BDD\u8BF4\u8FD9\u53E5\u8BDD', prompt_speech_16k, stream=False)):\n    torchaudio.save('instruct_{}.wav'.format(i), j['tts_speech'], cosyvoice.sample_rate)\n\n# bistream usage, you can use generator as input, this is useful when using text llm model as input\n# NOTE you should still have some basic sentence split logic because llm can not handle arbitrary sentence length\ndef text_generator():\n    yield '\u6536\u5230\u597D\u53CB\u4ECE\u8FDC\u65B9\u5BC4\u6765\u7684\u751F\u65E5\u793C\u7269\uFF0C'\n    yield '\u90A3\u4EFD\u610F\u5916\u7684\u60CA\u559C\u4E0E\u6DF1\u6DF1\u7684\u795D\u798F'\n    yield '\u8BA9\u6211\u5FC3\u4E2D\u5145\u6EE1\u4E86\u751C\u871C\u7684\u5FEB\u4E50\uFF0C'\n    yield '\u7B11\u5BB9\u5982\u82B1\u513F\u822C\u7EFD\u653E\u3002'\nfor i, j in enumerate(cosyvoice.inference_zero_shot(text_generator(), '\u5E0C\u671B\u4F60\u4EE5\u540E\u80FD\u591F\u505A\u7684\u6BD4\u6211\u8FD8\u597D\u5466\u3002', prompt_speech_16k, stream=False)):\n    torchaudio.save('zero_shot_{}.wav'.format(i), j['tts_speech'], cosyvoice.sample_rate)\n"})})]})}function p(e={}){let{wrapper:o}={...(0,t.R)(),...e.components};return o?(0,i.jsx)(o,{...e,children:(0,i.jsx)(d,{...e})}):d(e)}},917776:function(e,o,n){n.d(o,{R:()=>c,x:()=>r});var s=n(7378);let i={},t=s.createContext(i);function c(e){let o=s.useContext(t);return s.useMemo(function(){return"function"==typeof e?e(o):{...o,...e}},[o,e])}function r(e){let o;return o=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:c(e.components),s.createElement(t.Provider,{value:o},e.children)}}}]);