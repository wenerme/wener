---
title: ClickHouse
---

# ClickHouse

:::tip When to use ClickHouse

- insert > selectğ„‚10000 > deleteğ„‚100 > updateğ„‚10
- æ•°æ®ä¸å˜
- æ•°æ®æœ‰æ—¶é—´å±æ€§
- å½’æ¡£æ•°æ®
- äº‹ä»¶ã€æ—¥å¿—ã€ç›‘æ§ã€æŒ‡æ ‡
- éœ€è¦èšåˆéå¸¸å¤šçš„æ•°æ®æº - OLAP

:::

- [yandex/ClickHouse](https://github.com/yandex/ClickHouse)
  - Apache-2.0, C++
  - OLAP, åˆ—å­˜å‚¨
    - æ¯åˆ—å­˜ä¸€ä¸ªæ–‡ä»¶
  - æ”¯æŒ Key è®¿é—®
- é»˜è®¤åº“ default
- DataGrip å¯ä»¥ä½¿ç”¨ JDBC è¿æ¥
- é€‚ç”¨åœºæ™¯
  - åº”ç”¨ã€ç³»ç»Ÿæ—¥å¿—
  - timeseries-oriented
- ç«¯å£
  - 8123 HTTP
  - 9000 native client
- å‚è€ƒ
  - [å•é¡µæ–‡æ¡£](https://clickhouse.yandex/docs/en/single/)
  - [wiki/ClickHouse](https://en.wikipedia.org/wiki/ClickHouse)
  - [clickhouse.yandex](https://clickhouse.yandex/)
  - https://clickhouse.yandex/docs/en/development/architecture/
  - [Altinity/clickhouse-operator](https://github.com/Altinity/clickhouse-operator)
  - [Usage Recommendations](https://clickhouse.com/docs/en/operations/tips/)
  - [Requirements](https://clickhouse.com/docs/en/operations/requirements/)
  - Why
    - [Fast and Reliable Schema-Agnostic Log Analytics Platform](https://eng.uber.com/logging/)
      - Uber, ES -> ClickHouse
    - [Our Online Analytical Processing Journey with ClickHouse on Kubernetes](https://tech.ebayinc.com/engineering/ou-online-analytical-processing/)
      - ebay, Druid -> ClickHouse
    - [HTTP Analytics for 6M requests per second using ClickHouse](https://blog.cloudflare.com/http-analytics-for-6m-requests-per-second-using-clickhouse/)
      - cloudflare, PostgreSQL+Citus -> ClickHouse
    - https://www.nedmcclain.com/why-devops-love-clickhouse/amp/
      - æ¨è ext4 è€Œé zfs
    - ClickHouse as an alternative to Elasticsearch for log storage and analysis [HN](https://news.ycombinator.com/item?id=26316401)
    - [A Practical Introduction to Handling Log Data in ClickHouse](https://www.youtube.com/watch?v=pZkKsfr8n3M)

:::tip

- immutable data
- æš‚ä¸æ”¯æŒ UDF
- å½“æ•°æ®é‡è¾ƒå°‘(< 1TB)æ—¶ä¸å»ºè®®ä½¿ç”¨
- æ’å…¥æ“ä½œéå¸¸å¿« - å› ä¸ºæ˜¯å¼‚æ­¥çš„ï¼Œåå°ä¼šå¤„ç†
- keep non-timeseries data out of clickhouse
- dynamic subcolumns - JSON
- OLAP
  - ä¸é€‚åˆ KV
  - ä¸é€‚åˆå°æ•°æ®ç²¾ç¡®æŸ¥è¯¢

:::

:::caution

- ä¸æ”¯æŒ UDF [#11](https://github.com/ClickHouse/ClickHouse/issues/11)
- ä¸æ”¯æŒ UPDATE, DELETE, REPLACE, MERGE, UPSERT, INSERT UPDATE
  - å¯ä»¥ DROP PARTITION å®ç°éƒ¨åˆ†æ•°æ®åˆ é™¤
  - æ”¯æŒ mutation `ALTER TABLE name UPDATE/DELETE column=exp WHERE filter`
    - åå°å¼‚æ­¥æ‰§è¡Œï¼Œå…¨éƒ¨æ•°æ®é‡å†™ï¼ŒéåŸå­æ“ä½œï¼Œéå¸¸æ…¢
    - æ»¡è¶³ GDPR è¦æ±‚
- ä¸æ”¯æŒè¯»å†™åˆ†ç¦» [#18452](https://github.com/ClickHouse/ClickHouse/issues/18452)

:::

```bash
# https://hub.docker.com/r/yandex/clickhouse-server/
# /etc/clickhouse-server/config.xml
docker run --rm -it \
  -v $PWD/data:/var/lib/clickhouse \
  -p 8123:8123 -p 9000:9000 \
  --ulimit nofile=262144:262144 \
  --name ch-server yandex/clickhouse-server

# Client
docker run -it --rm --link ch-server yandex/clickhouse-client --host ch-server

# å¯åŠ¨
clickhouse-server --config-file=/etc/clickhouse-server/config.xml
clickhouse-client --host=example.com

# å¯¼å…¥ CSV
cat my.csv | clickhouse-client --host=example-perftest01j --query="INSERT INTO rankings_tiny FORMAT CSV"
# å¯¼å…¥ TSV, å¹¶è®¡ç®—æ—¶é—´
time clickhouse-client --query="INSERT INTO trips FORMAT TabSeparated" < trips.tsv

curl 'http://localhost:8123/?query=SELECT%20NOW()'
```

```sql
COPY(
  SELECT
    t.id,
    t.name
  FROM
    t
) TO '/opt/data/export.tsv';

-- ä» PostgreSQL å¯¼å…¥
COPY...TO PROGRAM
```

## Note

- åˆ— DEFAULT materializes on merge, MATERIALIZED materializes on INSERT, OPTIMIZE TABLE
- ENGINE = Null - ETL è¡¨, trigger materialized view
- index_granularity
  - how many rows reps index key and pk
  - lower, point query æ›´å¿«
  - ä¾‹å¦‚ 8196

## å®‰è£…

```bash
# è¦æ±‚ SSE 4.2
grep -q sse4_2 /proc/cpuinfo && echo "SSE 4.2 supported" || echo "SSE 4.2 not supported"

# Docker
# https://hub.docker.com/r/clickhouse/clickhouse-server/
# /etc/clickhouse-server/config.xml
docker run -it --rm \
  --ulimit nofile=262144:262144 \
  -v=$HOME/data:/var/lib/clickhouse \
  --name clickhouse-server clickhouse/clickhouse-server
```

## Notes

- primary keys
  - å¹¶ä¸è¦æ±‚å”¯ä¸€
  - å°è±¡ç£ç›˜ä¸Šæ•°æ®æ’åº
- index granularity
  - 8,192 rows or 10MB of data
- sparse index

## æ•°æ®ç±»å‹

> æ”¯æŒèŒƒå‹çš„å¼ºç±»å‹ Schema

- UInt8, UInt16, UInt32, UInt64, UInt256, Int8, Int16, Int32, Int64, Int128, Int256
  - Int8 â€” TINYINT, BOOL, BOOLEAN, INT1.
  - Int16 â€” SMALLINT, INT2.
  - Int32 â€” INT, INT4, INTEGER.
  - Int64 â€” BIGINT
- Float32, Float64
  - FLOAT, DOUBLE
- Decimal(P, S), Decimal32(S), Decimal64(S), Decimal128(S), Decimal256(S)
  - P - precision - [ 1 : 76 ]
  - S - scale - [ 0 : P ]
  - P [ 1 : 9 ] - Decimal32(S)
  - P [ 10 : 18 ] - Decimal64(S)
  - P [ 19 : 38 ] - Decimal128(S)
  - P [ 39 : 76 ] - Decimal256(S)
- Boolean
  - UInt8 - 0,1
- String
  - å¦‚æœåˆ›å»º VARCHAR(255) ä¼šå¿½ç•¥é•¿åº¦
- FixedString(N) - N bytes
- UUID
- Date
  - 2byte, days since 1970-01-01
  - æœ€å¤§ 2148 å¹´
- `DateTime([timezone])`
  - unix timestamp
  - æœ€å¤§ 2105 å¹´
- `DateTime64(precision, [timezone])`
  - Int64, epoch
  - precision=3 åˆ™æ˜¯æ¯«ç§’çº§
- `Enum('k1'=1,'k2'=2)`
- LowCardinality(data_type)
  - change internal to dictionary-encoded
- array(T) - `[]`
- `AggregateFunction(name, types_of_argumentsâ€¦)`
- Nested - åµŒå¥—ç±»å‹ - ç±»ä¼¼å®šä¹‰ä¸€ä¸ª struct

## Table Engine

- PostgreSQL
- Kafka
- MySQL
- S3
  - Function+Engine

```sql
-- PostgreSQL
-- =======================
-- connect table
-- insert, select
CREATE TABLE
  db_in_ch.table1 (id UInt64, column1 String) ENGINE = PostgreSQL(
    'postgres-host.domain.com:5432',
    'db_in_psg',
    'table1',
    'clickhouse_user',
    'ClickHouse_123'
  );

-- materialized
-- replica of the database
SET
  allow_experimental_database_materialized_postgresql = 1;

CREATE DATABASE db1_postgres ENGINE = MaterializedPostgreSQL(
  'postgres-host.domain.com:5432',
  'db1',
  'clickhouse_user',
  'ClickHouse_123'
) SETTINGS materialized_postgresql_tables_list = 'table1';

-- S3
-- =========================
-- å…ƒä¿¡æ¯ _path, _file
SELECT
  *
FROM
  s3(
    'https://datasets-documentation.s3.eu-west-3.amazonaws.com/nyc-taxi/trips_*.gz',
    'TabSeparatedWithNames'
  )
LIMIT
  10;

-- å†™å…¥
INSERT INTO
  FUNCTION s3(
    'https://datasets-documentation.s3.eu-west-3.amazonaws.com/csv/trips.csv.lz4',
    's3_key',
    's3_secret',
    'CSV'
  )
SELECT
  *
FROM
  trips
LIMIT
  10000;

-- å†™å…¥å¤šä¸ªæ–‡ä»¶
INSERT INTO
  FUNCTION s3(
    'https://datasets-documentation.s3.eu-west-3.amazonaws.com/csv/trips_{_partition_id}.csv.lz4',
    's3_key',
    's3_secret',
    'CSV'
  ) PARTITION BY rand() % 10
SELECT
  *
FROM
  trips
LIMIT
  100000;

-- engine
CREATE TABLE
  trips_dest (
    `trip_id` UInt32,
    `pickup_date` Date,
    `pickup_datetime` DateTime,
    `dropoff_datetime` DateTime,
    `tip_amount` Float32,
    `total_amount` Float32
  ) ENGINE = S3('<bucket path>/trips.bin', 'Native');
```

## MergeTree

> MergeTree æœ¬èº«æ¯”è¾ƒé‡ï¼Œå»ºè®®ç”¨äºè¾ƒå¤§çš„æ•°æ®é›†ã€‚

- ä¸»è¦ç‰¹æ€§
  - åˆ—å­˜å‚¨
  - è‡ªå®šä¹‰åˆ†ç‰‡
  - sparse primary index
  - secondary data-skipping indexes

**MergeTree Engine Family**

- MergeTree - å»ºè®®ç”¨äºå•èŠ‚ç‚¹
  - åŸºäº PK æ’åº
  - æ”¯æŒå‰¯æœ¬
  - æ”¯æŒé‡‡æ ·
- ReplacingMergeTree - å»ºè®®ç”¨äºç”Ÿäº§åˆ†å¸ƒå¼é«˜å¯ç”¨
  - æ”¯æŒæ•°æ®å»é‡
- SummingMergeTree
- AggregatingMergeTree
- CollapsingMergeTree
- VersionedCollapsingMergeTree
- GraphiteMergeTree
- `tuple(T1, T2, ...)`
- ç‰¹æ®Šç±»å‹
  - è¡¨è¾¾å¼
  - Set
  - Nothing
  - Interval
- åŸŸç±»å‹
  - IPv4, IPv6
- Geo ç±»å‹
  - Point, Ring, Polygon, MultiPolygon
- Map(key, value)
- `SimpleAggregateFunction(name, types_of_argumentsâ€¦)`

## Log

- TinyLog
- StripeLog
- Log

### é›†æˆå¼•æ“

- ODBC,JDBC,MySQL,MongoDB, HDFS, S3, Kafka, RocksDB, RabbitMQ
- PostgreSQL
  - é€šè¿‡ `COPY (SELECT ...) TO STDOUT` å®ç°
  - æ”¯æŒ Materialized - åˆæ¬¡åŒæ­¥ååç»­é€šè¿‡ WAL æ›´æ–°

### ç‰¹æ®Šå¼•æ“

- Distributed
- MaterializedView
- Dictionary
- Merge
- File
- Null
- Set
- Join
- URL
- View
- Memory
- Buffer

## è¿ç»´

- æ¨è ext4 noatime, nobarrier

```bash
# CPU ä½¿ç”¨æ€§èƒ½æ¨¡å¼
echo 'performance' | sudo tee /sys/devices/system/cpu/cpu*/cpufreq/scaling_governor
# ä¸è¦ç¦ç”¨ overcommit
echo 0 | sudo tee /proc/sys/vm/overcommit_memory

# æ—§ Linux Kernel
echo 'madvise' | sudo tee /sys/kernel/mm/transparent_hugepage/enabled
```

### Auth

- Kerberos
- LDAP

## Stats

```sql
SELECT
  tabe,
  name,
  sub(data_compressed_bytes) AS compressed,
  sub(data_uncompressed_bytes) AS uncompressed,
  floor((compressed / uncompressed) * 100, 4) as percent
FROM
  system.columns
WHERE
  database = currentDatabase()
GROUP BY
  table,
  name
ORDER BY
  table ASC,
  name ASC;
```

## formats

- https://clickhouse.com/docs/en/interfaces/formats/

## config.xml

- https://clickhouse.com/docs/en/operations/configuration-files/

## åˆ é™¤

1. TTL
1. DELETE FROM
1. ALTER DELETE
1. DROP PARTITION
1. TRUNCATE

```sql
-- DELETE FROM
SET allow_experimental_lightweight_delete = true;
```

## å¯¼å‡ºæ•°æ®

```sql
SELECT * FROM table INTO OUTFILE 'file' FORMAT CSV
```

```bash
$ clickhouse-client --query "SELECT * from table" --format FormatName > result.txt
```

## å¯¼å…¥æ•°æ®

```bash
# HTTP API
echo '{"foo":"bar"}' | curl 'http://localhost:8123/?query=INSERT%20INTO%20test%20FORMAT%20JSONEachRow' --data-binary @-
# CLI
echo '{"foo":"bar"}' | clickhouse-client --query="INSERT INTO test FORMAT JSONEachRow"
```
