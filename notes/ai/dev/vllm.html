<!doctype html><html lang=en dir=ltr class="docs-wrapper plugin-docs plugin-id-default docs-version-current docs-doc-page docs-doc-id-ai/dev/vllm" data-has-hydrated=false><head><meta charset=UTF-8><meta name=generator content="Docusaurus v3.9.1"><title data-rh=true>vllm | Wener Live & Life</title><meta data-rh=true name=viewport content="width=device-width, initial-scale=1.0"/><meta data-rh=true name=twitter:card content=summary_large_image /><meta data-rh=true property=og:url content=https://wener.me/notes/ai/dev/vllm /><meta data-rh=true property=og:locale content=en /><meta data-rh=true name=docusaurus_locale content=en /><meta data-rh=true name=docsearch:language content=en /><meta data-rh=true name=docusaurus_version content=current /><meta data-rh=true name=docusaurus_tag content=docs-default-current /><meta data-rh=true name=docsearch:version content=current /><meta data-rh=true name=docsearch:docusaurus_tag content=docs-default-current /><meta data-rh=true property=og:title content="vllm | Wener Live & Life"/><meta data-rh=true name=description content="- vllm-project/vllm"/><meta data-rh=true property=og:description content="- vllm-project/vllm"/><link data-rh=true rel=icon href=/img/favicon.ico /><link data-rh=true rel=canonical href=https://wener.me/notes/ai/dev/vllm /><link data-rh=true rel=alternate href=https://wener.me/notes/ai/dev/vllm hreflang=en /><link data-rh=true rel=alternate href=https://wener.me/notes/ai/dev/vllm hreflang=x-default /><link data-rh=true rel=preconnect href=https://37P8DMWBKF-dsn.algolia.net crossorigin=anonymous /><script data-rh=true type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","item":"https://wener.me/notes/ai/","name":"AI","position":1},{"@type":"ListItem","item":"https://wener.me/notes/ai/dev/vllm","name":"vllm","position":2}]}</script><link rel=alternate type=application/rss+xml href=/story/rss.xml title="Wener Live & Life RSS Feed"><link rel=alternate type=application/atom+xml href=/story/atom.xml title="Wener Live & Life Atom Feed"><link rel=preconnect href=https://www.google-analytics.com><script>window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)},ga.l=+new Date,ga("create","UA-30404720-1","auto"),ga("send","pageview")</script><script async src=https://www.google-analytics.com/analytics.js></script><link rel=preconnect href=https://www.google-analytics.com><link rel=preconnect href=https://www.googletagmanager.com><script async src="https://www.googletagmanager.com/gtag/js?id=UA-30404720-1"></script><script>function gtag(){dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","UA-30404720-1",{})</script><link rel=search type=application/opensearchdescription+xml title="Wener Live & Life" href=/opensearch.xml><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.13.24/dist/katex.min.css><script src=https://static.cloudflareinsights.com/beacon.min.js async data-cf-beacon='{"token": "e9a1b931103044f3940ee67b78c7df70"}' defer></script><link rel=stylesheet href=/assets/css/styles.fa221ee1.css /><script src=/assets/js/runtime~main.72c320db.js defer></script><script src=/assets/js/main.c07313b6.js defer></script></head><body class=navigation-with-keyboard><svg style="display: none;"><defs>
<symbol id=theme-svg-external-link viewBox="0 0 24 24"><path fill=currentColor d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"/></symbol>
</defs></svg>
<script>!function(){var t=function(){try{return new URLSearchParams(window.location.search).get("docusaurus-theme")}catch(t){}}()||function(){try{return window.localStorage.getItem("theme")}catch(t){}}();document.documentElement.setAttribute("data-theme",t||"light"),document.documentElement.setAttribute("data-theme-choice",t||"light")}(),function(){try{for(var[t,e]of new URLSearchParams(window.location.search).entries())if(t.startsWith("docusaurus-data-")){var a=t.replace("docusaurus-data-","data-");document.documentElement.setAttribute(a,e)}}catch(t){}}()</script><div id=__docusaurus><div role=region aria-label="Skip to main content"><a class=skipToContent_jKDA href=#__docusaurus_skipToContent_fallback>Skip to main content</a></div><nav aria-label=Main class="theme-layout-navbar navbar navbar--fixed-top"><div class=navbar__inner><div class="theme-layout-navbar-left navbar__items"><button aria-label="Toggle navigation bar" aria-expanded=false class="navbar__toggle clean-btn" type=button><svg width=30 height=30 viewBox="0 0 30 30" aria-hidden=true><path stroke=currentColor stroke-linecap=round stroke-miterlimit=10 stroke-width=2 d="M4 7h22M4 15h22M4 23h22"/></svg></button><a class=navbar__brand href=/><div class=navbar__logo><img src=/img/wener-logo-head.svg alt="Wener Logo" class="themedComponent_j2y5 themedComponent--light_v877"/><img src=/img/wener-logo-head.svg alt="Wener Logo" class="themedComponent_j2y5 themedComponent--dark_PUQY"/></div><b class="navbar__title text--truncate">Wener</b></a><a aria-current=page class="navbar__item navbar__link navbar__link--active" href=/notes>笔记</a><a class="navbar__item navbar__link" href=/story>故事</a><a class="navbar__item navbar__link" href=/notes/howto/network/dns-prevent-spoofing>指南</a></div><div class="theme-layout-navbar-right navbar__items navbar__items--right"><a href=https://github.com/wenerme/wener target=_blank rel="noopener noreferrer" class="navbar__item navbar__link">GitHub<svg width=13.5 height=13.5 aria-label="(opens in new tab)" class=iconExternalLink_kUGX><use href=#theme-svg-external-link /></svg></a><div class="toggle_N7Mq colorModeToggle_vh_y"><button class="clean-btn toggleButton_PaHe toggleButtonDisabled_RC4w" type=button disabled title="system mode" aria-label="Switch between dark and light mode (currently system mode)"><svg viewBox="0 0 24 24" width=24 height=24 aria-hidden=true class="toggleIcon_Q188 lightToggleIcon_M1mU"><path fill=currentColor d="M12,9c1.65,0,3,1.35,3,3s-1.35,3-3,3s-3-1.35-3-3S10.35,9,12,9 M12,7c-2.76,0-5,2.24-5,5s2.24,5,5,5s5-2.24,5-5 S14.76,7,12,7L12,7z M2,13l2,0c0.55,0,1-0.45,1-1s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S1.45,13,2,13z M20,13l2,0c0.55,0,1-0.45,1-1 s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S19.45,13,20,13z M11,2v2c0,0.55,0.45,1,1,1s1-0.45,1-1V2c0-0.55-0.45-1-1-1S11,1.45,11,2z M11,20v2c0,0.55,0.45,1,1,1s1-0.45,1-1v-2c0-0.55-0.45-1-1-1C11.45,19,11,19.45,11,20z M5.99,4.58c-0.39-0.39-1.03-0.39-1.41,0 c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0s0.39-1.03,0-1.41L5.99,4.58z M18.36,16.95 c-0.39-0.39-1.03-0.39-1.41,0c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0c0.39-0.39,0.39-1.03,0-1.41 L18.36,16.95z M19.42,5.99c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06c-0.39,0.39-0.39,1.03,0,1.41 s1.03,0.39,1.41,0L19.42,5.99z M7.05,18.36c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06 c-0.39,0.39-0.39,1.03,0,1.41s1.03,0.39,1.41,0L7.05,18.36z"/></svg><svg viewBox="0 0 24 24" width=24 height=24 aria-hidden=true class="toggleIcon_Q188 darkToggleIcon_sEdo"><path fill=currentColor d="M9.37,5.51C9.19,6.15,9.1,6.82,9.1,7.5c0,4.08,3.32,7.4,7.4,7.4c0.68,0,1.35-0.09,1.99-0.27C17.45,17.19,14.93,19,12,19 c-3.86,0-7-3.14-7-7C5,9.07,6.81,6.55,9.37,5.51z M12,3c-4.97,0-9,4.03-9,9s4.03,9,9,9s9-4.03,9-9c0-0.46-0.04-0.92-0.1-1.36 c-0.98,1.37-2.58,2.26-4.4,2.26c-2.98,0-5.4-2.42-5.4-5.4c0-1.81,0.89-3.42,2.26-4.4C12.92,3.04,12.46,3,12,3L12,3z"/></svg><svg viewBox="0 0 24 24" width=24 height=24 aria-hidden=true class="toggleIcon_Q188 systemToggleIcon_cOyE"><path fill=currentColor d="m12 21c4.971 0 9-4.029 9-9s-4.029-9-9-9-9 4.029-9 9 4.029 9 9 9zm4.95-13.95c1.313 1.313 2.05 3.093 2.05 4.95s-0.738 3.637-2.05 4.95c-1.313 1.313-3.093 2.05-4.95 2.05v-14c1.857 0 3.637 0.737 4.95 2.05z"/></svg></button></div><div class=navbarSearchContainer_d_Bn><button type=button class="DocSearch DocSearch-Button" aria-label="Search (Meta+k)" aria-keyshortcuts=Meta+k><span class=DocSearch-Button-Container><svg width=20 height=20 class=DocSearch-Search-Icon viewBox="0 0 24 24" aria-hidden=true><circle cx=11 cy=11 r=8 stroke=currentColor fill=none stroke-width=1.4 /><path d="m21 21-4.3-4.3" stroke=currentColor fill=none stroke-linecap=round stroke-linejoin=round /></svg><span class=DocSearch-Button-Placeholder>Search</span></span><span class=DocSearch-Button-Keys></span></button></div></div></div><div role=presentation class=navbar-sidebar__backdrop></div></nav><div id=__docusaurus_skipToContent_fallback class="theme-layout-main main-wrapper mainWrapper_c22C"><div class=docsWrapper_oymo><button aria-label="Scroll back to top" class="clean-btn theme-back-to-top-button backToTopButton_ZPA9" type=button></button><div class=docRoot_ng2Q><aside class="theme-doc-sidebar-container docSidebarContainer_nyHU"><div class=sidebarViewport_rLoj><div class=sidebar_TmDF><nav aria-label="Docs sidebar" class="menu thin-scrollbar menu_jRin"><ul class="theme-doc-sidebar-menu menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class=menu__link href=/notes><span title=笔记 class=linkLabel_JPPZ>笔记</span></a><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="categoryLink_irRK menu__link menu__link--sublist menu__link--sublist-caret" role=button aria-expanded=false href=/notes/adobe/photoshop><span title=adobe class=categoryLinkLabel_RkOu>adobe</span></a></div><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item"><div class=menu__list-item-collapsible><a class="categoryLink_irRK menu__link menu__link--sublist menu__link--active" href=/notes/ai><span title=AI class=categoryLinkLabel_RkOu>AI</span></a><button aria-label="Collapse sidebar category 'AI'" aria-expanded=true type=button class="clean-btn menu__caret"></button></div><ul class=menu__list><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class=menu__link tabindex=0 href=/notes/ai/awesome><span title="AI Awesome" class=linkLabel_JPPZ>AI Awesome</span></a><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class=menu__link tabindex=0 href=/notes/ai/faq><span title="AI FAQ" class=linkLabel_JPPZ>AI FAQ</span></a><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class=menu__link tabindex=0 href=/notes/ai/glossary><span title="AI Glossary" class=linkLabel_JPPZ>AI Glossary</span></a><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="categoryLink_irRK menu__link menu__link--sublist menu__link--sublist-caret" role=button aria-expanded=false tabindex=0 href=/notes/ai/coding/continue><span title=coding class=categoryLinkLabel_RkOu>coding</span></a></div><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item"><div class=menu__list-item-collapsible><a class="categoryLink_irRK menu__link menu__link--sublist menu__link--sublist-caret menu__link--active" role=button aria-expanded=true tabindex=0 href=/notes/ai/dev/agent/awesome><span title=dev class=categoryLinkLabel_RkOu>dev</span></a></div><ul class=menu__list><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-3 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="categoryLink_irRK menu__link menu__link--sublist menu__link--sublist-caret" role=button aria-expanded=false tabindex=0 href=/notes/ai/dev/agent/awesome><span title=agent class=categoryLinkLabel_RkOu>agent</span></a></div><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class=menu__link tabindex=0 href=/notes/ai/dev/agents><span title=AGENTS.md class=linkLabel_JPPZ>AGENTS.md</span></a><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class=menu__link tabindex=0 href=/notes/ai/dev/awesome><span title="AI Dev Awesome" class=linkLabel_JPPZ>AI Dev Awesome</span></a><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class=menu__link tabindex=0 href=/notes/ai/dev/faq><span title="AI Dev FAQ" class=linkLabel_JPPZ>AI Dev FAQ</span></a><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class=menu__link tabindex=0 href=/notes/ai/dev/bricksllm><span title=BricksLLM class=linkLabel_JPPZ>BricksLLM</span></a><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class=menu__link tabindex=0 href=/notes/ai/dev/chatgpt-next-web><span title=ChatGPT-Next-Web class=linkLabel_JPPZ>ChatGPT-Next-Web</span></a><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class=menu__link tabindex=0 href=/notes/ai/dev/comfyui><span title=ComfyUI class=linkLabel_JPPZ>ComfyUI</span></a><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class=menu__link tabindex=0 href=/notes/ai/dev/copilotkit><span title=CopilotKit class=linkLabel_JPPZ>CopilotKit</span></a><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class=menu__link tabindex=0 href=/notes/ai/dev/cuda><span title=FAQ class=linkLabel_JPPZ>FAQ</span></a><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class=menu__link tabindex=0 href=/notes/ai/dev/langchain><span title=langchain class=linkLabel_JPPZ>langchain</span></a><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-3 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="categoryLink_irRK menu__link menu__link--sublist" tabindex=0 href=/notes/ai/dev/litellm><span title=LiteLLM class=categoryLinkLabel_RkOu>LiteLLM</span></a><button aria-label="Expand sidebar category 'LiteLLM'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class=menu__link tabindex=0 href=/notes/ai/dev/llama.cpp><span title=llama.cpp class=linkLabel_JPPZ>llama.cpp</span></a><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class=menu__link tabindex=0 href=/notes/ai/dev/localai><span title=LocalAI class=linkLabel_JPPZ>LocalAI</span></a><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-3 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="categoryLink_irRK menu__link menu__link--sublist" tabindex=0 href=/notes/ai/dev/mcp><span title=MCP class=categoryLinkLabel_RkOu>MCP</span></a><button aria-label="Expand sidebar category 'MCP'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class=menu__link tabindex=0 href=/notes/ai/dev/ollama><span title=ollama class=linkLabel_JPPZ>ollama</span></a><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class=menu__link tabindex=0 href=/notes/ai/dev/rag><span title=RAG class=linkLabel_JPPZ>RAG</span></a><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class=menu__link tabindex=0 href=/notes/ai/dev/repomix><span title=Repomix class=linkLabel_JPPZ>Repomix</span></a><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class=menu__link tabindex=0 href=/notes/ai/dev/sglang><span title=sglang class=linkLabel_JPPZ>sglang</span></a><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class=menu__link tabindex=0 href=/notes/ai/dev/tgi><span title="Text Generation Inference" class=linkLabel_JPPZ>Text Generation Inference</span></a><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link menu__link--active" aria-current=page tabindex=0 href=/notes/ai/dev/vllm><span title=vllm class=linkLabel_JPPZ>vllm</span></a></ul><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="categoryLink_irRK menu__link menu__link--sublist" tabindex=0 href=/notes/ai/diffusion><span title=Diffusion class=categoryLinkLabel_RkOu>Diffusion</span></a><button aria-label="Expand sidebar category 'Diffusion'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class=menu__link tabindex=0 href=/notes/ai/embedding><span title=Embedding class=linkLabel_JPPZ>Embedding</span></a><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="categoryLink_irRK menu__link menu__link--sublist" tabindex=0 href=/notes/ai/gan><span title=GANs class=categoryLinkLabel_RkOu>GANs</span></a><button aria-label="Expand sidebar category 'GANs'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="categoryLink_irRK menu__link menu__link--sublist" tabindex=0 href=/notes/ai/gpt><span title=GPT class=categoryLinkLabel_RkOu>GPT</span></a><button aria-label="Expand sidebar category 'GPT'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="categoryLink_irRK menu__link menu__link--sublist" tabindex=0 href=/notes/ai/inference><span title=推理 class=categoryLinkLabel_RkOu>推理</span></a><button aria-label="Expand sidebar category '推理'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="categoryLink_irRK menu__link menu__link--sublist" tabindex=0 href=/notes/ai/llm><span title=LLM class=categoryLinkLabel_RkOu>LLM</span></a><button aria-label="Expand sidebar category 'LLM'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="categoryLink_irRK menu__link menu__link--sublist" tabindex=0 href=/notes/ai/ml><span title=机器学习 class=categoryLinkLabel_RkOu>机器学习</span></a><button aria-label="Expand sidebar category '机器学习'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="categoryLink_irRK menu__link menu__link--sublist menu__link--sublist-caret" role=button aria-expanded=false tabindex=0 href=/notes/ai/model/awesome><span title=model class=categoryLinkLabel_RkOu>model</span></a></div><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class=menu__link tabindex=0 href=/notes/ai/ner><span title="Named Entity Recognition" class=linkLabel_JPPZ>Named Entity Recognition</span></a><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="categoryLink_irRK menu__link menu__link--sublist" tabindex=0 href=/notes/ai/nlp><span title=NLP class=categoryLinkLabel_RkOu>NLP</span></a><button aria-label="Expand sidebar category 'NLP'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="categoryLink_irRK menu__link menu__link--sublist" tabindex=0 href=/notes/ai/ocr><span title=OCR class=categoryLinkLabel_RkOu>OCR</span></a><button aria-label="Expand sidebar category 'OCR'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class=menu__link tabindex=0 href=/notes/ai/precision><span title=精度 class=linkLabel_JPPZ>精度</span></a><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class=menu__link tabindex=0 href=/notes/ai/prompt-awesome><span title="Prompt Awesome" class=linkLabel_JPPZ>Prompt Awesome</span></a><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="categoryLink_irRK menu__link menu__link--sublist" tabindex=0 href=/notes/ai/service><span title="AI Service" class=categoryLinkLabel_RkOu>AI Service</span></a><button aria-label="Expand sidebar category 'AI Service'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="categoryLink_irRK menu__link menu__link--sublist" tabindex=0 href=/notes/ai/traning><span title=训练 class=categoryLinkLabel_RkOu>训练</span></a><button aria-label="Expand sidebar category '训练'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class=menu__link tabindex=0 href=/notes/ai/vision><span title=视觉 class=linkLabel_JPPZ>视觉</span></a><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="categoryLink_irRK menu__link menu__link--sublist" tabindex=0 href=/notes/ai/voice><span title=Voice class=categoryLinkLabel_RkOu>Voice</span></a><button aria-label="Expand sidebar category 'Voice'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div></ul><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="categoryLink_irRK menu__link menu__link--sublist" href=/notes/algorithm><span title=算法 class=categoryLinkLabel_RkOu>算法</span></a><button aria-label="Expand sidebar category '算法'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="categoryLink_irRK menu__link menu__link--sublist" href=/notes/blockchain><span title=区块链 class=categoryLinkLabel_RkOu>区块链</span></a><button aria-label="Expand sidebar category '区块链'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="categoryLink_irRK menu__link menu__link--sublist" href=/notes/cn><span title=CN class=categoryLinkLabel_RkOu>CN</span></a><button aria-label="Expand sidebar category 'CN'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="categoryLink_irRK menu__link menu__link--sublist" href=/notes/courses><span title=课程 class=categoryLinkLabel_RkOu>课程</span></a><button aria-label="Expand sidebar category '课程'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="categoryLink_irRK menu__link menu__link--sublist" href=/notes/culture><span title=文化 class=categoryLinkLabel_RkOu>文化</span></a><button aria-label="Expand sidebar category '文化'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="categoryLink_irRK menu__link menu__link--sublist" href=/notes/db><span title=数据库 class=categoryLinkLabel_RkOu>数据库</span></a><button aria-label="Expand sidebar category '数据库'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="categoryLink_irRK menu__link menu__link--sublist" href=/notes/dev><span title=开发 class=categoryLinkLabel_RkOu>开发</span></a><button aria-label="Expand sidebar category '开发'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="categoryLink_irRK menu__link menu__link--sublist" href=/notes/devops><span title=DevOps class=categoryLinkLabel_RkOu>DevOps</span></a><button aria-label="Expand sidebar category 'DevOps'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="categoryLink_irRK menu__link menu__link--sublist" href=/notes/economics><span title=经济学 class=categoryLinkLabel_RkOu>经济学</span></a><button aria-label="Expand sidebar category '经济学'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="categoryLink_irRK menu__link menu__link--sublist" href=/notes/electrical><span title=电学基础 class=categoryLinkLabel_RkOu>电学基础</span></a><button aria-label="Expand sidebar category '电学基础'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="categoryLink_irRK menu__link menu__link--sublist menu__link--sublist-caret" role=button aria-expanded=false href=/notes/embedded/awesome><span title=embedded class=categoryLinkLabel_RkOu>embedded</span></a></div><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="categoryLink_irRK menu__link menu__link--sublist" href=/notes/evolve><span title=自我成长 class=categoryLinkLabel_RkOu>自我成长</span></a><button aria-label="Expand sidebar category '自我成长'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="categoryLink_irRK menu__link menu__link--sublist" href=/notes/hardware><span title=硬件 class=categoryLinkLabel_RkOu>硬件</span></a><button aria-label="Expand sidebar category '硬件'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="categoryLink_irRK menu__link menu__link--sublist" href=/notes/healthcare><span title=健康 class=categoryLinkLabel_RkOu>健康</span></a><button aria-label="Expand sidebar category '健康'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="categoryLink_irRK menu__link menu__link--sublist menu__link--sublist-caret" role=button aria-expanded=false href=/notes/howto/network/dns-prevent-spoofing><span title=howto class=categoryLinkLabel_RkOu>howto</span></a></div><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="categoryLink_irRK menu__link menu__link--sublist" href=/notes/java><span title=Java class=categoryLinkLabel_RkOu>Java</span></a><button aria-label="Expand sidebar category 'Java'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="categoryLink_irRK menu__link menu__link--sublist" href=/notes/languages><span title=语言 class=categoryLinkLabel_RkOu>语言</span></a><button aria-label="Expand sidebar category '语言'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class=menu__link href=/notes/linguistics><span title=Linguistics class=linkLabel_JPPZ>Linguistics</span></a><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="categoryLink_irRK menu__link menu__link--sublist" href=/notes/math><span title=数学 class=categoryLinkLabel_RkOu>数学</span></a><button aria-label="Expand sidebar category '数学'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="categoryLink_irRK menu__link menu__link--sublist" href=/notes/mgmt><span title=管理 class=categoryLinkLabel_RkOu>管理</span></a><button aria-label="Expand sidebar category '管理'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="categoryLink_irRK menu__link menu__link--sublist menu__link--sublist-caret" role=button aria-expanded=false href=/notes/network/application/dns><span title=network class=categoryLinkLabel_RkOu>network</span></a></div><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="categoryLink_irRK menu__link menu__link--sublist" href=/notes/ops><span title=运维 class=categoryLinkLabel_RkOu>运维</span></a><button aria-label="Expand sidebar category '运维'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="categoryLink_irRK menu__link menu__link--sublist" href=/notes/os><span title=操作系统 class=categoryLinkLabel_RkOu>操作系统</span></a><button aria-label="Expand sidebar category '操作系统'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="categoryLink_irRK menu__link menu__link--sublist" href=/notes/philosophy><span title=理念 class=categoryLinkLabel_RkOu>理念</span></a><button aria-label="Expand sidebar category '理念'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="categoryLink_irRK menu__link menu__link--sublist" href=/notes/photography><span title=摄影 class=categoryLinkLabel_RkOu>摄影</span></a><button aria-label="Expand sidebar category '摄影'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="categoryLink_irRK menu__link menu__link--sublist" href=/notes/platform><span title=平台 class=categoryLinkLabel_RkOu>平台</span></a><button aria-label="Expand sidebar category '平台'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="categoryLink_irRK menu__link menu__link--sublist menu__link--sublist-caret" role=button aria-expanded=false href=/notes/psychology/glossary><span title=psychology class=categoryLinkLabel_RkOu>psychology</span></a></div><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="categoryLink_irRK menu__link menu__link--sublist" href=/notes/queue><span title=Queue class=categoryLinkLabel_RkOu>Queue</span></a><button aria-label="Expand sidebar category 'Queue'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="categoryLink_irRK menu__link menu__link--sublist" href=/notes/security><span title=安全 class=categoryLinkLabel_RkOu>安全</span></a><button aria-label="Expand sidebar category '安全'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="categoryLink_irRK menu__link menu__link--sublist" href=/notes/service><span title=服务 class=categoryLinkLabel_RkOu>服务</span></a><button aria-label="Expand sidebar category '服务'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="categoryLink_irRK menu__link menu__link--sublist" href=/notes/software><span title=软件 class=categoryLinkLabel_RkOu>软件</span></a><button aria-label="Expand sidebar category '软件'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="categoryLink_irRK menu__link menu__link--sublist" href=/notes/std><span title=标准数据 class=categoryLinkLabel_RkOu>标准数据</span></a><button aria-label="Expand sidebar category '标准数据'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="categoryLink_irRK menu__link menu__link--sublist" href=/notes/voip><span title=VoIP class=categoryLinkLabel_RkOu>VoIP</span></a><button aria-label="Expand sidebar category 'VoIP'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="categoryLink_irRK menu__link menu__link--sublist" href=/notes/web><span title=Web class=categoryLinkLabel_RkOu>Web</span></a><button aria-label="Expand sidebar category 'Web'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div></ul></nav><button type=button title="Collapse sidebar" aria-label="Collapse sidebar" class="button button--secondary button--outline collapseSidebarButton_NY7h"><svg width=20 height=20 aria-hidden=true class=collapseSidebarButtonIcon_c8nQ><g fill=#7a7a7a><path d="M9.992 10.023c0 .2-.062.399-.172.547l-4.996 7.492a.982.982 0 01-.828.454H1c-.55 0-1-.453-1-1 0-.2.059-.403.168-.551l4.629-6.942L.168 3.078A.939.939 0 010 2.528c0-.548.45-.997 1-.997h2.996c.352 0 .649.18.828.45L9.82 9.472c.11.148.172.347.172.55zm0 0"/><path d="M19.98 10.023c0 .2-.058.399-.168.547l-4.996 7.492a.987.987 0 01-.828.454h-3c-.547 0-.996-.453-.996-1 0-.2.059-.403.168-.551l4.625-6.942-4.625-6.945a.939.939 0 01-.168-.55 1 1 0 01.996-.997h3c.348 0 .649.18.828.45l4.996 7.492c.11.148.168.347.168.55zm0 0"/></g></svg></button></div></div></aside><main class=docMainContainer_GAGP><div class="container padding-top--md padding-bottom--lg"><div class=row><div class="col docItemCol_YQ1L"><div class=docItemContainer_Wjg4><article><nav class="theme-doc-breadcrumbs breadcrumbsContainer_tiqF" aria-label=Breadcrumbs><ul class=breadcrumbs><li class=breadcrumbs__item><a aria-label="Home page" class=breadcrumbs__link href=/><svg viewBox="0 0 24 24" class=breadcrumbHomeIcon_WTWT><path d="M10 19v-5h4v5c0 .55.45 1 1 1h3c.55 0 1-.45 1-1v-7h1.7c.46 0 .68-.57.33-.87L12.67 3.6c-.38-.34-.96-.34-1.34 0l-8.36 7.53c-.34.3-.13.87.33.87H5v7c0 .55.45 1 1 1h3c.55 0 1-.45 1-1z" fill=currentColor /></svg></a><li class=breadcrumbs__item><a class=breadcrumbs__link href=/notes/ai><span>AI</span></a><li class=breadcrumbs__item><span class=breadcrumbs__link>dev</span><li class="breadcrumbs__item breadcrumbs__item--active"><span class=breadcrumbs__link>vllm</span></ul></nav><div class="tocCollapsible_GvsH theme-doc-toc-mobile tocMobile_qJgM"><button type=button class="clean-btn tocCollapsibleButton_LnP0">On this page</button></div><div class="theme-doc-markdown markdown"><header><h1>vllm</h1></header>
<ul>
<li><a href=https://github.com/vllm-project/vllm target=_blank rel="noopener noreferrer">vllm-project/vllm</a>
<ul>
<li>Apache-2.0, Python, CUDA, Linux</li>
</ul>
</li>
</ul>
<div class="theme-admonition theme-admonition-caution admonition_phik alert alert--warning"><div class=admonitionHeading_zxjO><span class=admonitionIcon_bSNx><svg viewBox="0 0 16 16"><path fill-rule=evenodd d="M8.893 1.5c-.183-.31-.52-.5-.887-.5s-.703.19-.886.5L.138 13.499a.98.98 0 0 0 0 1.001c.193.31.53.501.886.501h13.964c.367 0 .704-.19.877-.5a1.03 1.03 0 0 0 .01-1.002L8.893 1.5zm.133 11.497H6.987v-2.003h2.039v2.003zm0-3.004H6.987V5.987h2.039v4.006z"/></svg></span>caution</div><div class=admonitionContent_R6P7><ul>
<li>不支持 macOS <a href=https://github.com/vllm-project/vllm/issues/1441 target=_blank rel="noopener noreferrer">vllm-project/vllm#1441</a>
<ul>
<li>docker x86 跑可以, 但是性能差</li>
</ul>
</li>
<li>大多新模型需要 CUDA 7.5+ / 2018 Nvidia Turing+</li>
</ul></div></div>
<div class="language-bash codeBlockContainer_ljD1 theme-code-block" style=--prism-color:#bfc7d5;--prism-background-color:#292d3e><div class=codeBlockContent_rhaG><pre tabindex=0 class="prism-code language-bash codeBlock_mx9Q thin-scrollbar" style=color:#bfc7d5;background-color:#292d3e><code class=codeBlockLines_NG8l><span class=token-line style=color:#bfc7d5><span class="token plain"># --shm-size or --ipc=host</span><br/></span><span class=token-line style=color:#bfc7d5><span class="token plain">docker run --rm -it \</span><br/></span><span class=token-line style=color:#bfc7d5><span class="token plain">  --runtime nvidia --gpus all \</span><br/></span><span class=token-line style=color:#bfc7d5><span class="token plain">  -p 8000:8000 \</span><br/></span><span class=token-line style=color:#bfc7d5><span class="token plain">  --ipc=host \</span><br/></span><span class=token-line style=color:#bfc7d5><span class="token plain">  -v ~/.cache/huggingface:/root/.cache/huggingface \</span><br/></span><span class=token-line style=color:#bfc7d5><span class="token plain">  --entrypoint bash \</span><br/></span><span class=token-line style=color:#bfc7d5><span class="token plain">  --name vllm vllm/vllm-openai:v0.8.5</span><br/></span><span class=token-line style=color:#bfc7d5><span class="token plain" style=display:inline-block></span><br/></span><span class=token-line style=color:#bfc7d5><span class="token plain"># 目前默认开启</span><br/></span><span class=token-line style=color:#bfc7d5><span class="token plain">export VLLM_USE_V1=1</span><br/></span><span class=token-line style=color:#bfc7d5><span class="token plain">vllm serve</span><br/></span><span class=token-line style=color:#bfc7d5><span class="token plain" style=display:inline-block></span><br/></span><span class=token-line style=color:#bfc7d5><span class="token plain">vllm -v</span><br/></span><span class=token-line style=color:#bfc7d5><span class="token plain" style=display:inline-block></span><br/></span><span class=token-line style=color:#bfc7d5><span class="token plain">curl -v https://huggingface.co                       # 确保 HF 访问正常</span><br/></span><span class=token-line style=color:#bfc7d5><span class="token plain">huggingface-cli download Qwen/Qwen2.5-VL-7B-Instruct # 手动下载</span><br/></span><span class=token-line style=color:#bfc7d5><span class="token plain" style=display:inline-block></span><br/></span><span class=token-line style=color:#bfc7d5><span class="token plain">vllm serve Qwen/Qwen2.5-VL-7B-Instruct --dtype auto --api-key token-abc123</span><br/></span><span class=token-line style=color:#bfc7d5><span class="token plain">vllm serve NousResearch/Meta-Llama-3-8B-Instruct --dtype auto --api-key token-abc123</span><br/></span><span class=token-line style=color:#bfc7d5><span class="token plain" style=display:inline-block></span><br/></span><span class=token-line style=color:#bfc7d5><span class="token plain">curl http://localhost:8000/v1/models</span><br/></span><span class=token-line style=color:#bfc7d5><span class="token plain">curl http://localhost:8000/v1/completions \</span><br/></span><span class=token-line style=color:#bfc7d5><span class="token plain">  -H "Content-Type: application/json" \</span><br/></span><span class=token-line style=color:#bfc7d5><span class="token plain">  -d '{</span><br/></span><span class=token-line style=color:#bfc7d5><span class="token plain">        "model": "Qwen/Qwen2.5-7B-Instruct-GPTQ-Int4",</span><br/></span><span class=token-line style=color:#bfc7d5><span class="token plain">        "prompt": "Today is",</span><br/></span><span class=token-line style=color:#bfc7d5><span class="token plain">        "max_tokens": 100,</span><br/></span><span class=token-line style=color:#bfc7d5><span class="token plain">        "temperature": 0</span><br/></span><span class=token-line style=color:#bfc7d5><span class="token plain">    }'</span><br/></span><span class=token-line style=color:#bfc7d5><span class="token plain" style=display:inline-block></span><br/></span><span class=token-line style=color:#bfc7d5><span class="token plain">curl http://localhost:8000/v1/chat/completions \</span><br/></span><span class=token-line style=color:#bfc7d5><span class="token plain">  -H "Content-Type: application/json" \</span><br/></span><span class=token-line style=color:#bfc7d5><span class="token plain">  -d '{</span><br/></span><span class=token-line style=color:#bfc7d5><span class="token plain">        "model": "Qwen/Qwen2.5-7B-Instruct-GPTQ-Int4",</span><br/></span><span class=token-line style=color:#bfc7d5><span class="token plain">        "messages": [</span><br/></span><span class=token-line style=color:#bfc7d5><span class="token plain">            {"role": "system", "content": "You are a helpful assistant."},</span><br/></span><span class=token-line style=color:#bfc7d5><span class="token plain">            {"role": "user", "content": "Who won the world series in 2020?"}</span><br/></span><span class=token-line style=color:#bfc7d5><span class="token plain">        ]</span><br/></span><span class=token-line style=color:#bfc7d5><span class="token plain">    }'</span><br/></span><span class=token-line style=color:#bfc7d5><span class="token plain" style=display:inline-block></span><br/></span><span class=token-line style=color:#bfc7d5><span class="token plain">docker run --rm -it \</span><br/></span><span class=token-line style=color:#bfc7d5><span class="token plain">  --runtime nvidia --gpus all \</span><br/></span><span class=token-line style=color:#bfc7d5><span class="token plain">  -p 8000:8000 \</span><br/></span><span class=token-line style=color:#bfc7d5><span class="token plain">  --ipc=host \</span><br/></span><span class=token-line style=color:#bfc7d5><span class="token plain">  -v ~/.cache/huggingface:/root/.cache/huggingface \</span><br/></span><span class=token-line style=color:#bfc7d5><span class="token plain">  --entrypoint bash \</span><br/></span><span class=token-line style=color:#bfc7d5><span class="token plain">  --name vllm vllm/vllm-openai:v0.8.5 \</span><br/></span><span class=token-line style=color:#bfc7d5><span class="token plain">  --model Qwen/Qwen2.5-7B-Instruct-GPTQ-Int4 \</span><br/></span><span class=token-line style=color:#bfc7d5><span class="token plain">  --enable-sleep-mode \</span><br/></span><span class=token-line style=color:#bfc7d5><span class="token plain">  --dtype float16</span><br/></span><span class=token-line style=color:#bfc7d5><span class="token plain" style=display:inline-block></span><br/></span><span class=token-line style=color:#bfc7d5><span class="token plain">VLLM_USE_PRECOMPILED=1 pip install git+https://github.com/vllm-project/vllm.git</span><br/></span></code></pre></div></div>
<ul>
<li>参考
<ul>
<li><a href=https://docs.vllm.ai/en/v0.8.5/serving/engine_args.html target=_blank rel="noopener noreferrer">https://docs.vllm.ai/en/v0.8.5/serving/engine_args.html</a></li>
</ul>
</li>
</ul>
<div class="language-py codeBlockContainer_ljD1 theme-code-block" style=--prism-color:#bfc7d5;--prism-background-color:#292d3e><div class=codeBlockContent_rhaG><pre tabindex=0 class="prism-code language-py codeBlock_mx9Q thin-scrollbar" style=color:#bfc7d5;background-color:#292d3e><code class=codeBlockLines_NG8l><span class=token-line style=color:#bfc7d5><span class="token keyword" style=font-style:italic>from</span><span class="token plain"> vllm </span><span class="token keyword" style=font-style:italic>import</span><span class="token plain"> LLM</span><br/></span><span class=token-line style=color:#bfc7d5><span class="token plain" style=display:inline-block></span><br/></span><span class=token-line style=color:#bfc7d5><span class="token plain">llm </span><span class="token operator" style="color:rgb(137, 221, 255)">=</span><span class="token plain"> LLM</span><span class="token punctuation" style="color:rgb(199, 146, 234)">(</span><span class="token plain"></span><br/></span><span class=token-line style=color:#bfc7d5><span class="token plain">    </span><span class="token string" style="color:rgb(195, 232, 141)">"Qwen/Qwen2.5-VL-7B-Instruct"</span><span class="token punctuation" style="color:rgb(199, 146, 234)">,</span><span class="token plain"></span><br/></span><span class=token-line style=color:#bfc7d5><span class="token plain">    max_num_seqs</span><span class="token operator" style="color:rgb(137, 221, 255)">=</span><span class="token number" style="color:rgb(247, 140, 108)">1</span><span class="token punctuation" style="color:rgb(199, 146, 234)">,</span><span class="token plain"> </span><span class="token comment" style="color:rgb(105, 112, 152);font-style:italic"># 1+</span><span class="token plain"></span><br/></span><span class=token-line style=color:#bfc7d5><span class="token plain">    gpu_memory_utilization</span><span class="token operator" style="color:rgb(137, 221, 255)">=</span><span class="token number" style="color:rgb(247, 140, 108)">0.9</span><span class="token punctuation" style="color:rgb(199, 146, 234)">,</span><span class="token plain"> </span><span class="token comment" style="color:rgb(105, 112, 152);font-style:italic"># 0.9 - 0.95</span><span class="token plain"></span><br/></span><span class=token-line style=color:#bfc7d5><span class="token plain">    max_model_len</span><span class="token operator" style="color:rgb(137, 221, 255)">=</span><span class="token number" style="color:rgb(247, 140, 108)">4096</span><span class="token punctuation" style="color:rgb(199, 146, 234)">,</span><span class="token plain"></span><br/></span><span class=token-line style=color:#bfc7d5><span class="token plain">    max_num_batched_tokens</span><span class="token operator" style="color:rgb(137, 221, 255)">=</span><span class="token number" style="color:rgb(247, 140, 108)">4096</span><span class="token punctuation" style="color:rgb(199, 146, 234)">,</span><span class="token plain"></span><br/></span><span class=token-line style=color:#bfc7d5><span class="token plain">    mm_processor_kwargs</span><span class="token operator" style="color:rgb(137, 221, 255)">=</span><span class="token punctuation" style="color:rgb(199, 146, 234)">{</span><span class="token plain"></span><br/></span><span class=token-line style=color:#bfc7d5><span class="token plain">        </span><span class="token string" style="color:rgb(195, 232, 141)">"min_pixels"</span><span class="token punctuation" style="color:rgb(199, 146, 234)">:</span><span class="token plain"> </span><span class="token number" style="color:rgb(247, 140, 108)">56</span><span class="token plain"> </span><span class="token operator" style="color:rgb(137, 221, 255)">*</span><span class="token plain"> </span><span class="token number" style="color:rgb(247, 140, 108)">56</span><span class="token punctuation" style="color:rgb(199, 146, 234)">,</span><span class="token plain"></span><br/></span><span class=token-line style=color:#bfc7d5><span class="token plain">        </span><span class="token string" style="color:rgb(195, 232, 141)">"max_pixels"</span><span class="token punctuation" style="color:rgb(199, 146, 234)">:</span><span class="token plain"> </span><span class="token number" style="color:rgb(247, 140, 108)">1024</span><span class="token plain"> </span><span class="token operator" style="color:rgb(137, 221, 255)">*</span><span class="token plain"> </span><span class="token number" style="color:rgb(247, 140, 108)">1024</span><span class="token punctuation" style="color:rgb(199, 146, 234)">,</span><span class="token plain"></span><br/></span><span class=token-line style=color:#bfc7d5><span class="token plain">    </span><span class="token punctuation" style="color:rgb(199, 146, 234)">}</span><span class="token punctuation" style="color:rgb(199, 146, 234)">,</span><span class="token plain"></span><br/></span><span class=token-line style=color:#bfc7d5><span class="token plain"></span><span class="token punctuation" style="color:rgb(199, 146, 234)">)</span><br/></span></code></pre></div></div>
<p>| env                                     | default                    | desc                                                                                                                                                                                                                                                              |
| --------------------------------------- | -------------------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | -------- |
| VLLM_TARGET_DEVICE                      | <code>cuda</code>                     | vLLM 的目标设备，支持 <code>[cuda (默认), rocm, neuron, cpu]</code>                                                                                                                                                                                                          |
| MAX_JOBS                                | <code>None</code> (CPU 数量)          | 最大并行编译作业数。默认情况下是 CPU 数量。                                                                                                                                                                                                                       |
| NVCC_THREADS                            | <code>None</code> (1)                 | 用于 nvcc 的线程数。默认为 1。如果设置，<code>MAX_JOBS</code> 将被减少以避免 CPU 过度占用。                                                                                                                                                                                  |
| VLLM_USE_PRECOMPILED                    | <code>False</code>                    | 如果设置，vLLM 将使用预编译的二进制文件（*.so）。                                                                                                                                                                                                                |
| VLLM_TEST_USE_PRECOMPILED_NIGHTLY_WHEEL | <code>0</code>                        | 是否强制在 Python 构建中使用 nightly wheel。这用于在 Python 构建中测试 nightly wheel。                                                                                                                                                                            |
| CMAKE_BUILD_TYPE                        | <code>Debug</code> / <code>RelWithDebInfo</code> | CMake 构建类型。如果未设置，默认为 "Debug" 或 "RelWithDebInfo"。可用选项："Debug", "Release", "RelWithDebInfo"。                                                                                                                                                  |
| VERBOSE                                 | <code>0</code>                        | 如果设置，vLLM 在安装期间将打印详细日志。                                                                                                                                                                                                                         |
| VLLM_CONFIG_ROOT                        | <code>~/.config/vllm</code>           | vLLM 配置文件根目录。除非设置了 <code>XDG_CONFIG_HOME</code>，否则默认为 <code>~/.config/vllm</code>。请注意，这不仅影响 vLLM 在运行时查找其配置文件的方式，还影响 vLLM 在 <strong>安装</strong> 期间安装其配置文件的方式。                                                                          |
| VLLM_CACHE_ROOT                         | <code>~/.cache/vllm</code>            | vLLM 缓存文件根目录。除非设置了 <code>XDG_CACHE_HOME</code>，否则默认为 <code>~/.cache/vllm</code>。                                                                                                                                                                                    |
| VLLM_HOST_IP                            | <code>""</code>                       | 在分布式环境中用于确定当前节点的 IP 地址，当节点有多个网络接口时。如果您使用多节点推理，应在每个节点上进行不同设置。                                                                                                                                              |
| VLLM_PORT                               | <code>None</code> (0)                 | 在分布式环境中用于手动设置通信端口。注意：如果设置了 <code>VLLM_PORT</code>，并且某些代码需要多个端口，<code>VLLM_PORT</code> 将用作第一个端口，其余端口将通过递增 <code>VLLM_PORT</code> 值生成。'0' 用于使 mypy 满意。                                                                           |
| VLLM_RPC_BASE_PATH                      | 系统的临时目录             | 当前端 API 服务器以多进程模式运行时，用于与后端引擎进程通信的 IPC 路径。                                                                                                                                                                                          |
| VLLM_USE_MODELSCOPE                     | <code>False</code>                    | 如果为 true，将从 ModelScope 而非 Hugging Face Hub 加载模型。请注意，值为 true 或 false，而不是数字。                                                                                                                                                             |
| VLLM_RINGBUFFER_WARNING_INTERVAL        | <code>60</code>                       | 当环形缓冲区满时，记录警告消息的间隔时间（秒）。                                                                                                                                                                                                                  |
| CUDA_HOME                               | <code>None</code>                     | cudatoolkit 主目录的路径，该目录下应包含 bin, include 和 lib 目录。                                                                                                                                                                                               |
| VLLM_NCCL_SO_PATH                       | <code>None</code>                     | NCCL 库文件的路径。需要它是因为 PyTorch 带来的 nccl>=2.19 包含一个 bug：<a href=https://github.com/NVIDIA/nccl/issues/1234 target=_blank rel="noopener noreferrer">https://github.com/NVIDIA/nccl/issues/1234</a>。                                                                                                |
| LD_LIBRARY_PATH                         | <code>None</code>                     | 当 <code>VLLM_NCCL_SO_PATH</code> 未设置时，vLLM 将尝试在 <code>LD_LIBRARY_PATH</code> 指定的位置查找 nccl 库文件。                                                                                                                                                                     |
| VLLM_USE_TRITON_FLASH_ATTN              | <code>True</code>                     | 控制 vLLM 是否应使用 Triton flash attention 的标志。                                                                                                                                                                                                              |
| VLLM_FLASH_ATTN_VERSION                 | <code>None</code>                     | 强制 vLLM 使用特定的 flash-attention 版本（2 或 3），仅在使用 flash-attention 后端时有效。                                                                                                                                                                        |
| VLLM_TEST_DYNAMO_FULLGRAPH_CAPTURE      | <code>True</code>                     | 启用 Dynamo fullgraph 捕获的内部标志。                                                                                                                                                                                                                            |
| LOCAL_RANK                              | <code>0</code>                        | 分布式设置中进程的本地排名，用于确定 GPU 设备 ID。                                                                                                                                                                                                                |
| CUDA_VISIBLE_DEVICES                    | <code>None</code>                     | 用于控制分布式设置中可见设备的变量。                                                                                                                                                                                                                              |
| VLLM_ENGINE_ITERATION_TIMEOUT_S         | <code>60</code>                       | 引擎中每次迭代的超时时间（秒）。                                                                                                                                                                                                                                  |
| VLLM_API_KEY                            | <code>None</code>                     | vLLM API 服务器的 API 密钥。                                                                                                                                                                                                                                      |
| VLLM_DEBUG_LOG_API_SERVER_RESPONSE      | <code>False</code>                    | 是否记录 API 服务器响应以进行调试。                                                                                                                                                                                                                               |
| S3_ACCESS_KEY_ID                        | <code>None</code>                     | S3 访问信息，用于 tensorizer 从 S3 加载模型。                                                                                                                                                                                                                     |
| S3_SECRET_ACCESS_KEY                    | <code>None</code>                     | S3 访问信息，用于 tensorizer 从 S3 加载模型。                                                                                                                                                                                                                     |
| S3_ENDPOINT_URL                         | <code>None</code>                     | S3 访问信息，用于 tensorizer 从 S3 加载模型。                                                                                                                                                                                                                     |
| VLLM_USAGE_STATS_SERVER                 | <code>https://stats.vllm.ai</code>    | 使用情况统计服务器。                                                                                                                                                                                                                                              |
| VLLM_NO_USAGE_STATS                     | <code>0</code>                        | 是否禁用使用情况统计。                                                                                                                                                                                                                                            |
| VLLM_DO_NOT_TRACK                       | <code>0</code>                        | 是否启用“请勿跟踪”功能。                                                                                                                                                                                                                                          |
| VLLM_USAGE_SOURCE                       | <code>production</code>               | 使用情况来源。                                                                                                                                                                                                                                                    |
| VLLM_CONFIGURE_LOGGING                  | <code>1</code>                        | 日志配置。如果设置为 0，vLLM 将不配置日志。如果设置为 1，vLLM 将使用默认配置或 <code>VLLM_LOGGING_CONFIG_PATH</code> 指定的配置文件配置日志。                                                                                                                                |
| VLLM_LOGGING_CONFIG_PATH                | <code>None</code>                     | 日志配置文件的路径。                                                                                                                                                                                                                                              |
| VLLM_LOGGING_LEVEL                      | <code>INFO</code>                     | 用于配置默认日志级别。                                                                                                                                                                                                                                            |
| VLLM_LOGGING_PREFIX                     | <code>""</code>                       | 如果设置，<code>VLLM_LOGGING_PREFIX</code> 将添加到所有日志消息的前面。                                                                                                                                                                                                      |
| VLLM_LOGITS_PROCESSOR_THREADS           | <code>None</code> (0)                 | 如果设置，vLLM 将使用此数量的线程在线程池中调用 logits 处理器。当使用自定义 logits 处理器（a）启动额外的 CUDA 内核或（b）在不持有 Python GIL 的情况下进行大量 CPU 密集型工作，或两者兼有时，这很有用。                                                            |
| VLLM_TRACE_FUNCTION                     | <code>0</code>                        | 跟踪函数调用。如果设置为 1，vLLM 将跟踪函数调用。对于调试很有用。                                                                                                                                                                                                 |
| VLLM_ATTENTION_BACKEND                  | <code>None</code>                     | 注意力计算的后端。可用选项："TORCH_SDPA", "FLASH_ATTN", "XFORMERS", "ROCM_FLASH", "FLASHINFER", "FLASHMLA"。                                                                                                                                                      |
| VLLM_USE_FLASHINFER_SAMPLER             | <code>None</code>                     | 如果设置，vLLM 将使用 flashinfer 采样器。                                                                                                                                                                                                                         |
| VLLM_FLASHINFER_FORCE_TENSOR_CORES      | <code>0</code>                        | 如果设置，vLLM 将强制 flashinfer 使用 tensor cores；否则将根据模型架构使用启发式方法。                                                                                                                                                                            |
| VLLM_PP_LAYER_PARTITION                 | <code>None</code>                     | 流水线阶段分区策略。                                                                                                                                                                                                                                              |
| VLLM_CPU_KVCACHE_SPACE                  | <code>0</code> (4 GiB)                | （仅限 CPU 后端）CPU 键值缓存空间。默认为 4 GiB。                                                                                                                                                                                                                 |
| VLLM_CPU_OMP_THREADS_BIND               | <code>all</code>                      | （仅限 CPU 后端）OpenMP 线程绑定的 CPU 核心 ID，例如 "0-31", "0,1,2", "0-31,33"。不同等级的 CPU 核心用 '                                                                                                                                                          | ' 分隔。 |
| VLLM_CPU_MOE_PREPACK                    | <code>1</code>                        | （仅限 CPU 后端）是否对 MoE 层使用预打包。这将传递给 ipex.llm.modules.GatedMLPMOE。在不支持的 CPU 上，您可能需要将其设置为 "0" (False)。                                                                                                                          |
| VLLM_USE_RAY_SPMD_WORKER                | <code>0</code>                        | 如果设置，所有 worker 将作为独立进程从引擎执行，并且我们使用相同的机制触发所有 worker 的执行。运行 <code>vLLM</code> 并设置 <code>VLLM_USE_RAY_SPMD_WORKER=1</code> 以启用它。                                                                                                          |
| VLLM_USE_RAY_COMPILED_DAG               | <code>0</code>                        | 如果设置，它使用 Ray 的 Compiled Graph（以前称为 ADAG）API，该 API 优化了控制平面开销。请注意，当使用 Ray 分布式执行器时，此变量在 V1 中默认设置为 1。                                                                                                            |
| VLLM_USE_RAY_COMPILED_DAG_CHANNEL_TYPE  | <code>auto</code>                     | 如果设置，Ray Compiled Graph 使用指定的通道类型在属于不同流水线并行阶段的 worker 之间进行通信。可用选项："auto"：使用默认通道类型；"nccl"：使用 NCCL 进行通信；"shm"：使用共享内存和 gRPC 进行通信。如果 <code>VLLM_USE_RAY_COMPILED_DAG</code> 未设置，则忽略此标志。       |
| VLLM_USE_RAY_COMPILED_DAG_OVERLAP_COMM  | <code>0</code>                        | 如果设置，它在 Ray 的 Compiled Graph 中启用 GPU 通信重叠（实验性功能）。如果 <code>VLLM_USE_RAY_COMPILED_DAG</code> 未设置，则忽略此标志。                                                                                                                                   |
| VLLM_WORKER_MULTIPROC_METHOD            | <code>fork</code>                     | 为 worker 使用专用的多进程上下文。<code>spawn</code> 和 <code>fork</code> 都有效。                                                                                                                                                                                                      |
| VLLM_ASSETS_CACHE                       | <code>~/.cache/vllm/assets</code>     | 用于存储下载资产的缓存路径。                                                                                                                                                                                                                                      |
| VLLM_IMAGE_FETCH_TIMEOUT                | <code>5</code>                        | 服务多模态模型时获取图像的超时时间（秒）。默认值为 5 秒。                                                                                                                                                                                                         |
| VLLM_VIDEO_FETCH_TIMEOUT                | <code>30</code>                       | 服务多模态模型时获取视频的超时时间（秒）。默认值为 30 秒。                                                                                                                                                                                                        |
| VLLM_AUDIO_FETCH_TIMEOUT                | <code>10</code>                       | 服务多模态模型时获取音频的超时时间（秒）。默认值为 10 秒。                                                                                                                                                                                                        |
| VLLM_MM_INPUT_CACHE_GIB                 | <code>4</code>                        | 多模态输入缓存大小（GiB）。默认值为 4 GiB。                                                                                                                                                                                                                       |
| VLLM_XLA_CACHE_PATH                     | <code>~/.cache/vllm/xla_cache</code>  | XLA 持久缓存目录的路径。仅用于 TPU 等 XLA 设备。                                                                                                                                                                                                                  |
| VLLM_XLA_CHECK_RECOMPILATION            | <code>0</code>                        | 如果设置，每次执行步骤后都会断言 XLA 重新编译。                                                                                                                                                                                                                   |
| VLLM_FUSED_MOE_CHUNK_SIZE               | <code>32768</code>                    | 融合 MoE 块的大小。                                                                                                                                                                                                                                               |
| VLLM_NO_DEPRECATION_WARNING             | <code>0</code>                        | 如果设置，vLLM 将跳过弃用警告。                                                                                                                                                                                                                                   |
| VLLM_KEEP_ALIVE_ON_ENGINE_DEATH         | <code>0</code>                        | 如果设置，即使底层 AsyncLLMEngine 出错并停止服务请求，OpenAI API 服务器也将保持活动状态。                                                                                                                                                                         |
| VLLM_ALLOW_LONG_MAX_MODEL_LEN           | <code>0</code>                        | 如果设置了环境变量 <code>VLLM_ALLOW_LONG_MAX_MODEL_LEN</code>，它允许用户指定一个大于模型 <code>config.json</code> 中推导出的最大序列长度。要启用此功能，请设置 <code>VLLM_ALLOW_LONG_MAX_MODEL_LEN=1</code>。                                                                                     |
| VLLM_TEST_FORCE_FP8_MARLIN              | <code>0</code>                        | 如果设置，无论硬件是否支持 FP8 计算，都强制使用 FP8 Marlin 进行 FP8 量化。                                                                                                                                                                                        |
| VLLM_TEST_FORCE_LOAD_FORMAT             | <code>dummy</code>                    | 强制加载格式。                                                                                                                                                                                                                                                    |
| VLLM_RPC_TIMEOUT                        | <code>10000</code>                    | zmq 客户端等待后端服务器响应简单数据操作的超时时间（毫秒）。                                                                                                                                                                                                      |
| VLLM_PLUGINS                            | <code>None</code>                     | 要加载的插件名称列表，以逗号分隔。如果未设置，表示将加载所有插件；如果设置为空字符串，则不加载任何插件。                                                                                                                                                          |
| VLLM_TORCH_PROFILER_DIR                 | <code>None</code>                     | 如果设置，启用 torch profiler。torch profiler 跟踪文件保存的目录路径。请注意，它必须是绝对路径。                                                                                                                                                                  |
| VLLM_USE_TRITON_AWQ                     | <code>0</code>                        | 如果设置，vLLM 将使用 Triton 实现的 AWQ。                                                                                                                                                                                                                         |
| VLLM_ALLOW_RUNTIME_LORA_UPDATING        | <code>0</code>                        | 如果设置，允许在运行时加载或卸载 Lora 适配器。                                                                                                                                                                                                                    |
| VLLM_SKIP_P2P_CHECK                     | <code>0</code>                        | 默认情况下，vLLM 会自行检查对等通信能力，以防驱动程序损坏。如果此环境变量设置为 1，vLLM 将跳过对等检查，并信任驱动程序的对等通信能力报告。                                                                                                                        |
| VLLM_DISABLED_KERNELS                   | <code>[]</code>                       | 应禁用（用于测试和性能比较）的量化内核列表。目前仅影响 MPLinearKernel 选择。                                                                                                                                                                                      |
| VLLM_USE_V1                             | <code>1</code>                        | 如果设置，使用 V1 代码路径。                                                                                                                                                                                                                                      |
| VLLM_ROCM_USE_AITER                     | <code>False</code>                    | 除非明确启用，否则禁用 aiter 操作。作为启用其余操作的父开关。                                                                                                                                                                                                     |
| VLLM_ROCM_USE_AITER_PAGED_ATTN          | <code>False</code>                    | 是否使用 aiter 分页注意力。默认禁用。                                                                                                                                                                                                                             |
| VLLM_ROCM_USE_AITER_LINEAR              | <code>True</code>                     | 如果启用了 aiter 操作，则使用 aiter 线性操作。相关操作列表：scaled_mm (per-tensor / rowwise)。                                                                                                                                                                    |
| VLLM_ROCM_USE_AITER_MOE                 | <code>True</code>                     | 是否使用 aiter moe 操作。默认启用。                                                                                                                                                                                                                               |
| VLLM_ROCM_USE_AITER_RMSNORM             | <code>True</code>                     | 如果启用了 aiter 操作，则使用 aiter rms norm 操作。                                                                                                                                                                                                               |
| VLLM_ROCM_USE_AITER_MLA                 | <code>True</code>                     | 是否使用 aiter mla 操作。默认启用。                                                                                                                                                                                                                               |
| VLLM_ROCM_USE_SKINNY_GEMM               | <code>True</code>                     | 使用 rocm skinny gemms。                                                                                                                                                                                                                                          |
| VLLM_ROCM_FP8_PADDING                   | <code>1</code>                        | 为 ROCm 将 FP8 权重填充到 256 字节。                                                                                                                                                                                                                              |
| VLLM_ROCM_MOE_PADDING                   | <code>1</code>                        | 为 MoE 内核填充权重。                                                                                                                                                                                                                                             |
| VLLM_ROCM_CUSTOM_PAGED_ATTN             | <code>True</code>                     | 适用于 MI3* 显卡的自定义分页注意力内核。                                                                                                                                                                                                                         |
| Q_SCALE_CONSTANT                        | <code>200</code>                      | 用于 FP8 KV Cache 动态查询比例因子计算的除数。                                                                                                                                                                                                                    |
| K_SCALE_CONSTANT                        | <code>200</code>                      | 用于 FP8 KV Cache 动态键比例因子计算的除数。                                                                                                                                                                                                                      |
| V_SCALE_CONSTANT                        | <code>100</code>                      | 用于 FP8 KV Cache 动态值比例因子计算的除数。                                                                                                                                                                                                                      |
| VLLM_ENABLE_V1_MULTIPROCESSING          | <code>1</code>                        | 如果设置，在 V1 代码路径中为 LLM 启用多进程。                                                                                                                                                                                                                     |
| VLLM_LOG_BATCHSIZE_INTERVAL             | <code>-1</code>                       | 记录批处理大小的间隔时间（秒）。                                                                                                                                                                                                                                  |
| VLLM_DISABLE_COMPILE_CACHE              | <code>0</code>                        | 如果设置，禁用编译缓存。                                                                                                                                                                                                                                          |
| VLLM_SERVER_DEV_MODE                    | <code>0</code>                        | 如果设置，vLLM 将以开发模式运行，这将启用一些用于开发和调试的附加端点，例如 <code>/reset_prefix_cache</code>。                                                                                                                                                               |
| VLLM_V1_OUTPUT_PROC_CHUNK_SIZE          | <code>128</code>                      | 在 V1 AsyncLLM 接口中处理每 token 输出时，单个 asyncio 任务处理请求的最大数量。它适用于处理高并发流式请求的情况。设置过高可能导致消息间延迟的方差增加。设置过低可能对 TTFT 和整体吞吐量产生负面影响。                                                             |
| VLLM_MLA_DISABLE                        | <code>0</code>                        | 如果设置，vLLM 将禁用 MLA 注意力优化。                                                                                                                                                                                                                            |
| VLLM_ENABLE_MOE_ALIGN_BLOCK_SIZE_TRITON | <code>0</code>                        | 如果设置，vLLM 将使用 Triton 实现的 moe_align_block_size，即 fused_moe.py 中的 moe_align_block_size_triton。                                                                                                                                                      |
| VLLM_RAY_PER_WORKER_GPUS                | <code>1.0</code>                      | Ray 中每个 worker 的 GPU 数量，如果设置为小数，则 Ray 可以将多个 actor 调度到单个 GPU 上，以便用户可以将其他 actor 与 vLLM 放置在相同的 GPU 上。                                                                                                                  |
| VLLM_RAY_BUNDLE_INDICES                 | <code>""</code>                       | Ray 的 Bundle 索引，如果设置，它可以精确控制每个 worker 使用哪些索引作为 Ray Bundle。格式：逗号分隔的整数列表，例如 "0,1,2,3"。                                                                                                                                   |
| VLLM_CUDART_SO_PATH                     | <code>None</code>                     | 在某些系统中，<code>find_loaded_library()</code> 可能无法工作。因此，我们允许用户通过环境变量 <code>VLLM_CUDART_SO_PATH</code> 指定路径。                                                                                                                                               |
| VLLM_USE_HPU_CONTIGUOUS_CACHE_FETCH     | <code>True</code>                     | 连续缓存获取，以避免在 Gaudi3 上使用昂贵的 gather 操作。这仅适用于 HPU 连续缓存。如果设置为 true，将使用连续缓存获取。                                                                                                                                            |
| VLLM_HPU_USE_DELAYED_SAMPLING           | <code>False</code>                    | 为 HPU 使用延迟采样以减少每步之间的主机 CPU 开销。                                                                                                                                                                                                                |
| VLLM_DP_RANK                            | <code>0</code>                        | 数据并行设置中进程的排名。                                                                                                                                                                                                                                        |
| VLLM_DP_RANK_LOCAL                      | <code>VLLM_DP_RANK</code>             | 数据并行设置中进程的本地排名。如果未设置，则默认为 <code>VLLM_DP_RANK</code>。                                                                                                                                                                                               |
| VLLM_DP_SIZE                            | <code>1</code>                        | 数据并行设置的世界大小。                                                                                                                                                                                                                                          |
| VLLM_DP_MASTER_IP                       | <code>127.0.0.1</code>                | 数据并行设置中主节点的 IP 地址。                                                                                                                                                                                                                                  |
| VLLM_DP_MASTER_PORT                     | <code>0</code>                        | 数据并行设置中主节点的端口。                                                                                                                                                                                                                                      |
| VLLM_CI_USE_S3                          | <code>0</code>                        | 在 CI 中是否通过 RunAI Streamer 使用 S3 路径进行模型加载。                                                                                                                                                                                                        |
| VLLM_MODEL_REDIRECT_PATH                | <code>None</code>                     | 使用 <code>model_redirect</code> 将模型名称重定向到本地文件夹。<code>model_redirect</code> 可以是一个 JSON 文件，映射 repo_id 和本地文件夹之间的模型：<code>{"meta-llama/Llama-3.2-1B": "/tmp/Llama-3.2-1B"}</code>；也可以是一个空格分隔的值表文件：<code>meta-llama/Llama-3.2-1B /tmp/Llama-3.2-1B</code>。 |
| VLLM_MARLIN_USE_ATOMIC_ADD              | <code>0</code>                        | 在 gptq/awq marlin 内核中是否使用 <code>atomicAdd</code> 归约。                                                                                                                                                                                                              |
| VLLM_V0_USE_OUTLINES_CACHE              | <code>0</code>                        | 是否为 V0 启用 outlines 缓存。此缓存是无限制的，并且存储在磁盘上，因此在可能存在恶意用户的环境中不安全使用。                                                                                                                                                      |
| VLLM_TPU_BUCKET_PADDING_GAP             | <code>0</code>                        | 前向传播的填充桶之间的间隙。例如，如果为 8，则前向传播将使用 [16, 24, 32, ...]。                                                                                                                                                                                  |
| VLLM_USE_DEEP_GEMM                      | <code>0</code>                        | 允许将 DeepGemm 内核用于融合的 moe 操作。                                                                                                                                                                                                                         |
| VLLM_XGRAMMAR_CACHE_MB                  | <code>512</code>                      | 控制 xgrammar 编译器使用的缓存大小。默认 512 MB 应该足以容纳大约 1000 个 JSON schema。如果需要，可以通过此变量进行更改。                                                                                                                                          |
| VLLM_MSGPACK_ZERO_COPY_THRESHOLD        | <code>256</code>                      | 控制 msgspec 使用“零拷贝”进行张量序列化/反序列化的阈值。低于此限制的张量将被编码到 msgpack 缓冲区中，而高于此限制的张量将通过单独的消息发送。虽然发送方在所有情况下仍然复制张量，但在接收方，高于此限制的张量将实际进行零拷贝解码。                               |</p>
<ul>
<li><a href=https://docs.vllm.ai/en/stable/serving/env_vars.html target=_blank rel="noopener noreferrer">https://docs.vllm.ai/en/stable/serving/env_vars.html</a></li>
</ul>
<h1>API</h1>
<ul>
<li><a href=https://docs.vllm.ai/en/v0.8.5/serving/openai_compatible_server.html target=_blank rel="noopener noreferrer">https://docs.vllm.ai/en/v0.8.5/serving/openai_compatible_server.html</a></li>
</ul>
<h1>FAQ</h1>
<ul>
<li>dtype
<ul>
<li>auto, half, float16, bfloat16, float, float32</li>
<li>“auto” will use FP16 precision for FP32 and FP16 models, and BF16 precision for BF16 models.</li>
<li>“half” for FP16. Recommended for AWQ quantization.</li>
<li>“float16” is the same as “half”.</li>
<li>“bfloat16” for a balance between precision and range.</li>
<li>“float” is shorthand for FP32 precision.</li>
<li>“float32” for FP32 precision.</li>
</ul>
</li>
<li>--quantization
<ul>
<li>aqlm,awq,deepspeedfp,tpu_int8,fp8,ptpc_fp8,fbgemm_fp8,modelopt,nvfp4,marlin,bitblas,gguf,gptq_marlin_24,gptq_marlin</li>
<li>gptq_bitblas,awq_marlin,gptq,compressed-tensors,bitsandbytes,qqq,hqq,experts_int8,neuron_quant,ipex,quark,moe_wna16,torchao,None</li>
</ul>
</li>
<li>capability
<ul>
<li>8.0 bfloat16
<ul>
<li>不支持会 cast 为 fp16 或 fp32 - 性能影响很大</li>
</ul>
</li>
<li>7.5 awq</li>
</ul>
</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_EG6R" id=compute-capability--80-is-not-supported-by-the-v1-engine-falling-back-to-v0>Compute Capability &lt; 8.0 is not supported by the V1 Engine. Falling back to V0.<a href=#compute-capability--80-is-not-supported-by-the-v1-engine-falling-back-to-v0 class=hash-link aria-label="Direct link to Compute Capability < 8.0 is not supported by the V1 Engine. Falling back to V0." title="Direct link to Compute Capability < 8.0 is not supported by the V1 Engine. Falling back to V0." translate=no>​</a></h2>
<div class="language-text codeBlockContainer_ljD1 theme-code-block" style=--prism-color:#bfc7d5;--prism-background-color:#292d3e><div class=codeBlockContent_rhaG><pre tabindex=0 class="prism-code language-text codeBlock_mx9Q thin-scrollbar" style=color:#bfc7d5;background-color:#292d3e><code class=codeBlockLines_NG8l><span class=token-line style=color:#bfc7d5><span class="token plain">Cannot use FlashAttention-2 backend for Volta and Turing GPUs.</span><br/></span></code></pre></div></div>
<div class="language-text codeBlockContainer_ljD1 theme-code-block" style=--prism-color:#bfc7d5;--prism-background-color:#292d3e><div class=codeBlockContent_rhaG><pre tabindex=0 class="prism-code language-text codeBlock_mx9Q thin-scrollbar" style=color:#bfc7d5;background-color:#292d3e><code class=codeBlockLines_NG8l><span class=token-line style=color:#bfc7d5><span class="token plain">Found GPU0 Tesla V100-SXM2-16GB which is of cuda capability 7.0.</span><br/></span><span class=token-line style=color:#bfc7d5><span class="token plain">PyTorch no longer supports this GPU because it is too old.</span><br/></span><span class=token-line style=color:#bfc7d5><span class="token plain">The minimum cuda capability supported by this library is 7.5.</span><br/></span></code></pre></div></div>
<div class="language-text codeBlockContainer_ljD1 theme-code-block" style=--prism-color:#bfc7d5;--prism-background-color:#292d3e><div class=codeBlockContent_rhaG><pre tabindex=0 class="prism-code language-text codeBlock_mx9Q thin-scrollbar" style=color:#bfc7d5;background-color:#292d3e><code class=codeBlockLines_NG8l><span class=token-line style=color:#bfc7d5><span class="token plain">RuntimeError: CUDA error: no kernel image is available for execution on the device</span><br/></span></code></pre></div></div>
<h2 class="anchor anchorWithStickyNavbar_EG6R" id=dtypeauto>dtype=auto<a href=#dtypeauto class=hash-link aria-label="Direct link to dtype=auto" title="Direct link to dtype=auto" translate=no>​</a></h2>
<div class="language-text codeBlockContainer_ljD1 theme-code-block" style=--prism-color:#bfc7d5;--prism-background-color:#292d3e><div class=codeBlockContent_rhaG><pre tabindex=0 class="prism-code language-text codeBlock_mx9Q thin-scrollbar" style=color:#bfc7d5;background-color:#292d3e><code class=codeBlockLines_NG8l><span class=token-line style=color:#bfc7d5><span class="token plain">ValueError: Bfloat16 is only supported on GPUs with compute capability of at least 8.0. Your Tesla V100-SXM2-16GB GPU has compute capability 7.0. You can use float16 instead by explicitly setting the `dtype` flag in CLI, for example: --dtype=half.</span><br/></span></code></pre></div></div>
<h2 class="anchor anchorWithStickyNavbar_EG6R" id=the-quantization-method-awq-is-not-supported-for-the-current-gpu-minimum-capability-75-current-capability-70>The quantization method awq is not supported for the current GPU. Minimum capability: 75. Current capability: 70.<a href=#the-quantization-method-awq-is-not-supported-for-the-current-gpu-minimum-capability-75-current-capability-70 class=hash-link aria-label="Direct link to The quantization method awq is not supported for the current GPU. Minimum capability: 75. Current capability: 70." title="Direct link to The quantization method awq is not supported for the current GPU. Minimum capability: 75. Current capability: 70." translate=no>​</a></h2>
<ul>
<li>V100</li>
<li><a href=https://github.com/vllm-project/vllm/issues/1345 target=_blank rel="noopener noreferrer">https://github.com/vllm-project/vllm/issues/1345</a></li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_EG6R" id=gptq-int4>GPTQ-Int4<a href=#gptq-int4 class=hash-link aria-label="Direct link to GPTQ-Int4" title="Direct link to GPTQ-Int4" translate=no>​</a></h2>
<div class="language-bash codeBlockContainer_ljD1 theme-code-block" style=--prism-color:#bfc7d5;--prism-background-color:#292d3e><div class=codeBlockContent_rhaG><pre tabindex=0 class="prism-code language-bash codeBlock_mx9Q thin-scrollbar" style=color:#bfc7d5;background-color:#292d3e><code class=codeBlockLines_NG8l><span class=token-line style=color:#bfc7d5><span class="token plain">vllm serve Qwen/Qwen2.5-7B-Instruct-GPTQ-Int4 --dtype=half</span><br/></span></code></pre></div></div>
<div class="language-text codeBlockContainer_ljD1 theme-code-block" style=--prism-color:#bfc7d5;--prism-background-color:#292d3e><div class=codeBlockContent_rhaG><pre tabindex=0 class="prism-code language-text codeBlock_mx9Q thin-scrollbar" style=color:#bfc7d5;background-color:#292d3e><code class=codeBlockLines_NG8l><span class=token-line style=color:#bfc7d5><span class="token plain">ValueError: Trying to use the bitblas backend, but could not importwith the following error: No module named 'bitblas'. Please install bitblas through the following command: `pip install bitblas>=0.1.0`</span><br/></span></code></pre></div></div>
<h2 class="anchor anchorWithStickyNavbar_EG6R" id=no-optimized-function-available-for-platform-cuda>No optimized function available for platform CUDA<a href=#no-optimized-function-available-for-platform-cuda class=hash-link aria-label="Direct link to No optimized function available for platform CUDA" title="Direct link to No optimized function available for platform CUDA" translate=no>​</a></h2>
<div class="language-text codeBlockContainer_ljD1 theme-code-block" style=--prism-color:#bfc7d5;--prism-background-color:#292d3e><div class=codeBlockContent_rhaG><pre tabindex=0 class="prism-code language-text codeBlock_mx9Q thin-scrollbar" style=color:#bfc7d5;background-color:#292d3e><code class=codeBlockLines_NG8l><span class=token-line style=color:#bfc7d5><span class="token plain">TVM target not found. Please set the TVM target environment variable using export TVM_TARGET=&lt;target></span><br/></span></code></pre></div></div>
<div class="language-bash codeBlockContainer_ljD1 theme-code-block" style=--prism-color:#bfc7d5;--prism-background-color:#292d3e><div class=codeBlockContent_rhaG><pre tabindex=0 class="prism-code language-bash codeBlock_mx9Q thin-scrollbar" style=color:#bfc7d5;background-color:#292d3e><code class=codeBlockLines_NG8l><span class=token-line style=color:#bfc7d5><span class="token plain"># force --quantization=gptq</span><br/></span><span class=token-line style=color:#bfc7d5><span class="token plain">CUDA_DEVICE_ORDER=PCI_BUS_ID vllm serve Qwen/Qwen2.5-7B-Instruct-GPTQ-Int4 --dtype=half --quantization=gptq</span><br/></span></code></pre></div></div>
<ul>
<li>Volta (sm_70) 出错</li>
<li>gptq_bitblas 会使用 BitBLAS</li>
<li>BitBLAS 依赖 TVM</li>
<li>参考
<ul>
<li>Disable gptq_bitblas for &lt; SM80 to fix GPTQ on V100/T4 <a href=https://github.com/vllm-project/vllm/pull/17541 target=_blank rel="noopener noreferrer">vllm-project/vllm#17541</a></li>
</ul>
</li>
</ul></div><footer class="theme-doc-footer docusaurus-mt-lg"><div class="row margin-top--sm theme-doc-footer-tags-row"><div class=col><b>Tags:</b><ul class="tags_noif padding--none margin-left--sm"><li class=tag_mzC7><a rel=tag class="tag_xI_x tagRegular_Q4uH" href=/notes/tags/server>Server</a></ul></div></div><div class="row margin-top--sm theme-doc-footer-edit-meta-row"><div class=col><a href=https://github.com/wenerme/wener/edit/master/notes/../notes/ai/dev/vllm.md target=_blank rel="noopener noreferrer" class=theme-edit-this-page><svg fill=currentColor height=20 width=20 viewBox="0 0 40 40" class=iconEdit_A7Jz aria-hidden=true><g><path d="m34.5 11.7l-3 3.1-6.3-6.3 3.1-3q0.5-0.5 1.2-0.5t1.1 0.5l3.9 3.9q0.5 0.4 0.5 1.1t-0.5 1.2z m-29.5 17.1l18.4-18.5 6.3 6.3-18.4 18.4h-6.3v-6.2z"/></g></svg>Edit this page</a></div><div class="col lastUpdated_Hg3U"><span class=theme-last-updated>Last updated<!-- --> on <b><time datetime=2025-08-04T03:21:03.000Z itemprop=dateModified>Aug 4, 2025</time></b> by <b>wener</b></span></div></div></footer></article><nav class="docusaurus-mt-lg pagination-nav" aria-label="Docs pages"><a class="pagination-nav__link pagination-nav__link--prev" href=/notes/ai/dev/tgi><div class=pagination-nav__sublabel>Previous</div><div class=pagination-nav__label>Text Generation Inference</div></a><a class="pagination-nav__link pagination-nav__link--next" href=/notes/ai/diffusion><div class=pagination-nav__sublabel>Next</div><div class=pagination-nav__label>Diffusion</div></a></nav></div></div><div class="col col--3"><div class="tableOfContents_wpyd thin-scrollbar theme-doc-toc-desktop"><ul class="table-of-contents table-of-contents__left-border"><li><a href=#compute-capability--80-is-not-supported-by-the-v1-engine-falling-back-to-v0 class="table-of-contents__link toc-highlight">Compute Capability &lt; 8.0 is not supported by the V1 Engine. Falling back to V0.</a><li><a href=#dtypeauto class="table-of-contents__link toc-highlight">dtype=auto</a><li><a href=#the-quantization-method-awq-is-not-supported-for-the-current-gpu-minimum-capability-75-current-capability-70 class="table-of-contents__link toc-highlight">The quantization method awq is not supported for the current GPU. Minimum capability: 75. Current capability: 70.</a><li><a href=#gptq-int4 class="table-of-contents__link toc-highlight">GPTQ-Int4</a><li><a href=#no-optimized-function-available-for-platform-cuda class="table-of-contents__link toc-highlight">No optimized function available for platform CUDA</a></ul></div></div></div></div></main></div></div></div><footer class="theme-layout-footer footer footer--dark"><div class="container container-fluid"><div class="row footer__links"><div class="theme-layout-footer-column col footer__col"><div class=footer__title>笔记</div><ul class="footer__items clean-list"><li class=footer__item><a class=footer__link-item href=/notes/java>Java</a><li class=footer__item><a class=footer__link-item href=/notes/os/alpine>AlpineLinux</a><li class=footer__item><a class=footer__link-item href=/notes/devops/kubernetes>Kubernates</a><li class=footer__item><a class=footer__link-item href=/notes/voip>VoIP</a></ul></div><div class="theme-layout-footer-column col footer__col"><div class=footer__title>Projects</div><ul class="footer__items clean-list"><li class=footer__item>
              <div>
              <a class=footer__link-item href=https://github.com/wenerme/wener>Wener</a>
              -
              <a class=footer__link-item href=https://github.com/wenerme/wener/actions title="wenerme/wener - ci">
              <img style="vertical-align: middle;opacity: .4;" src=https://github.com/wenerme/wener/workflows/Build/badge.svg />
              </a>
              </div>
              <li class=footer__item><a href=https://apis.wener.me target=_blank rel="noopener noreferrer" class=footer__link-item>Wener's Apis<svg width=13.5 height=13.5 aria-label="(opens in new tab)" class=iconExternalLink_kUGX><use href=#theme-svg-external-link /></svg></a></ul></div><div class="theme-layout-footer-column col footer__col"><div class=footer__title>Social</div><ul class="footer__items clean-list"><li class=footer__item><a class=footer__link-item href=/story>Blog</a><li class=footer__item><a href=https://github.com/wenerme target=_blank rel="noopener noreferrer" class=footer__link-item>GitHub</a><li class=footer__item><a href=https://twitter.com/wenerme target=_blank rel="noopener noreferrer" class=footer__link-item>Twitter</a></ul></div></div><div class="footer__bottom text--center"><div class=margin-bottom--sm><img src=/img/wener-logo.svg alt="Wener Site" class="footer__logo themedComponent_j2y5 themedComponent--light_v877"/><img src=/img/wener-logo.svg alt="Wener Site" class="footer__logo themedComponent_j2y5 themedComponent--dark_PUQY"/></div><div class=footer__copyright>Copyright © 1992-2025 Wener - <img alt=cc-by-sa-4.0 src=https://mirrors.creativecommons.org/presskit/buttons/80x15/svg/by-sa.svg /> - Build @2025-10-09 13:46</div></div></div></footer></div></body>