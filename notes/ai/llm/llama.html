<!doctype html><html lang=en dir=ltr class="docs-wrapper plugin-docs plugin-id-default docs-version-current docs-doc-page docs-doc-id-ai/llm/llama" data-has-hydrated=false><head><meta charset=UTF-8><meta name=generator content="Docusaurus v3.9.1"><title data-rh=true>LLaMa | Wener Live & Life</title><meta data-rh=true name=viewport content="width=device-width, initial-scale=1.0"/><meta data-rh=true name=twitter:card content=summary_large_image /><meta data-rh=true property=og:url content=https://wener.me/notes/ai/llm/llama /><meta data-rh=true property=og:locale content=en /><meta data-rh=true name=docusaurus_locale content=en /><meta data-rh=true name=docsearch:language content=en /><meta data-rh=true name=docusaurus_version content=current /><meta data-rh=true name=docusaurus_tag content=docs-default-current /><meta data-rh=true name=docsearch:version content=current /><meta data-rh=true name=docsearch:docusaurus_tag content=docs-default-current /><meta data-rh=true property=og:title content="LLaMa | Wener Live & Life"/><meta data-rh=true name=description content="- LLaMA-7B, 3.5GB, 6GB"/><meta data-rh=true property=og:description content="- LLaMA-7B, 3.5GB, 6GB"/><link data-rh=true rel=icon href=/img/favicon.ico /><link data-rh=true rel=canonical href=https://wener.me/notes/ai/llm/llama /><link data-rh=true rel=alternate href=https://wener.me/notes/ai/llm/llama hreflang=en /><link data-rh=true rel=alternate href=https://wener.me/notes/ai/llm/llama hreflang=x-default /><link data-rh=true rel=preconnect href=https://37P8DMWBKF-dsn.algolia.net crossorigin=anonymous /><script data-rh=true type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","item":"https://wener.me/notes/ai/","name":"AI","position":1},{"@type":"ListItem","item":"https://wener.me/notes/ai/llm/","name":"LLM","position":2},{"@type":"ListItem","item":"https://wener.me/notes/ai/llm/llama","name":"LLaMa","position":3}]}</script><link rel=alternate type=application/rss+xml href=/story/rss.xml title="Wener Live & Life RSS Feed"><link rel=alternate type=application/atom+xml href=/story/atom.xml title="Wener Live & Life Atom Feed"><link rel=preconnect href=https://www.google-analytics.com><script>window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)},ga.l=+new Date,ga("create","UA-30404720-1","auto"),ga("send","pageview")</script><script async src=https://www.google-analytics.com/analytics.js></script><link rel=preconnect href=https://www.google-analytics.com><link rel=preconnect href=https://www.googletagmanager.com><script async src="https://www.googletagmanager.com/gtag/js?id=UA-30404720-1"></script><script>function gtag(){dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","UA-30404720-1",{})</script><link rel=search type=application/opensearchdescription+xml title="Wener Live & Life" href=/opensearch.xml><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.13.24/dist/katex.min.css><script src=https://static.cloudflareinsights.com/beacon.min.js async data-cf-beacon='{"token": "e9a1b931103044f3940ee67b78c7df70"}' defer></script><link rel=stylesheet href=/assets/css/styles.fa221ee1.css /><script src=/assets/js/runtime~main.72c320db.js defer></script><script src=/assets/js/main.c07313b6.js defer></script></head><body class=navigation-with-keyboard><svg style="display: none;"><defs>
<symbol id=theme-svg-external-link viewBox="0 0 24 24"><path fill=currentColor d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"/></symbol>
</defs></svg>
<script>!function(){var t=function(){try{return new URLSearchParams(window.location.search).get("docusaurus-theme")}catch(t){}}()||function(){try{return window.localStorage.getItem("theme")}catch(t){}}();document.documentElement.setAttribute("data-theme",t||"light"),document.documentElement.setAttribute("data-theme-choice",t||"light")}(),function(){try{for(var[t,e]of new URLSearchParams(window.location.search).entries())if(t.startsWith("docusaurus-data-")){var a=t.replace("docusaurus-data-","data-");document.documentElement.setAttribute(a,e)}}catch(t){}}()</script><div id=__docusaurus><div role=region aria-label="Skip to main content"><a class=skipToContent_jKDA href=#__docusaurus_skipToContent_fallback>Skip to main content</a></div><nav aria-label=Main class="theme-layout-navbar navbar navbar--fixed-top"><div class=navbar__inner><div class="theme-layout-navbar-left navbar__items"><button aria-label="Toggle navigation bar" aria-expanded=false class="navbar__toggle clean-btn" type=button><svg width=30 height=30 viewBox="0 0 30 30" aria-hidden=true><path stroke=currentColor stroke-linecap=round stroke-miterlimit=10 stroke-width=2 d="M4 7h22M4 15h22M4 23h22"/></svg></button><a class=navbar__brand href=/><div class=navbar__logo><img src=/img/wener-logo-head.svg alt="Wener Logo" class="themedComponent_j2y5 themedComponent--light_v877"/><img src=/img/wener-logo-head.svg alt="Wener Logo" class="themedComponent_j2y5 themedComponent--dark_PUQY"/></div><b class="navbar__title text--truncate">Wener</b></a><a aria-current=page class="navbar__item navbar__link navbar__link--active" href=/notes>笔记</a><a class="navbar__item navbar__link" href=/story>故事</a><a class="navbar__item navbar__link" href=/notes/howto/network/dns-prevent-spoofing>指南</a></div><div class="theme-layout-navbar-right navbar__items navbar__items--right"><a href=https://github.com/wenerme/wener target=_blank rel="noopener noreferrer" class="navbar__item navbar__link">GitHub<svg width=13.5 height=13.5 aria-label="(opens in new tab)" class=iconExternalLink_kUGX><use href=#theme-svg-external-link /></svg></a><div class="toggle_N7Mq colorModeToggle_vh_y"><button class="clean-btn toggleButton_PaHe toggleButtonDisabled_RC4w" type=button disabled title="system mode" aria-label="Switch between dark and light mode (currently system mode)"><svg viewBox="0 0 24 24" width=24 height=24 aria-hidden=true class="toggleIcon_Q188 lightToggleIcon_M1mU"><path fill=currentColor d="M12,9c1.65,0,3,1.35,3,3s-1.35,3-3,3s-3-1.35-3-3S10.35,9,12,9 M12,7c-2.76,0-5,2.24-5,5s2.24,5,5,5s5-2.24,5-5 S14.76,7,12,7L12,7z M2,13l2,0c0.55,0,1-0.45,1-1s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S1.45,13,2,13z M20,13l2,0c0.55,0,1-0.45,1-1 s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S19.45,13,20,13z M11,2v2c0,0.55,0.45,1,1,1s1-0.45,1-1V2c0-0.55-0.45-1-1-1S11,1.45,11,2z M11,20v2c0,0.55,0.45,1,1,1s1-0.45,1-1v-2c0-0.55-0.45-1-1-1C11.45,19,11,19.45,11,20z M5.99,4.58c-0.39-0.39-1.03-0.39-1.41,0 c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0s0.39-1.03,0-1.41L5.99,4.58z M18.36,16.95 c-0.39-0.39-1.03-0.39-1.41,0c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0c0.39-0.39,0.39-1.03,0-1.41 L18.36,16.95z M19.42,5.99c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06c-0.39,0.39-0.39,1.03,0,1.41 s1.03,0.39,1.41,0L19.42,5.99z M7.05,18.36c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06 c-0.39,0.39-0.39,1.03,0,1.41s1.03,0.39,1.41,0L7.05,18.36z"/></svg><svg viewBox="0 0 24 24" width=24 height=24 aria-hidden=true class="toggleIcon_Q188 darkToggleIcon_sEdo"><path fill=currentColor d="M9.37,5.51C9.19,6.15,9.1,6.82,9.1,7.5c0,4.08,3.32,7.4,7.4,7.4c0.68,0,1.35-0.09,1.99-0.27C17.45,17.19,14.93,19,12,19 c-3.86,0-7-3.14-7-7C5,9.07,6.81,6.55,9.37,5.51z M12,3c-4.97,0-9,4.03-9,9s4.03,9,9,9s9-4.03,9-9c0-0.46-0.04-0.92-0.1-1.36 c-0.98,1.37-2.58,2.26-4.4,2.26c-2.98,0-5.4-2.42-5.4-5.4c0-1.81,0.89-3.42,2.26-4.4C12.92,3.04,12.46,3,12,3L12,3z"/></svg><svg viewBox="0 0 24 24" width=24 height=24 aria-hidden=true class="toggleIcon_Q188 systemToggleIcon_cOyE"><path fill=currentColor d="m12 21c4.971 0 9-4.029 9-9s-4.029-9-9-9-9 4.029-9 9 4.029 9 9 9zm4.95-13.95c1.313 1.313 2.05 3.093 2.05 4.95s-0.738 3.637-2.05 4.95c-1.313 1.313-3.093 2.05-4.95 2.05v-14c1.857 0 3.637 0.737 4.95 2.05z"/></svg></button></div><div class=navbarSearchContainer_d_Bn><button type=button class="DocSearch DocSearch-Button" aria-label="Search (Meta+k)" aria-keyshortcuts=Meta+k><span class=DocSearch-Button-Container><svg width=20 height=20 class=DocSearch-Search-Icon viewBox="0 0 24 24" aria-hidden=true><circle cx=11 cy=11 r=8 stroke=currentColor fill=none stroke-width=1.4 /><path d="m21 21-4.3-4.3" stroke=currentColor fill=none stroke-linecap=round stroke-linejoin=round /></svg><span class=DocSearch-Button-Placeholder>Search</span></span><span class=DocSearch-Button-Keys></span></button></div></div></div><div role=presentation class=navbar-sidebar__backdrop></div></nav><div id=__docusaurus_skipToContent_fallback class="theme-layout-main main-wrapper mainWrapper_c22C"><div class=docsWrapper_oymo><button aria-label="Scroll back to top" class="clean-btn theme-back-to-top-button backToTopButton_ZPA9" type=button></button><div class=docRoot_ng2Q><aside class="theme-doc-sidebar-container docSidebarContainer_nyHU"><div class=sidebarViewport_rLoj><div class=sidebar_TmDF><nav aria-label="Docs sidebar" class="menu thin-scrollbar menu_jRin"><ul class="theme-doc-sidebar-menu menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class=menu__link href=/notes><span title=笔记 class=linkLabel_JPPZ>笔记</span></a><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="categoryLink_irRK menu__link menu__link--sublist menu__link--sublist-caret" role=button aria-expanded=false href=/notes/adobe/photoshop><span title=adobe class=categoryLinkLabel_RkOu>adobe</span></a></div><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item"><div class=menu__list-item-collapsible><a class="categoryLink_irRK menu__link menu__link--sublist menu__link--active" href=/notes/ai><span title=AI class=categoryLinkLabel_RkOu>AI</span></a><button aria-label="Collapse sidebar category 'AI'" aria-expanded=true type=button class="clean-btn menu__caret"></button></div><ul class=menu__list><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class=menu__link tabindex=0 href=/notes/ai/awesome><span title="AI Awesome" class=linkLabel_JPPZ>AI Awesome</span></a><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class=menu__link tabindex=0 href=/notes/ai/faq><span title="AI FAQ" class=linkLabel_JPPZ>AI FAQ</span></a><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class=menu__link tabindex=0 href=/notes/ai/glossary><span title="AI Glossary" class=linkLabel_JPPZ>AI Glossary</span></a><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="categoryLink_irRK menu__link menu__link--sublist menu__link--sublist-caret" role=button aria-expanded=false tabindex=0 href=/notes/ai/coding/continue><span title=coding class=categoryLinkLabel_RkOu>coding</span></a></div><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="categoryLink_irRK menu__link menu__link--sublist menu__link--sublist-caret" role=button aria-expanded=false tabindex=0 href=/notes/ai/dev/agent/awesome><span title=dev class=categoryLinkLabel_RkOu>dev</span></a></div><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="categoryLink_irRK menu__link menu__link--sublist" tabindex=0 href=/notes/ai/diffusion><span title=Diffusion class=categoryLinkLabel_RkOu>Diffusion</span></a><button aria-label="Expand sidebar category 'Diffusion'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class=menu__link tabindex=0 href=/notes/ai/embedding><span title=Embedding class=linkLabel_JPPZ>Embedding</span></a><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="categoryLink_irRK menu__link menu__link--sublist" tabindex=0 href=/notes/ai/gan><span title=GANs class=categoryLinkLabel_RkOu>GANs</span></a><button aria-label="Expand sidebar category 'GANs'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="categoryLink_irRK menu__link menu__link--sublist" tabindex=0 href=/notes/ai/gpt><span title=GPT class=categoryLinkLabel_RkOu>GPT</span></a><button aria-label="Expand sidebar category 'GPT'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="categoryLink_irRK menu__link menu__link--sublist" tabindex=0 href=/notes/ai/inference><span title=推理 class=categoryLinkLabel_RkOu>推理</span></a><button aria-label="Expand sidebar category '推理'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item"><div class=menu__list-item-collapsible><a class="categoryLink_irRK menu__link menu__link--sublist menu__link--active" tabindex=0 href=/notes/ai/llm><span title=LLM class=categoryLinkLabel_RkOu>LLM</span></a><button aria-label="Collapse sidebar category 'LLM'" aria-expanded=true type=button class="clean-btn menu__caret"></button></div><ul class=menu__list><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link menu__link--active" aria-current=page tabindex=0 href=/notes/ai/llm/llama><span title=LLaMa class=linkLabel_JPPZ>LLaMa</span></a><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class=menu__link tabindex=0 href=/notes/ai/llm/awesome><span title="LLM Awesome" class=linkLabel_JPPZ>LLM Awesome</span></a><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class=menu__link tabindex=0 href=/notes/ai/llm/llmstxt><span title=llms.txt class=linkLabel_JPPZ>llms.txt</span></a><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class=menu__link tabindex=0 href=/notes/ai/llm/lm-faq><span title="LLM FAQ" class=linkLabel_JPPZ>LLM FAQ</span></a><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class=menu__link tabindex=0 href=/notes/ai/llm/lm-glossary><span title="LLM Glossary" class=linkLabel_JPPZ>LLM Glossary</span></a><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class=menu__link tabindex=0 href=/notes/ai/llm/lm-instruction><span title=instruction class=linkLabel_JPPZ>instruction</span></a><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class=menu__link tabindex=0 href=/notes/ai/llm/lm-playground><span title=Playground class=linkLabel_JPPZ>Playground</span></a><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class=menu__link tabindex=0 href=/notes/ai/llm/lm-pricing><span title=Pricing class=linkLabel_JPPZ>Pricing</span></a><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class=menu__link tabindex=0 href=/notes/ai/llm/lm-test><span title=lm-test class=linkLabel_JPPZ>lm-test</span></a><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class=menu__link tabindex=0 href=/notes/ai/llm/rwkv><span title=RWKV class=linkLabel_JPPZ>RWKV</span></a></ul><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="categoryLink_irRK menu__link menu__link--sublist" tabindex=0 href=/notes/ai/ml><span title=机器学习 class=categoryLinkLabel_RkOu>机器学习</span></a><button aria-label="Expand sidebar category '机器学习'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="categoryLink_irRK menu__link menu__link--sublist menu__link--sublist-caret" role=button aria-expanded=false tabindex=0 href=/notes/ai/model/awesome><span title=model class=categoryLinkLabel_RkOu>model</span></a></div><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class=menu__link tabindex=0 href=/notes/ai/ner><span title="Named Entity Recognition" class=linkLabel_JPPZ>Named Entity Recognition</span></a><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="categoryLink_irRK menu__link menu__link--sublist" tabindex=0 href=/notes/ai/nlp><span title=NLP class=categoryLinkLabel_RkOu>NLP</span></a><button aria-label="Expand sidebar category 'NLP'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="categoryLink_irRK menu__link menu__link--sublist" tabindex=0 href=/notes/ai/ocr><span title=OCR class=categoryLinkLabel_RkOu>OCR</span></a><button aria-label="Expand sidebar category 'OCR'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class=menu__link tabindex=0 href=/notes/ai/precision><span title=精度 class=linkLabel_JPPZ>精度</span></a><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class=menu__link tabindex=0 href=/notes/ai/prompt-awesome><span title="Prompt Awesome" class=linkLabel_JPPZ>Prompt Awesome</span></a><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="categoryLink_irRK menu__link menu__link--sublist" tabindex=0 href=/notes/ai/service><span title="AI Service" class=categoryLinkLabel_RkOu>AI Service</span></a><button aria-label="Expand sidebar category 'AI Service'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="categoryLink_irRK menu__link menu__link--sublist" tabindex=0 href=/notes/ai/traning><span title=训练 class=categoryLinkLabel_RkOu>训练</span></a><button aria-label="Expand sidebar category '训练'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class=menu__link tabindex=0 href=/notes/ai/vision><span title=视觉 class=linkLabel_JPPZ>视觉</span></a><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="categoryLink_irRK menu__link menu__link--sublist" tabindex=0 href=/notes/ai/voice><span title=Voice class=categoryLinkLabel_RkOu>Voice</span></a><button aria-label="Expand sidebar category 'Voice'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div></ul><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="categoryLink_irRK menu__link menu__link--sublist" href=/notes/algorithm><span title=算法 class=categoryLinkLabel_RkOu>算法</span></a><button aria-label="Expand sidebar category '算法'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="categoryLink_irRK menu__link menu__link--sublist" href=/notes/blockchain><span title=区块链 class=categoryLinkLabel_RkOu>区块链</span></a><button aria-label="Expand sidebar category '区块链'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="categoryLink_irRK menu__link menu__link--sublist" href=/notes/cn><span title=CN class=categoryLinkLabel_RkOu>CN</span></a><button aria-label="Expand sidebar category 'CN'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="categoryLink_irRK menu__link menu__link--sublist" href=/notes/courses><span title=课程 class=categoryLinkLabel_RkOu>课程</span></a><button aria-label="Expand sidebar category '课程'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="categoryLink_irRK menu__link menu__link--sublist" href=/notes/culture><span title=文化 class=categoryLinkLabel_RkOu>文化</span></a><button aria-label="Expand sidebar category '文化'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="categoryLink_irRK menu__link menu__link--sublist" href=/notes/db><span title=数据库 class=categoryLinkLabel_RkOu>数据库</span></a><button aria-label="Expand sidebar category '数据库'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="categoryLink_irRK menu__link menu__link--sublist" href=/notes/dev><span title=开发 class=categoryLinkLabel_RkOu>开发</span></a><button aria-label="Expand sidebar category '开发'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="categoryLink_irRK menu__link menu__link--sublist" href=/notes/devops><span title=DevOps class=categoryLinkLabel_RkOu>DevOps</span></a><button aria-label="Expand sidebar category 'DevOps'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="categoryLink_irRK menu__link menu__link--sublist" href=/notes/economics><span title=经济学 class=categoryLinkLabel_RkOu>经济学</span></a><button aria-label="Expand sidebar category '经济学'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="categoryLink_irRK menu__link menu__link--sublist" href=/notes/electrical><span title=电学基础 class=categoryLinkLabel_RkOu>电学基础</span></a><button aria-label="Expand sidebar category '电学基础'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="categoryLink_irRK menu__link menu__link--sublist menu__link--sublist-caret" role=button aria-expanded=false href=/notes/embedded/awesome><span title=embedded class=categoryLinkLabel_RkOu>embedded</span></a></div><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="categoryLink_irRK menu__link menu__link--sublist" href=/notes/evolve><span title=自我成长 class=categoryLinkLabel_RkOu>自我成长</span></a><button aria-label="Expand sidebar category '自我成长'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="categoryLink_irRK menu__link menu__link--sublist" href=/notes/hardware><span title=硬件 class=categoryLinkLabel_RkOu>硬件</span></a><button aria-label="Expand sidebar category '硬件'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="categoryLink_irRK menu__link menu__link--sublist" href=/notes/healthcare><span title=健康 class=categoryLinkLabel_RkOu>健康</span></a><button aria-label="Expand sidebar category '健康'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="categoryLink_irRK menu__link menu__link--sublist menu__link--sublist-caret" role=button aria-expanded=false href=/notes/howto/network/dns-prevent-spoofing><span title=howto class=categoryLinkLabel_RkOu>howto</span></a></div><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="categoryLink_irRK menu__link menu__link--sublist" href=/notes/java><span title=Java class=categoryLinkLabel_RkOu>Java</span></a><button aria-label="Expand sidebar category 'Java'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="categoryLink_irRK menu__link menu__link--sublist" href=/notes/languages><span title=语言 class=categoryLinkLabel_RkOu>语言</span></a><button aria-label="Expand sidebar category '语言'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class=menu__link href=/notes/linguistics><span title=Linguistics class=linkLabel_JPPZ>Linguistics</span></a><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="categoryLink_irRK menu__link menu__link--sublist" href=/notes/math><span title=数学 class=categoryLinkLabel_RkOu>数学</span></a><button aria-label="Expand sidebar category '数学'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="categoryLink_irRK menu__link menu__link--sublist" href=/notes/mgmt><span title=管理 class=categoryLinkLabel_RkOu>管理</span></a><button aria-label="Expand sidebar category '管理'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="categoryLink_irRK menu__link menu__link--sublist menu__link--sublist-caret" role=button aria-expanded=false href=/notes/network/application/dns><span title=network class=categoryLinkLabel_RkOu>network</span></a></div><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="categoryLink_irRK menu__link menu__link--sublist" href=/notes/ops><span title=运维 class=categoryLinkLabel_RkOu>运维</span></a><button aria-label="Expand sidebar category '运维'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="categoryLink_irRK menu__link menu__link--sublist" href=/notes/os><span title=操作系统 class=categoryLinkLabel_RkOu>操作系统</span></a><button aria-label="Expand sidebar category '操作系统'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="categoryLink_irRK menu__link menu__link--sublist" href=/notes/philosophy><span title=理念 class=categoryLinkLabel_RkOu>理念</span></a><button aria-label="Expand sidebar category '理念'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="categoryLink_irRK menu__link menu__link--sublist" href=/notes/photography><span title=摄影 class=categoryLinkLabel_RkOu>摄影</span></a><button aria-label="Expand sidebar category '摄影'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="categoryLink_irRK menu__link menu__link--sublist" href=/notes/platform><span title=平台 class=categoryLinkLabel_RkOu>平台</span></a><button aria-label="Expand sidebar category '平台'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="categoryLink_irRK menu__link menu__link--sublist menu__link--sublist-caret" role=button aria-expanded=false href=/notes/psychology/glossary><span title=psychology class=categoryLinkLabel_RkOu>psychology</span></a></div><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="categoryLink_irRK menu__link menu__link--sublist" href=/notes/queue><span title=Queue class=categoryLinkLabel_RkOu>Queue</span></a><button aria-label="Expand sidebar category 'Queue'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="categoryLink_irRK menu__link menu__link--sublist" href=/notes/security><span title=安全 class=categoryLinkLabel_RkOu>安全</span></a><button aria-label="Expand sidebar category '安全'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="categoryLink_irRK menu__link menu__link--sublist" href=/notes/service><span title=服务 class=categoryLinkLabel_RkOu>服务</span></a><button aria-label="Expand sidebar category '服务'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="categoryLink_irRK menu__link menu__link--sublist" href=/notes/software><span title=软件 class=categoryLinkLabel_RkOu>软件</span></a><button aria-label="Expand sidebar category '软件'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="categoryLink_irRK menu__link menu__link--sublist" href=/notes/std><span title=标准数据 class=categoryLinkLabel_RkOu>标准数据</span></a><button aria-label="Expand sidebar category '标准数据'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="categoryLink_irRK menu__link menu__link--sublist" href=/notes/voip><span title=VoIP class=categoryLinkLabel_RkOu>VoIP</span></a><button aria-label="Expand sidebar category 'VoIP'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="categoryLink_irRK menu__link menu__link--sublist" href=/notes/web><span title=Web class=categoryLinkLabel_RkOu>Web</span></a><button aria-label="Expand sidebar category 'Web'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div></ul></nav><button type=button title="Collapse sidebar" aria-label="Collapse sidebar" class="button button--secondary button--outline collapseSidebarButton_NY7h"><svg width=20 height=20 aria-hidden=true class=collapseSidebarButtonIcon_c8nQ><g fill=#7a7a7a><path d="M9.992 10.023c0 .2-.062.399-.172.547l-4.996 7.492a.982.982 0 01-.828.454H1c-.55 0-1-.453-1-1 0-.2.059-.403.168-.551l4.629-6.942L.168 3.078A.939.939 0 010 2.528c0-.548.45-.997 1-.997h2.996c.352 0 .649.18.828.45L9.82 9.472c.11.148.172.347.172.55zm0 0"/><path d="M19.98 10.023c0 .2-.058.399-.168.547l-4.996 7.492a.987.987 0 01-.828.454h-3c-.547 0-.996-.453-.996-1 0-.2.059-.403.168-.551l4.625-6.942-4.625-6.945a.939.939 0 01-.168-.55 1 1 0 01.996-.997h3c.348 0 .649.18.828.45l4.996 7.492c.11.148.168.347.168.55zm0 0"/></g></svg></button></div></div></aside><main class=docMainContainer_GAGP><div class="container padding-top--md padding-bottom--lg"><div class=row><div class="col docItemCol_YQ1L"><div class=docItemContainer_Wjg4><article><nav class="theme-doc-breadcrumbs breadcrumbsContainer_tiqF" aria-label=Breadcrumbs><ul class=breadcrumbs><li class=breadcrumbs__item><a aria-label="Home page" class=breadcrumbs__link href=/><svg viewBox="0 0 24 24" class=breadcrumbHomeIcon_WTWT><path d="M10 19v-5h4v5c0 .55.45 1 1 1h3c.55 0 1-.45 1-1v-7h1.7c.46 0 .68-.57.33-.87L12.67 3.6c-.38-.34-.96-.34-1.34 0l-8.36 7.53c-.34.3-.13.87.33.87H5v7c0 .55.45 1 1 1h3c.55 0 1-.45 1-1z" fill=currentColor /></svg></a><li class=breadcrumbs__item><a class=breadcrumbs__link href=/notes/ai><span>AI</span></a><li class=breadcrumbs__item><a class=breadcrumbs__link href=/notes/ai/llm><span>LLM</span></a><li class="breadcrumbs__item breadcrumbs__item--active"><span class=breadcrumbs__link>LLaMa</span></ul></nav><div class="tocCollapsible_GvsH theme-doc-toc-mobile tocMobile_qJgM"><button type=button class="clean-btn tocCollapsibleButton_LnP0">On this page</button></div><div class="theme-doc-markdown markdown"><header><h1>llama</h1></header>
<ul>
<li>LLaMA-7B, 3.5GB, 6GB</li>
<li>LLaMA-13B, 6.5GB, 10GB</li>
<li>LLaMA-30B, 15.8GB, 20GB</li>
<li>LLaMA-65B, 31.2GB, 40GB</li>
<li><a href="https://news.ycombinator.com/item?id=35107058" target=_blank rel="noopener noreferrer">https://news.ycombinator.com/item?id=35107058</a></li>
<li><a href=https://github.com/ZrrSkywalker/LLaMA-Adapter target=_blank rel="noopener noreferrer">https://github.com/ZrrSkywalker/LLaMA-Adapter</a></li>
<li><a href=https://huggingface.co/blog/stackllama target=_blank rel="noopener noreferrer">https://huggingface.co/blog/stackllama</a></li>
</ul>
<div class="language-bash codeBlockContainer_ljD1 theme-code-block" style=--prism-color:#bfc7d5;--prism-background-color:#292d3e><div class=codeBlockContent_rhaG><pre tabindex=0 class="prism-code language-bash codeBlock_mx9Q thin-scrollbar" style=color:#bfc7d5;background-color:#292d3e><code class=codeBlockLines_NG8l><span class=token-line style=color:#bfc7d5><span class="token plain"># py for</span><br/></span><span class=token-line style=color:#bfc7d5><span class="token plain">apk add \</span><br/></span><span class=token-line style=color:#bfc7d5><span class="token plain">  gcc g++ python3 py3-pip musl-dev cmake make pkgconf build-base \</span><br/></span><span class=token-line style=color:#bfc7d5><span class="token plain">  git openssh-client binutils coreutils util-linux findutils sed grep tar wget curl neofetch \</span><br/></span><span class=token-line style=color:#bfc7d5><span class="token plain">  rust cargo python3-dev openssl-dev linux-headers</span><br/></span><span class=token-line style=color:#bfc7d5><span class="token plain" style=display:inline-block></span><br/></span><span class=token-line style=color:#bfc7d5><span class="token plain"># llama.cpp</span><br/></span><span class=token-line style=color:#bfc7d5><span class="token plain"># =========</span><br/></span><span class=token-line style=color:#bfc7d5><span class="token plain">git clone https://github.com/ggerganov/llama.cpp.git</span><br/></span><span class=token-line style=color:#bfc7d5><span class="token plain">cd llama.cpp</span><br/></span><span class=token-line style=color:#bfc7d5><span class="token plain">make -j</span><br/></span><span class=token-line style=color:#bfc7d5><span class="token plain" style=display:inline-block></span><br/></span><span class=token-line style=color:#bfc7d5><span class="token plain">./main -m ./models/7B/ggml-model-q4_0.bin -p "Building a website can be done in 10 simple steps:" -n 512</span><br/></span><span class=token-line style=color:#bfc7d5><span class="token plain">./main -m ./models/7B/ggml-model-q4_0.bin --file prompts/alpaca.txt --instruct --ctx_size 2048 --keep -1</span><br/></span><span class=token-line style=color:#bfc7d5><span class="token plain" style=display:inline-block></span><br/></span><span class=token-line style=color:#bfc7d5><span class="token plain">./main -m ./models/ggml-alpaca-7b-q4.bin --color -f ./prompts/alpaca.txt -ins -b 256 --top_k 10000 --temp 0.2 --repeat_penalty 1 -t 7</span><br/></span><span class=token-line style=color:#bfc7d5><span class="token plain" style=display:inline-block></span><br/></span><span class=token-line style=color:#bfc7d5><span class="token plain"># https://github.com/ymcui/Chinese-LLaMA-Alpaca</span><br/></span><span class=token-line style=color:#bfc7d5><span class="token plain"># =========</span><br/></span><span class=token-line style=color:#bfc7d5><span class="token plain">apk add rust cargo python3-dev openssl-dev cmake linux-headers</span><br/></span><span class=token-line style=color:#bfc7d5><span class="token plain">pip install git+https://github.com/huggingface/transformers</span><br/></span><span class=token-line style=color:#bfc7d5><span class="token plain">pip install sentencepiece</span><br/></span><span class=token-line style=color:#bfc7d5><span class="token plain">pip install torch --index-url https://download.pytorch.org/whl/cpu</span><br/></span><span class=token-line style=color:#bfc7d5><span class="token plain">pip install peft</span><br/></span><span class=token-line style=color:#bfc7d5><span class="token plain" style=display:inline-block></span><br/></span><span class=token-line style=color:#bfc7d5><span class="token plain">git clone https://github.com/huggingface/transformers</span><br/></span><span class=token-line style=color:#bfc7d5><span class="token plain" style=display:inline-block></span><br/></span><span class=token-line style=color:#bfc7d5><span class="token plain"># musl pthread_attr_setaffinity_np</span><br/></span><span class=token-line style=color:#bfc7d5><span class="token plain">python ./transformers/src/transformers/models/llama/convert_llama_weights_to_hf.py \</span><br/></span><span class=token-line style=color:#bfc7d5><span class="token plain">  --input_dir /ml/models/LLaMA \</span><br/></span><span class=token-line style=color:#bfc7d5><span class="token plain">  --model_size 7B \</span><br/></span><span class=token-line style=color:#bfc7d5><span class="token plain">  --output_dir /ml/models/LLaMA-hf</span><br/></span></code></pre></div></div>
<p><strong>ggml</strong></p>
<ul>
<li><a href=https://github.com/ggerganov/llama.cpp/pull/1305 target=_blank rel="noopener noreferrer">https://github.com/ggerganov/llama.cpp/pull/1305</a></li>
<li>ggjt v3</li>
</ul>
<h1>FAQ</h1>
<h1>Ref</h1>
<hr/>
<ul>
<li><a href=https://rentry.org/lmg_models target=_blank rel="noopener noreferrer">https://rentry.org/lmg_models</a></li>
<li><a href=https://rentry.org/lmg-resources target=_blank rel="noopener noreferrer">https://rentry.org/lmg-resources</a></li>
<li><a href=https://rentry.org/ayumi_erp_rating target=_blank rel="noopener noreferrer">https://rentry.org/ayumi_erp_rating</a></li>
<li><a href=https://rentry.co/ALLMRR target=_blank rel="noopener noreferrer">https://rentry.co/ALLMRR</a></li>
<li><a href=http://ayumi.m8geil.de/ target=_blank rel="noopener noreferrer">http://ayumi.m8geil.de/</a></li>
<li>ERP - erotic role playing - 情色角色扮演</li>
</ul>
<p>#->/lmg/ Model Links and Torrents &lt;-</p>
<p>[TOC2]</p>
<h2 class="anchor anchorWithStickyNavbar_EG6R" id=changelog-mdy>Changelog (MDY)<a href=#changelog-mdy class=hash-link aria-label="Direct link to Changelog (MDY)" title="Direct link to Changelog (MDY)" translate=no>​</a></h2>
<p>[05-10-2023] - Added WizardLM 13B Uncensored
[05-07-2023] - Added Vicuna 13B Cocktail, bluemoonrp-13b & AlpacaDente2
[05-05-2023] - Added CPU quantization variation links
[05-02-2023] - Initial Rentry</p>
<h2 class="anchor anchorWithStickyNavbar_EG6R" id=4-bit-gpu-model-requirements>4-bit GPU Model Requirements<a href=#4-bit-gpu-model-requirements class=hash-link aria-label="Direct link to 4-bit GPU Model Requirements" title="Direct link to 4-bit GPU Model Requirements" translate=no>​</a></h2>
<p>!!! note VRAM Required takes full context (2048) into account. You may be able to load the model on GPU's with slightly lower VRAM, but you will not be able to run at full context. If you do not have enough RAM to load model, it will load into swap. Groupsize models will increase VRAM usage, as will running a LoRA alongside the model.</p>



































<table><thead><tr><th>Model Parameters<th>VRAM Required<th>GPU Examples<th>RAM to Load<tbody><tr><td>7B<td>8GB<td>RTX 1660, 2060, AMD 5700xt, RTX 3050, RTX 3060, RTX 3070<td>6 GB<tr><td>13B<td>12GB<td>AMD 6900xt, RTX 2060 12GB, 3060 12GB, 3080 12GB, A2000<td>12GB<tr><td>30B<td>24GB<td>RTX 3090, RTX 4090, A4500, A5000, 6000, Tesla V100<td>32GB<tr><td>65B<td>42GB<td>A100 80GB, NVIDIA Quadro RTX 8000, Quadro RTX A6000<td>64GB</table>
<h2 class="anchor anchorWithStickyNavbar_EG6R" id=4-bit-cpullamacpp-ram-requirements>4-bit CPU/llama.cpp RAM Requirements<a href=#4-bit-cpullamacpp-ram-requirements class=hash-link aria-label="Direct link to 4-bit CPU/llama.cpp RAM Requirements" title="Direct link to 4-bit CPU/llama.cpp RAM Requirements" translate=no>​</a></h2>
<p>!!! note 5bit to 8bit Quantized models are becoming more common, and will obviously require more RAM. Will update these with the numbers when I have them.</p>



































<table><thead><tr><th>Model<th>4-bit<th>5-bit<th>8-bit<tbody><tr><td>7B<td>3.9 GB<td><td><tr><td>13B<td>7.8 GB<td><td><tr><td>30B<td>19.5 GB<td><td><tr><td>65B<td>38.5 GB<td><td></table>
<h1>Original Weights</h1>
<h2 class="anchor anchorWithStickyNavbar_EG6R" id=llama-16-bit-weights>LLaMA 16-bit Weights<a href=#llama-16-bit-weights class=hash-link aria-label="Direct link to LLaMA 16-bit Weights" title="Direct link to LLaMA 16-bit Weights" translate=no>​</a></h2>
<p>!!! info</p>
<div class="language-text codeBlockContainer_ljD1 theme-code-block" style=--prism-color:#bfc7d5;--prism-background-color:#292d3e><div class=codeBlockContent_rhaG><pre tabindex=0 class="prism-code language-text codeBlock_mx9Q thin-scrollbar" style=color:#bfc7d5;background-color:#292d3e><code class=codeBlockLines_NG8l><span class=token-line style=color:#bfc7d5><span class="token plain">The original LLaMA weights converted to Transformers @ 16bit. A torrent is available as well, but it uses outdated configuration files that will need to be updated. Note that these aren't for general use, as the VRAM requirements are beyond consumer scope.</span><br/></span><span class=token-line style=color:#bfc7d5><span class="token plain" style=display:inline-block></span><br/></span><span class=token-line style=color:#bfc7d5><span class="token plain">>Filtering : None</span><br/></span></code></pre></div></div>



































<table><thead><tr><th>Model<th>Type<th>Download<tbody><tr><td>7B 16bit<td>HF Format<td><a href=https://huggingface.co/Neko-Institute-of-Science/LLaMA-7B-HF target=_blank rel="noopener noreferrer">HuggingFace</a><tr><td>13B 16bit<td>HF Format<td><a href=https://huggingface.co/Neko-Institute-of-Science/LLaMA-13B-HF target=_blank rel="noopener noreferrer">HuggingFace</a><tr><td>30B 16bit<td>HF Format<td><a href=https://huggingface.co/Neko-Institute-of-Science/LLaMA-30B-HF target=_blank rel="noopener noreferrer">HuggingFace</a><tr><td>65B 16bit<td>HF Format<td><a href=https://huggingface.co/Neko-Institute-of-Science/LLaMA-13B-HF target=_blank rel="noopener noreferrer">HuggingFace</a><tr><td>All the above<td>HF Format<td><a href="magnet:?xt=urn:btih:8d634925911a03f787d9f68ac075a9b24281573a&dn=Safe-LLaMA-HF-v2%20(4-04-23)&tr=http%3a%2f%2fbt2.archive.org%3a6969%2fannounce&tr=http%3a%2f%2fbt1.archive.org%3a6969%2fannounce" target=_blank rel="noopener noreferrer">Torrent Magnet</a></table>
<h2 class="anchor anchorWithStickyNavbar_EG6R" id=llama-4-bit-weights>LLaMA 4-bit Weights<a href=#llama-4-bit-weights class=hash-link aria-label="Direct link to LLaMA 4-bit Weights" title="Direct link to LLaMA 4-bit Weights" translate=no>​</a></h2>
<p>!!! info</p>
<div class="language-text codeBlockContainer_ljD1 theme-code-block" style=--prism-color:#bfc7d5;--prism-background-color:#292d3e><div class=codeBlockContent_rhaG><pre tabindex=0 class="prism-code language-text codeBlock_mx9Q thin-scrollbar" style=color:#bfc7d5;background-color:#292d3e><code class=codeBlockLines_NG8l><span class=token-line style=color:#bfc7d5><span class="token plain">The original LLaMA weights quantized to 4-bit. The GPU CUDA versions have outdated tokenizer and configuration files. It is recommended to either update them with [this](https://rentry.org/544p2) or use the [universal LLaMA tokenizer.](https://github.com/oobabooga/text-generation-webui/blob/main/docs/LLaMA-model.md#option-1-pre-converted-weights)</span><br/></span><span class=token-line style=color:#bfc7d5><span class="token plain" style=display:inline-block></span><br/></span><span class=token-line style=color:#bfc7d5><span class="token plain">>Filtering : None</span><br/></span></code></pre></div></div>






























<table><thead><tr><th>Model<th>Type<th>Download<tbody><tr><td>7B, 13B, 30B, 65B<td>CPU<td><a href="magnet:?xt=urn:btih:481dee5424b7024433504803a90efd32dae40fdf&dn=LLaMA-ggml-4bit_2023-03-31&tr=udp%3a%2f%2ftracker1.bt.moack.co.kr%3a80%2fannounce&tr=udp%3a%2f%2ftracker.torrent.eu.org%3a451%2fannounce&tr=udp%3a%2f%2ftracker.altrosky.nl%3a6969%2fannounce&tr=udp%3a%2f%2fopentracker.i2p.rocks%3a6969%2fannounce&tr=udp%3a%2f%2ftracker.opentrackr.org%3a1337%2fannounce&tr=https%3a%2f%2fopentracker.i2p.rocks%3a443%2fannounce&tr=udp%3a%2f%2ftracker.theoks.net%3a6969%2fannounce&tr=udp%3a%2f%2ftracker-udp.gbitt.info%3a80%2fannounce&tr=udp%3a%2f%2ftracker.tiny-vps.com%3a6969%2fannounce&tr=udp%3a%2f%2ftracker.moeking.me%3a6969%2fannounce&tr=udp%3a%2f%2f9.rarbg.com%3a2810%2fannounce&tr=udp%3a%2f%2ftracker.openbittorrent.com%3a6969%2fannounce&tr=udp%3a%2f%2ftracker.monitorit4.me%3a6969%2fannounce&tr=udp%3a%2f%2ftracker.dler.org%3a6969%2fannounce&tr=http%3a%2f%2ftracker.openbittorrent.com%3a80%2fannounce&tr=udp%3a%2f%2ftracker2.dler.org%3a80%2fannounce" target=_blank rel="noopener noreferrer">Torrent Magnet</a><tr><td>7B, 13B, 30B, 65B<td>GPU CUDA (no groupsize)<td><a href="magnet:?xt=urn:btih:e88abf1b84290b162f00d3a9d79fb4f8719c2053&dn=LLaMA-HF-4bit&tr=http%3a%2f%2fbt2.archive.org%3a6969%2fannounce&tr=http%3a%2f%2fbt1.archive.org%3a6969%2fannounce" target=_blank rel="noopener noreferrer">Torrent Magnet</a><tr><td>7B, 13B, 30B, 65B<td>GPU CUDA (128gs)<td><a href="magnet:?xt=urn:btih:88f7d9d2460ffcaf78b21e83012de00939eacb65&dn=LLaMA-HF-4bit-128g&tr=http%3a%2f%2fbt2.archive.org%3a6969%2fannounce&tr=http%3a%2f%2fbt1.archive.org%3a6969%2fannounce" target=_blank rel="noopener noreferrer">Torrent Magnet</a><tr><td>7B, 13B, 30B, 65B<td>GPU Triton<td><a href=https://huggingface.co/Neko-Institute-of-Science target=_blank rel="noopener noreferrer">Neko Institute of Science HF page</a></table>
<h1>Models/Finetunes/LoRA's</h1>
<h2 class="anchor anchorWithStickyNavbar_EG6R" id=wizardlm-13b-uncensored-05102023>WizardLM 13B Uncensored (05/10/2023)<a href=#wizardlm-13b-uncensored-05102023 class=hash-link aria-label="Direct link to WizardLM 13B Uncensored (05/10/2023)" title="Direct link to WizardLM 13B Uncensored (05/10/2023)" translate=no>​</a></h2>
<p>!!! info</p>
<div class="language-text codeBlockContainer_ljD1 theme-code-block" style=--prism-color:#bfc7d5;--prism-background-color:#292d3e><div class=codeBlockContent_rhaG><pre tabindex=0 class="prism-code language-text codeBlock_mx9Q thin-scrollbar" style=color:#bfc7d5;background-color:#292d3e><code class=codeBlockLines_NG8l><span class=token-line style=color:#bfc7d5><span class="token plain">This is WizardLM trained with a subset of the dataset - responses that contained alignment / moralizing were removed. The intent is to train a WizardLM that doesn't have alignment built-in, so that alignment (of any sort) can be added separately with for example with a RLHF LoRA.</span><br/></span><span class=token-line style=color:#bfc7d5><span class="token plain" style=display:inline-block></span><br/></span><span class=token-line style=color:#bfc7d5><span class="token plain">Note that despite being an "uncensored" model, several tests have demonstrated that the model will still refuse to comply with certain requests.</span><br/></span><span class=token-line style=color:#bfc7d5><span class="token plain" style=display:inline-block></span><br/></span><span class=token-line style=color:#bfc7d5><span class="token plain">>Filtering : Light</span><br/></span></code></pre></div></div>




















<table><thead><tr><th>Model<th>Type<th>Download<tbody><tr><td>13B GGML<td>CPU<td><a href=https://huggingface.co/TehVenom/WizardLM-13B-Uncensored-Q5_1-GGML target=_blank rel="noopener noreferrer">Q5</a><tr><td>13B<td>GPU<td><a href=https://huggingface.co/ausboss/WizardLM-13B-Uncensored-4bit-128g target=_blank rel="noopener noreferrer">Q4 CUDA 128gs</a></table>
<h2 class="anchor anchorWithStickyNavbar_EG6R" id=bluemoonrp-13b-05072023>BluemoonRP 13B (05/07/2023)<a href=#bluemoonrp-13b-05072023 class=hash-link aria-label="Direct link to BluemoonRP 13B (05/07/2023)" title="Direct link to BluemoonRP 13B (05/07/2023)" translate=no>​</a></h2>
<p>!!! info</p>
<div class="language-text codeBlockContainer_ljD1 theme-code-block" style=--prism-color:#bfc7d5;--prism-background-color:#292d3e><div class=codeBlockContent_rhaG><pre tabindex=0 class="prism-code language-text codeBlock_mx9Q thin-scrollbar" style=color:#bfc7d5;background-color:#292d3e><code class=codeBlockLines_NG8l><span class=token-line style=color:#bfc7d5><span class="token plain">An RP/ERP focused finetune of LLaMA 13B finetuned on BluemoonRP logs. It is designed to simulate a 2-person RP session. Two versions are provided; a standard 13B with 2K context and an experimental 13B with 4K context. It has a non-standard format (LEAD/ASSOCIATE), so ensure that you read the model card and use the correct syntax.</span><br/></span><span class=token-line style=color:#bfc7d5><span class="token plain" style=display:inline-block></span><br/></span><span class=token-line style=color:#bfc7d5><span class="token plain">>Filtering : None</span><br/></span></code></pre></div></div>















<table><thead><tr><th>Model<th>Type<th>Download<tbody><tr><td>13B<td>GPU & CPU<td><a href=https://huggingface.co/reeducator/bluemoonrp-13b target=_blank rel="noopener noreferrer">https://huggingface.co/reeducator/bluemoonrp-13b</a></table>
<h2 class="anchor anchorWithStickyNavbar_EG6R" id=vicuna-13b-cocktail-05072023>Vicuna 13B Cocktail (05/07/2023)<a href=#vicuna-13b-cocktail-05072023 class=hash-link aria-label="Direct link to Vicuna 13B Cocktail (05/07/2023)" title="Direct link to Vicuna 13B Cocktail (05/07/2023)" translate=no>​</a></h2>
<p>!!! info</p>
<div class="language-text codeBlockContainer_ljD1 theme-code-block" style=--prism-color:#bfc7d5;--prism-background-color:#292d3e><div class=codeBlockContent_rhaG><pre tabindex=0 class="prism-code language-text codeBlock_mx9Q thin-scrollbar" style=color:#bfc7d5;background-color:#292d3e><code class=codeBlockLines_NG8l><span class=token-line style=color:#bfc7d5><span class="token plain">Vicuna 1.1 13B finetune incorporating various datasets in addition to the unfiltered ShareGPT. This is an experiment attempting to enhance the creativity of the Vicuna 1.1, while also reducing censorship as much as possible. All datasets have been cleaned. Additionally, only the "instruct" portion of GPTeacher has been used. It has a non-standard format (USER/ASSOCIATE), so ensure that you read the model card and use the correct syntax.</span><br/></span><span class=token-line style=color:#bfc7d5><span class="token plain" style=display:inline-block></span><br/></span><span class=token-line style=color:#bfc7d5><span class="token plain">>Filtering : Light</span><br/></span></code></pre></div></div>















<table><thead><tr><th>Model<th>Type<th>Download<tbody><tr><td>13B<td>GPU & CPU<td><a href=https://huggingface.co/reeducator/vicuna-13b-cocktail target=_blank rel="noopener noreferrer">https://huggingface.co/reeducator/vicuna-13b-cocktail</a></table>
<h2 class="anchor anchorWithStickyNavbar_EG6R" id=gpt4-x-alpacadente2-30b-05052023>GPT4-x-AlpacaDente2-30B (05/05/2023)<a href=#gpt4-x-alpacadente2-30b-05052023 class=hash-link aria-label="Direct link to GPT4-x-AlpacaDente2-30B (05/05/2023)" title="Direct link to GPT4-x-AlpacaDente2-30B (05/05/2023)" translate=no>​</a></h2>
<p>!!! info</p>
<div class="language-text codeBlockContainer_ljD1 theme-code-block" style=--prism-color:#bfc7d5;--prism-background-color:#292d3e><div class=codeBlockContent_rhaG><pre tabindex=0 class="prism-code language-text codeBlock_mx9Q thin-scrollbar" style=color:#bfc7d5;background-color:#292d3e><code class=codeBlockLines_NG8l><span class=token-line style=color:#bfc7d5><span class="token plain">ChanSung's Alpaca-LoRA-30B-elina merged with Open Assistant's second Finetune. Testing in progress.</span><br/></span><span class=token-line style=color:#bfc7d5><span class="token plain" style=display:inline-block></span><br/></span><span class=token-line style=color:#bfc7d5><span class="token plain">>Filtering : Medium</span><br/></span></code></pre></div></div>




















<table><thead><tr><th>Model<th>Type<th>Download<tbody><tr><td>30B GGML<td>CPU<td><a href=https://huggingface.co/Lumpen1/GPT4-x-AlpacaDente2-30b-ggml-q5_0 target=_blank rel="noopener noreferrer">Q5</a><tr><td>30B<td>GPU<td><a href=https://huggingface.co/askmyteapot/GPT4-x-AlpacaDente2-30b-4bit target=_blank rel="noopener noreferrer">Q4 CUDA</a></table>
<p><a href=https://huggingface.co/askmyteapot/GPT4-x-AlpacaDente2-30b-4bit target=_blank rel="noopener noreferrer">https://huggingface.co/askmyteapot/GPT4-x-AlpacaDente2-30b-4bit</a></p>
<h2 class="anchor anchorWithStickyNavbar_EG6R" id=vicuna-13b-free-v11-05012023>Vicuna 13B Free v1.1 (05/01/2023)<a href=#vicuna-13b-free-v11-05012023 class=hash-link aria-label="Direct link to Vicuna 13B Free v1.1 (05/01/2023)" title="Direct link to Vicuna 13B Free v1.1 (05/01/2023)" translate=no>​</a></h2>
<p>!!! info</p>
<div class="language-text codeBlockContainer_ljD1 theme-code-block" style=--prism-color:#bfc7d5;--prism-background-color:#292d3e><div class=codeBlockContent_rhaG><pre tabindex=0 class="prism-code language-text codeBlock_mx9Q thin-scrollbar" style=color:#bfc7d5;background-color:#292d3e><code class=codeBlockLines_NG8l><span class=token-line style=color:#bfc7d5><span class="token plain">A work-in-progress, community driven attempt to make an unfiltered version of Vicuna. It currently has an early stopping bug, and a partial workaround has been posted on the repo's model card.</span><br/></span><span class=token-line style=color:#bfc7d5><span class="token plain" style=display:inline-block></span><br/></span><span class=token-line style=color:#bfc7d5><span class="token plain">>Filtering : Light</span><br/></span></code></pre></div></div>















<table><thead><tr><th>Model<th>Type<th>Download<tbody><tr><td>13B<td>GPU & CPU<td><a href=https://huggingface.co/reeducator/vicuna-13b-free target=_blank rel="noopener noreferrer">https://huggingface.co/reeducator/vicuna-13b-free</a></table>
<h2 class="anchor anchorWithStickyNavbar_EG6R" id=pygmalionmetharme-7b-04302023>Pygmalion/Metharme 7B (04/30/2023)<a href=#pygmalionmetharme-7b-04302023 class=hash-link aria-label="Direct link to Pygmalion/Metharme 7B (04/30/2023)" title="Direct link to Pygmalion/Metharme 7B (04/30/2023)" translate=no>​</a></h2>
<p>!!! info</p>
<div class="language-text codeBlockContainer_ljD1 theme-code-block" style=--prism-color:#bfc7d5;--prism-background-color:#292d3e><div class=codeBlockContent_rhaG><pre tabindex=0 class="prism-code language-text codeBlock_mx9Q thin-scrollbar" style=color:#bfc7d5;background-color:#292d3e><code class=codeBlockLines_NG8l><span class=token-line style=color:#bfc7d5><span class="token plain">Pygmalion 7B is a dialogue model that uses LLaMA-7B as a base. The dataset includes RP/ERP content. Metharme 7B is an experimental instruct-tuned variation, which can be guided using natural language like other instruct models.</span><br/></span><span class=token-line style=color:#bfc7d5><span class="token plain" style=display:inline-block></span><br/></span><span class=token-line style=color:#bfc7d5><span class="token plain">PygmalionAI intend to use the same dataset on the higher parameter LLaMA models. No ETA as of yet.</span><br/></span><span class=token-line style=color:#bfc7d5><span class="token plain" style=display:inline-block></span><br/></span><span class=token-line style=color:#bfc7d5><span class="token plain">>Filtering : None</span><br/></span></code></pre></div></div>



































<table><thead><tr><th>Model<th>Type<th>Download<tbody><tr><td>7B Pygmalion/Metharme<td>XOR<td><a href=https://huggingface.co/PygmalionAI/ target=_blank rel="noopener noreferrer">https://huggingface.co/PygmalionAI/</a><tr><td>7B Pygmalion GGML<td>CPU<td><a href=https://huggingface.co/TehVenom/Pygmalion-7b-4bit-Q4_1-GGML target=_blank rel="noopener noreferrer">Q4</a>, <a href=https://huggingface.co/waifu-workshop/pygmalion-7b-ggml-q5_0 target=_blank rel="noopener noreferrer">Q5</a>, <a href=https://huggingface.co/waifu-workshop/pygmalion-7b-ggml-q8_0 target=_blank rel="noopener noreferrer">Q8</a><tr><td>7B Metharme GGML<td>CPU<td><a href=https://huggingface.co/TehVenom/Metharme-7b-4bit-Q4_1-GGML target=_blank rel="noopener noreferrer">Q4</a>, <a href=https://huggingface.co/waifu-workshop/metharme-7b-ggml-q5_1 target=_blank rel="noopener noreferrer">Q5</a><tr><td>7B Pygmalion<td>GPU<td><a href=https://huggingface.co/TehVenom/Pygmalion-7b-4bit-GPTQ-Safetensors target=_blank rel="noopener noreferrer">Q4 Triton</a>, <a href=https://huggingface.co/gozfarb/pygmalion-7b-4bit-128g-cuda target=_blank rel="noopener noreferrer">Q4 CUDA 128gs</a><tr><td>7B Metharme<td>GPU<td><a href=https://huggingface.co/TehVenom/Metharme-7b-4bit-GPTQ-Safetensors target=_blank rel="noopener noreferrer">Q4 Triton</a>, <a href=https://huggingface.co/askmyteapot/metharme target=_blank rel="noopener noreferrer">Q4 CUDA</a></table>
<h2 class="anchor anchorWithStickyNavbar_EG6R" id=gpt4-x-alpasta-30b-04292023>GPT4-X-Alpasta 30B (04/29/2023)<a href=#gpt4-x-alpasta-30b-04292023 class=hash-link aria-label="Direct link to GPT4-X-Alpasta 30B (04/29/2023)" title="Direct link to GPT4-X-Alpasta 30B (04/29/2023)" translate=no>​</a></h2>
<p>!!! info</p>
<div class="language-text codeBlockContainer_ljD1 theme-code-block" style=--prism-color:#bfc7d5;--prism-background-color:#292d3e><div class=codeBlockContent_rhaG><pre tabindex=0 class="prism-code language-text codeBlock_mx9Q thin-scrollbar" style=color:#bfc7d5;background-color:#292d3e><code class=codeBlockLines_NG8l><span class=token-line style=color:#bfc7d5><span class="token plain">An attempt at improving Open Assistant's performance as an instruct while retaining its excellent prose. The merge consists of Chansung's GPT4-Alpaca Lora and Open Assistant's native fine-tune.</span><br/></span><span class=token-line style=color:#bfc7d5><span class="token plain" style=display:inline-block></span><br/></span><span class=token-line style=color:#bfc7d5><span class="token plain">It is an extremely coherent model for logic based instruct outputs. And while the prose is generally very good, it does suffer from the "Assistant" personality bleedthrough that plagues the OpenAssistant dataset, which can give you dry dialogue for creative writing/chatbot purposes. However, several accounts claim it's nowhere near as bad as OA's finetunes, and that the prose and coherence gains makes up for it.</span><br/></span><span class=token-line style=color:#bfc7d5><span class="token plain" style=display:inline-block></span><br/></span><span class=token-line style=color:#bfc7d5><span class="token plain">>Filtering : Medium</span><br/></span></code></pre></div></div>















<table><thead><tr><th>Model<th>Type<th>Download<tbody><tr><td>30B 4bit<td>CPU & GPU CUDA<td><a href=https://huggingface.co/MetaIX/GPT4-X-Alpasta-30b-4bit target=_blank rel="noopener noreferrer">https://huggingface.co/MetaIX/GPT4-X-Alpasta-30b-4bit</a></table>
<h2 class="anchor anchorWithStickyNavbar_EG6R" id=openassistant-llama-30b-sft-6-04232023>OpenAssistant LLaMa 30B SFT 6 (04/23/2023)<a href=#openassistant-llama-30b-sft-6-04232023 class=hash-link aria-label="Direct link to OpenAssistant LLaMa 30B SFT 6 (04/23/2023)" title="Direct link to OpenAssistant LLaMa 30B SFT 6 (04/23/2023)" translate=no>​</a></h2>
<p>!!! info</p>
<div class="language-text codeBlockContainer_ljD1 theme-code-block" style=--prism-color:#bfc7d5;--prism-background-color:#292d3e><div class=codeBlockContent_rhaG><pre tabindex=0 class="prism-code language-text codeBlock_mx9Q thin-scrollbar" style=color:#bfc7d5;background-color:#292d3e><code class=codeBlockLines_NG8l><span class=token-line style=color:#bfc7d5><span class="token plain">An open-source alternative to OpenAI’s ChatGPT/GPT 3.5 Turbo. However, it seems to suffer from [overfitting](https://www.datarobot.com/wiki/overfitting/) and is heavily filtered. Not recommended for creative writing or chat bots, given the "assistant" personality constantly bleeds through, giving you dry dialogue.</span><br/></span><span class=token-line style=color:#bfc7d5><span class="token plain" style=display:inline-block></span><br/></span><span class=token-line style=color:#bfc7d5><span class="token plain">>Filtering : Heavy</span><br/></span></code></pre></div></div>

























<table><thead><tr><th>Model<th>Type<th>Download<tbody><tr><td>30B<td>XOR<td><a href=https://huggingface.co/OpenAssistant/oasst-sft-6-llama-30b-xor target=_blank rel="noopener noreferrer">https://huggingface.co/OpenAssistant/oasst-sft-6-llama-30b-xor</a><tr><td>30B GGML<td>CPU<td><a href=https://huggingface.co/MildlyAggressiveGoose1/ggml-oasst-sft-6-llama-30B-q4_2 target=_blank rel="noopener noreferrer">Q4</a><tr><td>30B<td>GPU<td><a href=https://huggingface.co/Peeepy/llama-33b-oasst-4bit target=_blank rel="noopener noreferrer">Q4 CUDA</a>, <a href=https://huggingface.co/Peeepy/llama-30b-oasst-4bit-128g target=_blank rel="noopener noreferrer">Q4 CUDA 128gs</a></table>
<h2 class="anchor anchorWithStickyNavbar_EG6R" id=supercot-04222023>SuperCOT (04/22/2023)<a href=#supercot-04222023 class=hash-link aria-label="Direct link to SuperCOT (04/22/2023)" title="Direct link to SuperCOT (04/22/2023)" translate=no>​</a></h2>
<p>!!! info</p>
<div class="language-text codeBlockContainer_ljD1 theme-code-block" style=--prism-color:#bfc7d5;--prism-background-color:#292d3e><div class=codeBlockContent_rhaG><pre tabindex=0 class="prism-code language-text codeBlock_mx9Q thin-scrollbar" style=color:#bfc7d5;background-color:#292d3e><code class=codeBlockLines_NG8l><span class=token-line style=color:#bfc7d5><span class="token plain">SuperCOT is a LoRA trained with the aim of making LLaMa follow prompts for Langchain better, by infusing chain-of-thought datasets, code explanations and instructions, snippets, logical deductions and Alpaca GPT-4 prompts.</span><br/></span><span class=token-line style=color:#bfc7d5><span class="token plain" style=display:inline-block></span><br/></span><span class=token-line style=color:#bfc7d5><span class="token plain">Though designed to improve Langchain, it's quite versatile and works very well for other tasks like creative writing and chatbots. The author also pruned a number of filters from the datasets. As of early May 2023, it's the most recommended model on /lmg/</span><br/></span><span class=token-line style=color:#bfc7d5><span class="token plain" style=display:inline-block></span><br/></span><span class=token-line style=color:#bfc7d5><span class="token plain">>Filtering : Light</span><br/></span></code></pre></div></div>



































<table><thead><tr><th>Model<th>Type<th>Download<tbody><tr><td>Original LoRA<td>LoRA<td><a href=https://huggingface.co/kaiokendev/SuperCOT-LoRA target=_blank rel="noopener noreferrer">https://huggingface.co/kaiokendev/SuperCOT-LoRA</a><tr><td>13B GGML<td>CPU<td><a href=https://huggingface.co/camelids/llama-13b-supercot-ggml-q4_2 target=_blank rel="noopener noreferrer">Q4</a>, <a href=https://huggingface.co/camelids/llama-13b-supercot-ggml-q8_0 target=_blank rel="noopener noreferrer">Q8</a><tr><td>30B GGML<td>CPU<td><a href=https://huggingface.co/camelids/llama-33b-supercot-ggml-q4_2 target=_blank rel="noopener noreferrer">Q4</a>, <a href=https://huggingface.co/camelids/llama-33b-supercot-ggml-q5_1 target=_blank rel="noopener noreferrer">Q5</a>, <a href=https://huggingface.co/camelids/llama-33b-supercot-ggml-q8_0 target=_blank rel="noopener noreferrer">Q8</a><tr><td>13B<td>GPU<td><a href=https://huggingface.co/ausboss/llama-13b-supercot-4bit-128g target=_blank rel="noopener noreferrer">Q4 CUDA 128gs</a><tr><td>30B<td>GPU<td><a href=https://huggingface.co/tsumeone/llama-30b-supercot-4bit-cuda target=_blank rel="noopener noreferrer">Q4 CUDA</a>, <a href=https://huggingface.co/tsumeone/llama-30b-supercot-4bit-128g-cuda target=_blank rel="noopener noreferrer">Q4 CUDA 128gs</a></table>
<h2 class="anchor anchorWithStickyNavbar_EG6R" id=previous-model-list>Previous Model List<a href=#previous-model-list class=hash-link aria-label="Direct link to Previous Model List" title="Direct link to Previous Model List" translate=no>​</a></h2>
<p>!!! info</p>
<div class="language-text codeBlockContainer_ljD1 theme-code-block" style=--prism-color:#bfc7d5;--prism-background-color:#292d3e><div class=codeBlockContent_rhaG><pre tabindex=0 class="prism-code language-text codeBlock_mx9Q thin-scrollbar" style=color:#bfc7d5;background-color:#292d3e><code class=codeBlockLines_NG8l><span class=token-line style=color:#bfc7d5><span class="token plain">The old rentry, retained for archiving purposes. Contains older and outdated models.</span><br/></span></code></pre></div></div>
<p><a href=https://rentry.org/backupmdlist target=_blank rel="noopener noreferrer">https://rentry.org/backupmdlist</a></p>
<hr/>
<h1>Models for <a href=https://github.com/ggerganov/llama.cpp target=_blank rel="noopener noreferrer">llama.cpp</a> (<a href=https://github.com/ggerganov/ggml target=_blank rel="noopener noreferrer">ggml</a> format)</h1>
<h2 class="anchor anchorWithStickyNavbar_EG6R" id=llama-quantized-4-bit-weights-ggml-q4_0>LLaMA quantized 4-bit weights (ggml q4_0)<a href=#llama-quantized-4-bit-weights-ggml-q4_0 class=hash-link aria-label="Direct link to LLaMA quantized 4-bit weights (ggml q4_0)" title="Direct link to LLaMA quantized 4-bit weights (ggml q4_0)" translate=no>​</a></h2>
<h4 class="anchor anchorWithStickyNavbar_EG6R" id=2023-03-31-torrent-magnet><a href="magnet:?xt=urn:btih:481dee5424b7024433504803a90efd32dae40fdf&dn=LLaMA-ggml-4bit_2023-03-31&tr=udp%3a%2f%2ftracker1.bt.moack.co.kr%3a80%2fannounce&tr=udp%3a%2f%2ftracker.torrent.eu.org%3a451%2fannounce&tr=udp%3a%2f%2ftracker.altrosky.nl%3a6969%2fannounce&tr=udp%3a%2f%2fopentracker.i2p.rocks%3a6969%2fannounce&tr=udp%3a%2f%2ftracker.opentrackr.org%3a1337%2fannounce&tr=https%3a%2f%2fopentracker.i2p.rocks%3a443%2fannounce&tr=udp%3a%2f%2ftracker.theoks.net%3a6969%2fannounce&tr=udp%3a%2f%2ftracker-udp.gbitt.info%3a80%2fannounce&tr=udp%3a%2f%2ftracker.tiny-vps.com%3a6969%2fannounce&tr=udp%3a%2f%2ftracker.moeking.me%3a6969%2fannounce&tr=udp%3a%2f%2f9.rarbg.com%3a2810%2fannounce&tr=udp%3a%2f%2ftracker.openbittorrent.com%3a6969%2fannounce&tr=udp%3a%2f%2ftracker.monitorit4.me%3a6969%2fannounce&tr=udp%3a%2f%2ftracker.dler.org%3a6969%2fannounce&tr=http%3a%2f%2ftracker.openbittorrent.com%3a80%2fannounce&tr=udp%3a%2f%2ftracker2.dler.org%3a80%2fannounce" target=_blank rel="noopener noreferrer">2023-03-31 torrent magnet</a><a href=#2023-03-31-torrent-magnet class=hash-link aria-label="Direct link to 2023-03-31-torrent-magnet" title="Direct link to 2023-03-31-torrent-magnet" translate=no>​</a></h4>
<p>!!! info <a href=https://github.com/ggerganov/llama.cpp#interactive-mode target=_blank rel="noopener noreferrer">Tutorial link for llama.cpp</a>
!!! info <a href=https://github.com/LostRuins/koboldcpp#usage target=_blank rel="noopener noreferrer">Tutorial link for koboldcpp</a></p>
<p>SHA256 checksums:</p>
<div class="language-text codeBlockContainer_ljD1 theme-code-block" style=--prism-color:#bfc7d5;--prism-background-color:#292d3e><div class=codeBlockContent_rhaG><pre tabindex=0 class="prism-code language-text codeBlock_mx9Q thin-scrollbar" style=color:#bfc7d5;background-color:#292d3e><code class=codeBlockLines_NG8l><span class=token-line style=color:#bfc7d5><span class="token plain">2dad53e70ca521fedcf9f9be5c26c15df602487a9c008bdafbb2bf8f946b6bf0  llama-7b-ggml-q4_0/ggml-model-q4_0.bin</span><br/></span><span class=token-line style=color:#bfc7d5><span class="token plain">9cd4d6c1f5f42d5abf529c51bde3303991fba912ab8ed452adfd7c97a4be77d7  llama-13b-ggml-q4_0/ggml-model-q4_0.bin</span><br/></span><span class=token-line style=color:#bfc7d5><span class="token plain">daefbc6b1b644a75be0286ef865253ab3786e96a2c1bca8b71216b1751eee63e  llama-33b-ggml-q4_0/ggml-model-q4_0.bin</span><br/></span><span class=token-line style=color:#bfc7d5><span class="token plain">d58a29c8403ecbd14258bbce07d90894fc5a8be25b9d359463c18f9f2ef96eb6  llama-65b-ggml-q4_0/ggml-model-q4_0.bin</span><br/></span></code></pre></div></div>
<p>ggml model file magic: <code>0x67676a74</code> (<code>ggjt</code> in hex)
ggml model file version: <code>1</code></p>
<h2 class="anchor anchorWithStickyNavbar_EG6R" id=alpaca-quantized-4-bit-weights-ggml-q4_0>Alpaca quantized 4-bit weights (ggml q4_0)<a href=#alpaca-quantized-4-bit-weights-ggml-q4_0 class=hash-link aria-label="Direct link to Alpaca quantized 4-bit weights (ggml q4_0)" title="Direct link to Alpaca quantized 4-bit weights (ggml q4_0)" translate=no>​</a></h2>

























<table><thead><tr><th>Model<th>Download<tbody><tr><td>LLaMA 7B fine-tune from <a href=https://huggingface.co/chavinlo/alpaca-native/tree/062111ff2af99db24f466562b8eb7e7e4ad7566d target=_blank rel="noopener noreferrer">chavinlo/alpaca-native</a><td><a href="magnet:?xt=urn:btih:d931a826b59443f4e543c18a25009b0ce8eabf39&dn=Alpaca-7B-ggml-4bit-native-finetune_2023-03-31&tr=udp%3a%2f%2ftracker.openbittorrent.com%3a6969%2fannounce&tr=udp%3a%2f%2ftracker.altrosky.nl%3a6969%2fannounce&tr=udp%3a%2f%2ftracker.dler.org%3a6969%2fannounce&tr=udp%3a%2f%2ftracker1.bt.moack.co.kr%3a80%2fannounce&tr=udp%3a%2f%2ftracker.moeking.me%3a6969%2fannounce&tr=udp%3a%2f%2ftracker2.dler.org%3a80%2fannounce&tr=udp%3a%2f%2ftracker.theoks.net%3a6969%2fannounce&tr=udp%3a%2f%2ftracker.tiny-vps.com%3a6969%2fannounce&tr=udp%3a%2f%2ftracker-udp.gbitt.info%3a80%2fannounce&tr=udp%3a%2f%2fopentracker.i2p.rocks%3a6969%2fannounce&tr=https%3a%2f%2fopentracker.i2p.rocks%3a443%2fannounce&tr=http%3a%2f%2ftracker.openbittorrent.com%3a80%2fannounce&tr=udp%3a%2f%2ftracker.torrent.eu.org%3a451%2fannounce&tr=udp%3a%2f%2f9.rarbg.com%3a2810%2fannounce&tr=udp%3a%2f%2ftracker.opentrackr.org%3a1337%2fannounce&tr=udp%3a%2f%2ftracker.monitorit4.me%3a6969%2fannounce" target=_blank rel="noopener noreferrer">2023-03-31 torrent magnet</a><tr><td>LLaMA 7B merged with <a href=https://huggingface.co/tloen/alpaca-lora-7b/tree/28801eabf63a125cee9e46d8073fb13c7c8bd8b9 target=_blank rel="noopener noreferrer">tloen/alpaca-lora-7b</a> LoRA<td><a href="magnet:?xt=urn:btih:694e206c1ce2780db673bdc2ecee78abcf228324&dn=Alpaca-7B-ggml-4bit-LoRA-merged_2023-03-31&tr=udp%3a%2f%2ftracker1.bt.moack.co.kr%3a80%2fannounce&tr=udp%3a%2f%2ftracker.monitorit4.me%3a6969%2fannounce&tr=udp%3a%2f%2ftracker.openbittorrent.com%3a6969%2fannounce&tr=udp%3a%2f%2ftracker.dler.org%3a6969%2fannounce&tr=http%3a%2f%2ftracker.openbittorrent.com%3a80%2fannounce&tr=udp%3a%2f%2ftracker.moeking.me%3a6969%2fannounce&tr=udp%3a%2f%2ftracker-udp.gbitt.info%3a80%2fannounce&tr=udp%3a%2f%2ftracker.theoks.net%3a6969%2fannounce&tr=udp%3a%2f%2fopentracker.i2p.rocks%3a6969%2fannounce&tr=udp%3a%2f%2ftracker.opentrackr.org%3a1337%2fannounce&tr=https%3a%2f%2fopentracker.i2p.rocks%3a443%2fannounce&tr=udp%3a%2f%2ftracker.altrosky.nl%3a6969%2fannounce&tr=udp%3a%2f%2ftracker.tiny-vps.com%3a6969%2fannounce&tr=udp%3a%2f%2ftracker2.dler.org%3a80%2fannounce&tr=udp%3a%2f%2ftracker.torrent.eu.org%3a451%2fannounce&tr=udp%3a%2f%2f9.rarbg.com%3a2810%2fannounce" target=_blank rel="noopener noreferrer">2023-03-31 torrent magnet</a><tr><td>LLaMA 13B merged with <a href=https://huggingface.co/chansung/alpaca-lora-13b/tree/abcdddb2778cace16f184dc1dda0ecf21ade23bc target=_blank rel="noopener noreferrer">chansung/alpaca-lora-13b</a> LoRA<td><a href="magnet:?xt=urn:btih:31ad0f8e8da5d43bad83eeed94f24cca504330d1&dn=Alpaca-13B-ggml-4bit-LoRA-merged_2023-03-31&tr=udp%3a%2f%2ftracker.openbittorrent.com%3a6969%2fannounce&tr=http%3a%2f%2ftracker.openbittorrent.com%3a80%2fannounce&tr=udp%3a%2f%2f9.rarbg.com%3a2810%2fannounce&tr=udp%3a%2f%2fopentracker.i2p.rocks%3a6969%2fannounce&tr=udp%3a%2f%2ftracker.monitorit4.me%3a6969%2fannounce&tr=https%3a%2f%2fopentracker.i2p.rocks%3a443%2fannounce&tr=udp%3a%2f%2ftracker.moeking.me%3a6969%2fannounce&tr=udp%3a%2f%2ftracker1.bt.moack.co.kr%3a80%2fannounce&tr=udp%3a%2f%2ftracker.opentrackr.org%3a1337%2fannounce&tr=udp%3a%2f%2ftracker.altrosky.nl%3a6969%2fannounce&tr=udp%3a%2f%2ftracker2.dler.org%3a80%2fannounce&tr=udp%3a%2f%2ftracker.theoks.net%3a6969%2fannounce&tr=udp%3a%2f%2ftracker-udp.gbitt.info%3a80%2fannounce&tr=udp%3a%2f%2ftracker.dler.org%3a6969%2fannounce&tr=udp%3a%2f%2ftracker.torrent.eu.org%3a451%2fannounce&tr=udp%3a%2f%2ftracker.tiny-vps.com%3a6969%2fannounce" target=_blank rel="noopener noreferrer">2023-03-31 torrent magnet</a><tr><td>LLaMA 33B merged with <a href=https://huggingface.co/chansung/alpaca-lora-30b/tree/bbbc77a38ad00a64780a76d119c783b6dc8200bd target=_blank rel="noopener noreferrer">chansung/alpaca-lora-30b</a> LoRA<td><a href="magnet:?xt=urn:btih:1e8681e255ec3078ef84fe4cdecdc7abd8b2b6e5&dn=Alpaca-33B-ggml-4bit-LoRA-merged_2023-03-31&tr=udp%3a%2f%2ftracker-udp.gbitt.info%3a80%2fannounce&tr=udp%3a%2f%2f9.rarbg.com%3a2810%2fannounce&tr=udp%3a%2f%2ftracker.moeking.me%3a6969%2fannounce&tr=udp%3a%2f%2ftracker.tiny-vps.com%3a6969%2fannounce&tr=udp%3a%2f%2ftracker.dler.org%3a6969%2fannounce&tr=udp%3a%2f%2ftracker.monitorit4.me%3a6969%2fannounce&tr=udp%3a%2f%2ftracker.theoks.net%3a6969%2fannounce&tr=udp%3a%2f%2ftracker.opentrackr.org%3a1337%2fannounce&tr=udp%3a%2f%2ftracker2.dler.org%3a80%2fannounce&tr=udp%3a%2f%2ftracker.openbittorrent.com%3a6969%2fannounce&tr=http%3a%2f%2ftracker.openbittorrent.com%3a80%2fannounce&tr=udp%3a%2f%2fopentracker.i2p.rocks%3a6969%2fannounce&tr=https%3a%2f%2fopentracker.i2p.rocks%3a443%2fannounce&tr=udp%3a%2f%2ftracker.torrent.eu.org%3a451%2fannounce&tr=udp%3a%2f%2ftracker1.bt.moack.co.kr%3a80%2fannounce&tr=udp%3a%2f%2ftracker.altrosky.nl%3a6969%2fannounce" target=_blank rel="noopener noreferrer">2023-03-31 torrent magnet</a></table>
<p>!!! info <a href=https://github.com/ggerganov/llama.cpp#instruction-mode-with-alpaca target=_blank rel="noopener noreferrer">Tutorial link for llama.cpp</a>
Example:
<code>./main --model ggml-model-q4_0.bin --file prompts/alpaca.txt --instruct --ctx_size 2048 --keep -1</code>
!!! info <a href=https://github.com/LostRuins/koboldcpp#usage target=_blank rel="noopener noreferrer">Tutorial link for koboldcpp</a></p>
<p>SHA256 checksums:</p>
<div class="language-text codeBlockContainer_ljD1 theme-code-block" style=--prism-color:#bfc7d5;--prism-background-color:#292d3e><div class=codeBlockContent_rhaG><pre tabindex=0 class="prism-code language-text codeBlock_mx9Q thin-scrollbar" style=color:#bfc7d5;background-color:#292d3e><code class=codeBlockLines_NG8l><span class=token-line style=color:#bfc7d5><span class="token plain">f5e264b10944c55a84810e8073dfdcd653fa8e47ff50ea043ec071051ac7821d  alpaca-7b-ggml-q4_0-native-finetune/ggml-model-q4_0.bin</span><br/></span><span class=token-line style=color:#bfc7d5><span class="token plain">d9777baad5cf6a5d196e70867338d8cc3c7af68c7744e68de839a522983860d7  alpaca-7b-ggml-q4_0-lora-merged/ggml-model-q4_0.bin</span><br/></span><span class=token-line style=color:#bfc7d5><span class="token plain">3838aa32651c65948e289374abd71f6feab1a62a4921a648e30d979df86a4af3  alpaca-13b-ggml-q4_0-lora-merged/ggml-model-q4_0.bin</span><br/></span><span class=token-line style=color:#bfc7d5><span class="token plain">2267ed1dc0bf0d6d300ba292c25083c7fa5395f3726c7c68a49b2be19a64b349  alpaca-33b-ggml-q4_0-lora-merged/ggml-model-q4_0.bin</span><br/></span></code></pre></div></div>
<p>ggml model file magic: <code>0x67676a74</code> (<code>ggjt</code> in hex)
ggml model file version: <code>1</code></p>
<h2 class="anchor anchorWithStickyNavbar_EG6R" id=gpt4all-7b-quantized-4-bit-weights-ggml-q4_0>GPT4All 7B quantized 4-bit weights (ggml q4_0)<a href=#gpt4all-7b-quantized-4-bit-weights-ggml-q4_0 class=hash-link aria-label="Direct link to GPT4All 7B quantized 4-bit weights (ggml q4_0)" title="Direct link to GPT4All 7B quantized 4-bit weights (ggml q4_0)" translate=no>​</a></h2>
<h4 class="anchor anchorWithStickyNavbar_EG6R" id=2023-03-31-torrent-magnet-1><a href="magnet:?xt=urn:btih:04584d8e5799c7838ccb987fae4f183936b9d744&dn=GPT4All-7B-ggml-4bit-lora-merged_2023-03-31&tr=udp%3a%2f%2ftracker1.bt.moack.co.kr%3a80%2fannounce&tr=udp%3a%2f%2ftracker.monitorit4.me%3a6969%2fannounce&tr=udp%3a%2f%2ftracker.theoks.net%3a6969%2fannounce&tr=udp%3a%2f%2f9.rarbg.com%3a2810%2fannounce&tr=udp%3a%2f%2ftracker.opentrackr.org%3a1337%2fannounce&tr=udp%3a%2f%2ftracker.openbittorrent.com%3a6969%2fannounce&tr=http%3a%2f%2ftracker.openbittorrent.com%3a80%2fannounce&tr=udp%3a%2f%2ftracker.tiny-vps.com%3a6969%2fannounce&tr=udp%3a%2f%2ftracker.altrosky.nl%3a6969%2fannounce&tr=udp%3a%2f%2ftracker.dler.org%3a6969%2fannounce&tr=udp%3a%2f%2ftracker2.dler.org%3a80%2fannounce&tr=udp%3a%2f%2ftracker.moeking.me%3a6969%2fannounce&tr=udp%3a%2f%2ftracker-udp.gbitt.info%3a80%2fannounce&tr=udp%3a%2f%2fopentracker.i2p.rocks%3a6969%2fannounce&tr=https%3a%2f%2fopentracker.i2p.rocks%3a443%2fannounce&tr=udp%3a%2f%2ftracker.torrent.eu.org%3a451%2fannounce" target=_blank rel="noopener noreferrer">2023-03-31 torrent magnet</a><a href=#2023-03-31-torrent-magnet-1 class=hash-link aria-label="Direct link to 2023-03-31-torrent-magnet-1" title="Direct link to 2023-03-31-torrent-magnet-1" translate=no>​</a></h4>
<p>!!! info <a href=https://github.com/ggerganov/llama.cpp#interactive-mode target=_blank rel="noopener noreferrer">Tutorial link for llama.cpp</a>
GPT4All can be used with llama.cpp in the same way as the other <code>ggml</code> models.
!!! info <a href=https://github.com/LostRuins/koboldcpp#usage target=_blank rel="noopener noreferrer">Tutorial link for koboldcpp</a></p>
<p>SHA256 checksums:</p>
<div class="language-text codeBlockContainer_ljD1 theme-code-block" style=--prism-color:#bfc7d5;--prism-background-color:#292d3e><div class=codeBlockContent_rhaG><pre tabindex=0 class="prism-code language-text codeBlock_mx9Q thin-scrollbar" style=color:#bfc7d5;background-color:#292d3e><code class=codeBlockLines_NG8l><span class=token-line style=color:#bfc7d5><span class="token plain">9f6cd4830a3c45a86147c80a32888e7be8f8a489284c87cdb882a7cfe40940c1  gpt4all-unfiltered-7b-ggml-q4_0-lora-merged/ggml-model-q4_0.bin</span><br/></span><span class=token-line style=color:#bfc7d5><span class="token plain">de314c5ee155ac40a03ca3b3be85ba2b02aef9e9f083c411c0b4490689dd047e  gpt4all-7b-ggml-q4_0-lora-merged/ggml-model-q4_0.bin</span><br/></span></code></pre></div></div>
<p>ggml model file magic: <code>0x67676a74</code> (<code>ggjt</code> in hex)
ggml model file version: <code>1</code></p>
<h2 class="anchor anchorWithStickyNavbar_EG6R" id=gpt4-x-alpaca-13b-quantized-4-bit-weights-ggml-q4_0>GPT4 x Alpaca 13B quantized 4-bit weights (ggml q4_0)<a href=#gpt4-x-alpaca-13b-quantized-4-bit-weights-ggml-q4_0 class=hash-link aria-label="Direct link to GPT4 x Alpaca 13B quantized 4-bit weights (ggml q4_0)" title="Direct link to GPT4 x Alpaca 13B quantized 4-bit weights (ggml q4_0)" translate=no>​</a></h2>
<h4 class="anchor anchorWithStickyNavbar_EG6R" id=2023-04-01-torrent-magnet><a href="magnet:?xt=urn:btih:f77827abd0cfb77399a0b281a1dbaeac5c386413&dn=GPT4-x-Alpaca-13B-ggml-4bit_2023-04-01&tr=udp%3a%2f%2ftracker.altrosky.nl%3a6969%2fannounce&tr=udp%3a%2f%2ftracker-udp.gbitt.info%3a80%2fannounce&tr=udp%3a%2f%2ftracker.theoks.net%3a6969%2fannounce&tr=udp%3a%2f%2ftracker.moeking.me%3a6969%2fannounce&tr=udp%3a%2f%2ftracker1.bt.moack.co.kr%3a80%2fannounce&tr=udp%3a%2f%2ftracker.openbittorrent.com%3a6969%2fannounce&tr=udp%3a%2f%2fopentracker.i2p.rocks%3a6969%2fannounce&tr=udp%3a%2f%2ftracker.torrent.eu.org%3a451%2fannounce&tr=https%3a%2f%2fopentracker.i2p.rocks%3a443%2fannounce&tr=udp%3a%2f%2ftracker2.dler.org%3a80%2fannounce&tr=udp%3a%2f%2ftracker.dler.org%3a6969%2fannounce&tr=http%3a%2f%2ftracker.openbittorrent.com%3a80%2fannounce&tr=udp%3a%2f%2ftracker.tiny-vps.com%3a6969%2fannounce&tr=udp%3a%2f%2f9.rarbg.com%3a2810%2fannounce&tr=udp%3a%2f%2ftracker.monitorit4.me%3a6969%2fannounce&tr=udp%3a%2f%2ftracker.opentrackr.org%3a1337%2fannounce" target=_blank rel="noopener noreferrer">2023-04-01 torrent magnet</a><a href=#2023-04-01-torrent-magnet class=hash-link aria-label="Direct link to 2023-04-01-torrent-magnet" title="Direct link to 2023-04-01-torrent-magnet" translate=no>​</a></h4>
<p>!!! info <a href=https://github.com/ggerganov/llama.cpp#interactive-mode target=_blank rel="noopener noreferrer">Tutorial link for llama.cpp</a>
GPT4 x Alpaca can be used with llama.cpp in the same way as the other <code>ggml</code> models.
Text generation with this version is faster compared to the <a href=https://rentry.org/nur779#gpt4-x-alpaca-13b-quantized-4-bit-weights-ggml-q4_1-from-gptq-with-groupsize-128 target=_blank rel="noopener noreferrer">GPTQ-quantized one</a>.
!!! info <a href=https://github.com/LostRuins/koboldcpp#usage target=_blank rel="noopener noreferrer">Tutorial link for koboldcpp</a></p>
<p>SHA256 checksum:</p>
<div class="language-text codeBlockContainer_ljD1 theme-code-block" style=--prism-color:#bfc7d5;--prism-background-color:#292d3e><div class=codeBlockContent_rhaG><pre tabindex=0 class="prism-code language-text codeBlock_mx9Q thin-scrollbar" style=color:#bfc7d5;background-color:#292d3e><code class=codeBlockLines_NG8l><span class=token-line style=color:#bfc7d5><span class="token plain">e6b77ebf297946949b25b3c4b870f10cdc98fb9fcaa6d19cef4dda9021031580  gpt4-x-alpaca-13b-ggml-q4_0/ggml-model-q4_0.bin</span><br/></span></code></pre></div></div>
<p>ggml model file magic: <code>0x67676a74</code> (<code>ggjt</code> in hex)
ggml model file version: <code>1</code></p>
<p><a href=https://desuarchive.org/g/thread/92479457/#q92481589 target=_blank rel="noopener noreferrer">Model source</a></p>
<h2 class="anchor anchorWithStickyNavbar_EG6R" id=gpt4-x-alpaca-13b-quantized-4-bit-weights-ggml-q4_1-from-gptq-with-groupsize-128>GPT4 x Alpaca 13B quantized 4-bit weights (ggml q4_1 from GPTQ with groupsize 128)<a href=#gpt4-x-alpaca-13b-quantized-4-bit-weights-ggml-q4_1-from-gptq-with-groupsize-128 class=hash-link aria-label="Direct link to GPT4 x Alpaca 13B quantized 4-bit weights (ggml q4_1 from GPTQ with groupsize 128)" title="Direct link to GPT4 x Alpaca 13B quantized 4-bit weights (ggml q4_1 from GPTQ with groupsize 128)" translate=no>​</a></h2>
<h4 class="anchor anchorWithStickyNavbar_EG6R" id=2023-04-01-torrent-magnet-1><a href="magnet:?xt=urn:btih:6cdb6ab819b13b00928182eea72106824e335734&dn=GPT4-x-Alpaca-13B-ggml-4bit-from-GPTQ-128g_2023-04-01&tr=udp%3a%2f%2ftracker-udp.gbitt.info%3a80%2fannounce&tr=udp%3a%2f%2ftracker.torrent.eu.org%3a451%2fannounce&tr=udp%3a%2f%2ftracker.dler.org%3a6969%2fannounce&tr=udp%3a%2f%2ftracker.moeking.me%3a6969%2fannounce&tr=udp%3a%2f%2ftracker.tiny-vps.com%3a6969%2fannounce&tr=udp%3a%2f%2ftracker.openbittorrent.com%3a6969%2fannounce&tr=udp%3a%2f%2fopentracker.i2p.rocks%3a6969%2fannounce&tr=udp%3a%2f%2ftracker.altrosky.nl%3a6969%2fannounce&tr=udp%3a%2f%2ftracker.theoks.net%3a6969%2fannounce&tr=https%3a%2f%2fopentracker.i2p.rocks%3a443%2fannounce&tr=udp%3a%2f%2ftracker.opentrackr.org%3a1337%2fannounce&tr=udp%3a%2f%2f9.rarbg.com%3a2810%2fannounce&tr=udp%3a%2f%2ftracker1.bt.moack.co.kr%3a80%2fannounce&tr=udp%3a%2f%2ftracker.monitorit4.me%3a6969%2fannounce&tr=udp%3a%2f%2ftracker2.dler.org%3a80%2fannounce&tr=http%3a%2f%2ftracker.openbittorrent.com%3a80%2fannounce" target=_blank rel="noopener noreferrer">2023-04-01 torrent magnet</a><a href=#2023-04-01-torrent-magnet-1 class=hash-link aria-label="Direct link to 2023-04-01-torrent-magnet-1" title="Direct link to 2023-04-01-torrent-magnet-1" translate=no>​</a></h4>
<p>!!! info <a href=https://github.com/ggerganov/llama.cpp#interactive-mode target=_blank rel="noopener noreferrer">Tutorial link for llama.cpp</a>
GPT4 x Alpaca can be used with llama.cpp in the same way as the other <code>ggml</code> models.
!!! info <a href=https://github.com/LostRuins/koboldcpp#usage target=_blank rel="noopener noreferrer">Tutorial link for koboldcpp</a></p>
<p>SHA256 checksum:</p>
<div class="language-text codeBlockContainer_ljD1 theme-code-block" style=--prism-color:#bfc7d5;--prism-background-color:#292d3e><div class=codeBlockContent_rhaG><pre tabindex=0 class="prism-code language-text codeBlock_mx9Q thin-scrollbar" style=color:#bfc7d5;background-color:#292d3e><code class=codeBlockLines_NG8l><span class=token-line style=color:#bfc7d5><span class="token plain">d4a640a1ce33009c244a361c6f87733aacbc2bea90e84d3c304a4c8be2bdf22d  gpt4-x-alpaca-13b-ggml-q4_1-from-gptq-4bit-128g/ggml-model-q4_1.bin</span><br/></span></code></pre></div></div>
<p>ggml model file magic: <code>0x67676a74</code> (<code>ggjt</code> in hex)
ggml model file version: <code>1</code></p>
<p><a href=https://desuarchive.org/g/thread/92479457/#q92481589 target=_blank rel="noopener noreferrer">Model source</a></p>
<h2 class="anchor anchorWithStickyNavbar_EG6R" id=vicuna-13b-quantized-4-bit-weights-ggml-q4_0>Vicuna 13B quantized 4-bit weights (ggml q4_0)<a href=#vicuna-13b-quantized-4-bit-weights-ggml-q4_0 class=hash-link aria-label="Direct link to Vicuna 13B quantized 4-bit weights (ggml q4_0)" title="Direct link to Vicuna 13B quantized 4-bit weights (ggml q4_0)" translate=no>​</a></h2>
<h4 class="anchor anchorWithStickyNavbar_EG6R" id=2023-04-03-torrent-magnet><a href="magnet:?xt=urn:btih:1e0c3dbeefe82483f81bd4e7ea959e4953c8081f&dn=Vicuna-13B-ggml-4bit-delta-merged_2023-04-03&tr=udp%3a%2f%2ftracker.opentrackr.org%3a1337%2fannounce&tr=udp%3a%2f%2f9.rarbg.com%3a2810%2fannounce&tr=udp%3a%2f%2ftracker.monitorit4.me%3a6969%2fannounce&tr=udp%3a%2f%2ftracker2.dler.org%3a80%2fannounce&tr=udp%3a%2f%2fopentracker.i2p.rocks%3a6969%2fannounce&tr=udp%3a%2f%2ftracker.torrent.eu.org%3a451%2fannounce&tr=udp%3a%2f%2ftracker.openbittorrent.com%3a6969%2fannounce&tr=udp%3a%2f%2ftracker.dler.org%3a6969%2fannounce&tr=udp%3a%2f%2ftracker-udp.gbitt.info%3a80%2fannounce&tr=udp%3a%2f%2ftracker1.bt.moack.co.kr%3a80%2fannounce&tr=https%3a%2f%2fopentracker.i2p.rocks%3a443%2fannounce&tr=udp%3a%2f%2ftracker.altrosky.nl%3a6969%2fannounce&tr=udp%3a%2f%2ftracker.tiny-vps.com%3a6969%2fannounce&tr=udp%3a%2f%2ftracker.moeking.me%3a6969%2fannounce&tr=http%3a%2f%2ftracker.openbittorrent.com%3a80%2fannounce&tr=udp%3a%2f%2ftracker.theoks.net%3a6969%2fannounce" target=_blank rel="noopener noreferrer">2023-04-03 torrent magnet</a><a href=#2023-04-03-torrent-magnet class=hash-link aria-label="Direct link to 2023-04-03-torrent-magnet" title="Direct link to 2023-04-03-torrent-magnet" translate=no>​</a></h4>
<p>!!! info <a href=https://github.com/ggerganov/llama.cpp#interactive-mode target=_blank rel="noopener noreferrer">Tutorial link for llama.cpp</a>
Vicuna can be used with llama.cpp in the same way as the other <code>ggml</code> models.
!!! info <a href=https://github.com/LostRuins/koboldcpp#usage target=_blank rel="noopener noreferrer">Tutorial link for koboldcpp</a></p>
<p>SHA256 checksum:</p>
<div class="language-text codeBlockContainer_ljD1 theme-code-block" style=--prism-color:#bfc7d5;--prism-background-color:#292d3e><div class=codeBlockContent_rhaG><pre tabindex=0 class="prism-code language-text codeBlock_mx9Q thin-scrollbar" style=color:#bfc7d5;background-color:#292d3e><code class=codeBlockLines_NG8l><span class=token-line style=color:#bfc7d5><span class="token plain">f96689a13c581f53b616887b2efe82bbfbc5321258dbcfdbe69a22076a7da461  vicuna-13b-ggml-q4_0-delta-merged/ggml-model-q4_0.bin</span><br/></span></code></pre></div></div>
<p>ggml model file magic: <code>0x67676a74</code> (<code>ggjt</code> in hex)
ggml model file version: <code>1</code></p>
<p><a href=https://huggingface.co/lmsys/vicuna-13b-delta/tree/da39ef5c586459f4d509bf7382475af584277e71 target=_blank rel="noopener noreferrer">Model source</a></p>
<h2 class="anchor anchorWithStickyNavbar_EG6R" id=openassistant-llama-13b-quantized-4-bit-weights-ggml-q4_0--q4_1>OpenAssistant LLaMA 13B quantized 4-bit weights (ggml q4_0 & q4_1)<a href=#openassistant-llama-13b-quantized-4-bit-weights-ggml-q4_0--q4_1 class=hash-link aria-label="Direct link to OpenAssistant LLaMA 13B quantized 4-bit weights (ggml q4_0 & q4_1)" title="Direct link to OpenAssistant LLaMA 13B quantized 4-bit weights (ggml q4_0 & q4_1)" translate=no>​</a></h2>
<p>!!! warning Note that this model is <a href=https://huggingface.co/dvruette/oasst-llama-13b-2-epochs/discussions/1#642ec79032e711e21aa11b60 target=_blank rel="noopener noreferrer">work-in-progress</a>.</p>
<h4 class="anchor anchorWithStickyNavbar_EG6R" id=2023-04-07-torrent-magnet--huggingface-hub-direct-download><a href="magnet:?xt=urn:btih:cad2f029978033f9c1487df3965546cc4d44489a&xt=urn:btmh:1220140702f43fbf90157db9531ad0454020bc212fddc48c7c30f593ec40d26eb19b&dn=oasst-llama-13b-ggml&tr=udp%3a%2f%2ftracker.opentrackr.org%3a1337%2fannounce&tr=udp%3a%2f%2f9.rarbg.com%3a2810%2fannounce&tr=udp%3a%2f%2ftracker.monitorit4.me%3a6969%2fannounce&tr=udp%3a%2f%2ftracker2.dler.org%3a80%2fannounce&tr=udp%3a%2f%2fopentracker.i2p.rocks%3a6969%2fannounce&tr=udp%3a%2f%2ftracker.torrent.eu.org%3a451%2fannounce&tr=udp%3a%2f%2ftracker.openbittorrent.com%3a6969%2fannounce&tr=udp%3a%2f%2ftracker.dler.org%3a6969%2fannounce&tr=udp%3a%2f%2ftracker-udp.gbitt.info%3a80%2fannounce&tr=udp%3a%2f%2ftracker1.bt.moack.co.kr%3a80%2fannounce&tr=https%3a%2f%2fopentracker.i2p.rocks%3a443%2fannounce&tr=udp%3a%2f%2ftracker.altrosky.nl%3a6969%2fannounce&tr=udp%3a%2f%2ftracker.tiny-vps.com%3a6969%2fannounce&tr=udp%3a%2f%2ftracker.moeking.me%3a6969%2fannounce&tr=http%3a%2f%2ftracker.openbittorrent.com%3a80%2fannounce&tr=udp%3a%2f%2ftracker.theoks.net%3a6969%2fannounce" target=_blank rel="noopener noreferrer">2023-04-07 torrent magnet</a> | <a href=https://huggingface.co/Black-Engineer/oasst-llama13b-ggml-q4/tree/main target=_blank rel="noopener noreferrer">HuggingFace Hub direct download</a><a href=#2023-04-07-torrent-magnet--huggingface-hub-direct-download class=hash-link aria-label="Direct link to 2023-04-07-torrent-magnet--huggingface-hub-direct-download" title="Direct link to 2023-04-07-torrent-magnet--huggingface-hub-direct-download" translate=no>​</a></h4>
<p>!!! info <a href=https://github.com/ggerganov/llama.cpp#interactive-mode target=_blank rel="noopener noreferrer">Tutorial link for llama.cpp</a>
!!! info <a href=https://github.com/LostRuins/koboldcpp#usage target=_blank rel="noopener noreferrer">Tutorial link for koboldcpp</a></p>
<p>SHA256 checksums:</p>
<div class="language-text codeBlockContainer_ljD1 theme-code-block" style=--prism-color:#bfc7d5;--prism-background-color:#292d3e><div class=codeBlockContent_rhaG><pre tabindex=0 class="prism-code language-text codeBlock_mx9Q thin-scrollbar" style=color:#bfc7d5;background-color:#292d3e><code class=codeBlockLines_NG8l><span class=token-line style=color:#bfc7d5><span class="token plain">fe77206c7890ecd0824c7b6b6a6deab92e471366b2e4271c05ece9a686474ef6  ggml-model-q4_0.bin</span><br/></span><span class=token-line style=color:#bfc7d5><span class="token plain">412da683b6ab0f710ce0adc8bc36db52bb92df96698558c5f2a1399af9bd0a78  ggml-model-q4_1.bin</span><br/></span></code></pre></div></div>
<p>ggml model file magic: <code>0x67676a74</code> (<code>ggjt</code> in hex)
ggml model file version: <code>1</code></p>
<p><a href=https://huggingface.co/dvruette/oasst-llama-13b-2-epochs target=_blank rel="noopener noreferrer">Original model source</a>
<a href=https://huggingface.co/gozfarb/oasst-llama13b-4bit-128g target=_blank rel="noopener noreferrer">GPTQ-quantized model source</a>
<a href=https://desuarchive.org/g/thread/92596368/#q92601864 target=_blank rel="noopener noreferrer">Torrent source</a></p>
<hr/>
<h1>Models for HuggingFace 🤗</h1>
<p>!!! danger Updated tokenizer and model configuration files can be found <a href=https://rentry.org/544p2 target=_blank rel="noopener noreferrer">here</a>.
Ensure that your models have the appropriate JSON files within the same directory as the weights, otherwise text generation might be impacted by tokenization problems. The issues were addressed <a href=https://github.com/huggingface/transformers/pull/22402 target=_blank rel="noopener noreferrer">here</a> and <a href=https://github.com/lm-sys/FastChat/pull/167 target=_blank rel="noopener noreferrer">here</a>, but a manual update of both the <code>transformers</code> library and your model configuration files is required.</p>
<h2 class="anchor anchorWithStickyNavbar_EG6R" id=llama-float16-weights>LLaMA float16 weights<a href=#llama-float16-weights class=hash-link aria-label="Direct link to LLaMA float16 weights" title="Direct link to LLaMA float16 weights" translate=no>​</a></h2>
<h4 class="anchor anchorWithStickyNavbar_EG6R" id=2023-03-26-torrent-magnet--huggingface-hub-direct-downloads><a href="magnet:?xt=urn:btih:496ee41a35f8d845f6d6cba11baa8b332f3c3318&dn=Safe-LLaMA-HF%20(3-26-23)&tr=http%3a%2f%2fbt2.archive.org%3a6969%2fannounce&tr=http%3a%2f%2fbt1.archive.org%3a6969%2fannounce" target=_blank rel="noopener noreferrer">2023-03-26 torrent magnet</a> | <a href=https://huggingface.co/Neko-Institute-of-Science target=_blank rel="noopener noreferrer">HuggingFace Hub direct downloads</a><a href=#2023-03-26-torrent-magnet--huggingface-hub-direct-downloads class=hash-link aria-label="Direct link to 2023-03-26-torrent-magnet--huggingface-hub-direct-downloads" title="Direct link to 2023-03-26-torrent-magnet--huggingface-hub-direct-downloads" translate=no>​</a></h4>
<p>!!! info <a href=https://github.com/oobabooga/text-generation-webui/wiki/LLaMA-model#hugging-face-format-weights target=_blank rel="noopener noreferrer">Tutorial link for Text generation web UI</a></p>
<p><a href=https://github.com/oobabooga/text-generation-webui/pull/530#issuecomment-1484235789 target=_blank rel="noopener noreferrer">Torrent source and SHA256 checksums</a></p>
<h2 class="anchor anchorWithStickyNavbar_EG6R" id=vicuna-13b-float16-weights>Vicuna 13B float16 weights<a href=#vicuna-13b-float16-weights class=hash-link aria-label="Direct link to Vicuna 13B float16 weights" title="Direct link to Vicuna 13B float16 weights" translate=no>​</a></h2>
<h4 class="anchor anchorWithStickyNavbar_EG6R" id=2023-04-03-torrent-magnet-1><a href="magnet:?xt=urn:btih:a7fac57094561a63d53eed943f904abf24c6969d&dn=Vicuna-13B-HF-fp16-delta-merged_2023-04-03&tr=udp%3a%2f%2ftracker.opentrackr.org%3a1337%2fannounce&tr=udp%3a%2f%2ftracker-udp.gbitt.info%3a80%2fannounce&tr=udp%3a%2f%2ftracker1.bt.moack.co.kr%3a80%2fannounce&tr=udp%3a%2f%2ftracker.tiny-vps.com%3a6969%2fannounce&tr=udp%3a%2f%2ftracker2.dler.org%3a80%2fannounce&tr=udp%3a%2f%2fopentracker.i2p.rocks%3a6969%2fannounce&tr=udp%3a%2f%2ftracker.altrosky.nl%3a6969%2fannounce&tr=udp%3a%2f%2ftracker.theoks.net%3a6969%2fannounce&tr=udp%3a%2f%2ftracker.dler.org%3a6969%2fannounce&tr=udp%3a%2f%2ftracker.torrent.eu.org%3a451%2fannounce&tr=udp%3a%2f%2ftracker.openbittorrent.com%3a6969%2fannounce&tr=https%3a%2f%2fopentracker.i2p.rocks%3a443%2fannounce&tr=http%3a%2f%2ftracker.openbittorrent.com%3a80%2fannounce&tr=udp%3a%2f%2ftracker.moeking.me%3a6969%2fannounce&tr=udp%3a%2f%2ftracker.monitorit4.me%3a6969%2fannounce&tr=udp%3a%2f%2f9.rarbg.com%3a2810%2fannounce" target=_blank rel="noopener noreferrer">2023-04-03 torrent magnet</a><a href=#2023-04-03-torrent-magnet-1 class=hash-link aria-label="Direct link to 2023-04-03-torrent-magnet-1" title="Direct link to 2023-04-03-torrent-magnet-1" translate=no>​</a></h4>
<p>!!! info <a href=https://github.com/oobabooga/text-generation-webui/wiki/LLaMA-model#hugging-face-format-weights target=_blank rel="noopener noreferrer">Tutorial link for Text generation web UI</a></p>
<p><a href=https://huggingface.co/lmsys/vicuna-13b-delta/tree/da39ef5c586459f4d509bf7382475af584277e71 target=_blank rel="noopener noreferrer">Model source</a></p>
<h2 class="anchor anchorWithStickyNavbar_EG6R" id=llama-quantized-4-bit-weights-gptq-format-without-groupsize>LLaMA quantized 4-bit weights (<a href=https://github.com/qwopqwop200/GPTQ-for-LLaMa target=_blank rel="noopener noreferrer">GPTQ</a> format without groupsize)<a href=#llama-quantized-4-bit-weights-gptq-format-without-groupsize class=hash-link aria-label="Direct link to llama-quantized-4-bit-weights-gptq-format-without-groupsize" title="Direct link to llama-quantized-4-bit-weights-gptq-format-without-groupsize" translate=no>​</a></h2>
<h4 class="anchor anchorWithStickyNavbar_EG6R" id=2023-03-26-torrent-magnet><a href="magnet:?xt=urn:btih:e88abf1b84290b162f00d3a9d79fb4f8719c2053&dn=LLaMA-HF-4bit&tr=http%3a%2f%2fbt2.archive.org%3a6969%2fannounce&tr=http%3a%2f%2fbt1.archive.org%3a6969%2fannounce" target=_blank rel="noopener noreferrer">2023-03-26 torrent magnet</a><a href=#2023-03-26-torrent-magnet class=hash-link aria-label="Direct link to 2023-03-26-torrent-magnet" title="Direct link to 2023-03-26-torrent-magnet" translate=no>​</a></h4>
<p>!!! info <a href=https://github.com/oobabooga/text-generation-webui/wiki/LLaMA-model#4-bit-mode target=_blank rel="noopener noreferrer">Tutorial link for Text generation web UI</a></p>
<p>SHA256 checksums:</p>
<div class="language-text codeBlockContainer_ljD1 theme-code-block" style=--prism-color:#bfc7d5;--prism-background-color:#292d3e><div class=codeBlockContent_rhaG><pre tabindex=0 class="prism-code language-text codeBlock_mx9Q thin-scrollbar" style=color:#bfc7d5;background-color:#292d3e><code class=codeBlockLines_NG8l><span class=token-line style=color:#bfc7d5><span class="token plain">09841a1c4895e1da3b05c1bdbfb8271c6d43812661e4348c862ff2ab1e6ff5b3  llama-7b-4bit/llama-7b-4bit.safetensors</span><br/></span><span class=token-line style=color:#bfc7d5><span class="token plain">edfa0b4060aae392b1e9df21fb60a97d78c9268ac6972e3888f6dc955ba0377b  llama-13b-4bit/llama-13b-4bit.safetensors</span><br/></span><span class=token-line style=color:#bfc7d5><span class="token plain">4cb560746fe58796233159612d8d3c9dbdebdf6f0443b47be71643f2f91b8541  llama-30b-4bit/llama-30b-4bit.safetensors</span><br/></span><span class=token-line style=color:#bfc7d5><span class="token plain">886ce814ed54c4bd6850e2216d5f198c49475210f8690f45dc63365d9aff3177  llama-65b-4bit/llama-65b-4bit.safetensors</span><br/></span></code></pre></div></div>
<p><a href=https://github.com/oobabooga/text-generation-webui/pull/530#issuecomment-1483891617 target=_blank rel="noopener noreferrer">Torrent source and more information</a></p>
<h2 class="anchor anchorWithStickyNavbar_EG6R" id=llama-quantized-4-bit-weights-gptq-format-with-groupsize-128>LLaMA quantized 4-bit weights (<a href=https://github.com/qwopqwop200/GPTQ-for-LLaMa target=_blank rel="noopener noreferrer">GPTQ</a> format with groupsize 128)<a href=#llama-quantized-4-bit-weights-gptq-format-with-groupsize-128 class=hash-link aria-label="Direct link to llama-quantized-4-bit-weights-gptq-format-with-groupsize-128" title="Direct link to llama-quantized-4-bit-weights-gptq-format-with-groupsize-128" translate=no>​</a></h2>
<h4 class="anchor anchorWithStickyNavbar_EG6R" id=2023-03-26-torrent-magnet-1><a href="magnet:?xt=urn:btih:88f7d9d2460ffcaf78b21e83012de00939eacb65&dn=LLaMA-HF-4bit-128g&tr=http%3a%2f%2fbt2.archive.org%3a6969%2fannounce&tr=http%3a%2f%2fbt1.archive.org%3a6969%2fannounce" target=_blank rel="noopener noreferrer">2023-03-26 torrent magnet</a><a href=#2023-03-26-torrent-magnet-1 class=hash-link aria-label="Direct link to 2023-03-26-torrent-magnet-1" title="Direct link to 2023-03-26-torrent-magnet-1" translate=no>​</a></h4>
<p>!!! info <a href=https://github.com/oobabooga/text-generation-webui/wiki/LLaMA-model#4-bit-mode target=_blank rel="noopener noreferrer">Tutorial link for Text generation web UI</a>
<code>Groupsize 128</code> is a better choice for the 13B, 33B and 65B models, according to <a href=https://github.com/oobabooga/text-generation-webui/pull/530#issuecomment-1483941105 target=_blank rel="noopener noreferrer">this</a>.</p>
<p>SHA256 checksums:</p>
<div class="language-text codeBlockContainer_ljD1 theme-code-block" style=--prism-color:#bfc7d5;--prism-background-color:#292d3e><div class=codeBlockContent_rhaG><pre tabindex=0 class="prism-code language-text codeBlock_mx9Q thin-scrollbar" style=color:#bfc7d5;background-color:#292d3e><code class=codeBlockLines_NG8l><span class=token-line style=color:#bfc7d5><span class="token plain">ed8ec9c9f0ebb83210157ad0e3c5148760a4e9fd2acfb02cf00f8f2054d2743b  llama-7b-4bit-128g/llama-7b-4bit-128g.safetensors</span><br/></span><span class=token-line style=color:#bfc7d5><span class="token plain">d3073ef1a2c0b441f95a5d4f8a5aa3b82884eef45d8997270619cb29bcc994b8  llama-13b-4bit-128g/llama-13b-4bit-128g.safetensors</span><br/></span><span class=token-line style=color:#bfc7d5><span class="token plain">8b7d75d562938823c4503b956cb4b8af6ac0a5afbce2278566cc787da0f8f682  llama-30b-4bit-128g/llama-30b-4bit-128g.safetensors</span><br/></span><span class=token-line style=color:#bfc7d5><span class="token plain">f1418091e3307611fb0a213e50a0f52c80841b9c4bcba67abc1f6c64c357c850  llama-65b-4bit-128g/llama-65b-4bit-128g.safetensors</span><br/></span></code></pre></div></div>
<p><a href=https://github.com/oobabooga/text-generation-webui/pull/530#issuecomment-1483941105 target=_blank rel="noopener noreferrer">Torrent source and more information</a></p>
<h2 class="anchor anchorWithStickyNavbar_EG6R" id=alpaca-quantized-4-bit-weights-gptq-format-with-groupsize-128>Alpaca quantized 4-bit weights (<a href=https://github.com/qwopqwop200/GPTQ-for-LLaMa target=_blank rel="noopener noreferrer">GPTQ</a> format with groupsize 128)<a href=#alpaca-quantized-4-bit-weights-gptq-format-with-groupsize-128 class=hash-link aria-label="Direct link to alpaca-quantized-4-bit-weights-gptq-format-with-groupsize-128" title="Direct link to alpaca-quantized-4-bit-weights-gptq-format-with-groupsize-128" translate=no>​</a></h2>

















<table><thead><tr><th>Model<th>Download<tbody><tr><td>LLaMA 7B fine-tune from <a href=https://huggingface.co/ozcur/alpaca-native-4bit target=_blank rel="noopener noreferrer">ozcur/alpaca-native-4bit</a> as safetensors<td><a href="magnet:?xt=urn:btih:90674fd4a3672c6eae5bf994634109bb75429e6b&dn=Alpaca-7B-GPTQ-4bit-128g-native-finetune_2023-03-29&tr=udp%3a%2f%2fopentracker.i2p.rocks%3a6969%2fannounce&tr=udp%3a%2f%2ftracker.altrosky.nl%3a6969%2fannounce&tr=udp%3a%2f%2ftracker-udp.gbitt.info%3a80%2fannounce&tr=udp%3a%2f%2ftracker.opentrackr.org%3a1337%2fannounce&tr=udp%3a%2f%2ftracker.skynetcloud.site%3a6969%2fannounce&tr=udp%3a%2f%2ftracker2.dler.org%3a80%2fannounce&tr=udp%3a%2f%2ftracker.monitorit4.me%3a6969%2fannounce&tr=udp%3a%2f%2ftracker.lelux.fi%3a6969%2fannounce&tr=udp%3a%2f%2ftracker1.bt.moack.co.kr%3a80%2fannounce&tr=udp%3a%2f%2ftracker.torrent.eu.org%3a451%2fannounce&tr=udp%3a%2f%2f9.rarbg.com%3a2810%2fannounce&tr=udp%3a%2f%2ftracker.theoks.net%3a6969%2fannounce&tr=udp%3a%2f%2ftracker.moeking.me%3a6969%2fannounce&tr=udp%3a%2f%2ftracker.tiny-vps.com%3a6969%2fannounce&tr=udp%3a%2f%2ftracker.openbittorrent.com%3a6969%2fannounce&tr=http%3a%2f%2ftracker.openbittorrent.com%3a80%2fannounce&tr=udp%3a%2f%2ftracker.dler.org%3a6969%2fannounce&tr=https%3a%2f%2fopentracker.i2p.rocks%3a443%2fannounce" target=_blank rel="noopener noreferrer">2023-03-29 torrent magnet</a><tr><td>LLaMA 33B merged with <a href=https://huggingface.co/baseten/alpaca-30b target=_blank rel="noopener noreferrer">baseten/alpaca-30b</a> LoRA by <a href=https://desuarchive.org/g/thread/92351574/#q92356537 target=_blank rel="noopener noreferrer">an anon</a><td><a href="magnet:?xt=urn:btih:81cf9b528cc80e390323f9ec50d4dfb4debcb490&dn=Alpaca%2030B%204bit%20groupsize%20128&tr=http%3A%2F%2Fbt2.archive.org%3A6969%2Fannounce" target=_blank rel="noopener noreferrer">2023-03-26 torrent magnet</a> | <a href=https://rentry.org/544p2#llama-33b target=_blank rel="noopener noreferrer">extra config files</a></table>
<p>!!! info <a href=https://github.com/oobabooga/text-generation-webui/wiki/LLaMA-model#4-bit-mode target=_blank rel="noopener noreferrer">Tutorial link for Text generation web UI</a></p>
<p>SHA256 checksums:</p>
<div class="language-text codeBlockContainer_ljD1 theme-code-block" style=--prism-color:#bfc7d5;--prism-background-color:#292d3e><div class=codeBlockContent_rhaG><pre tabindex=0 class="prism-code language-text codeBlock_mx9Q thin-scrollbar" style=color:#bfc7d5;background-color:#292d3e><code class=codeBlockLines_NG8l><span class=token-line style=color:#bfc7d5><span class="token plain">17d6ba8f83be89f8dfa05cd4720cdd06b4d32c3baed79986e3ba1501b2305530  Alpaca-7B-GPTQ-4bit-128g-native-finetune_2023-03-29/alpaca-7b-4bit-128g-native-finetune.safetensors</span><br/></span><span class=token-line style=color:#bfc7d5><span class="token plain">a2f8d202ce61b1b612afe08c11f97133c1d56076d65391e738b1ab57c854ee05  Alpaca-30B-4bit-128g/alpaca-30b-hf-4bit.safetensors</span><br/></span></code></pre></div></div>
<h2 class="anchor anchorWithStickyNavbar_EG6R" id=vicuna-13b-quantized-4-bit--8-bit-weights-gptq-format-with-groupsize-128>Vicuna 13B quantized 4-bit & 8-bit weights (<a href=https://github.com/qwopqwop200/GPTQ-for-LLaMa target=_blank rel="noopener noreferrer">GPTQ</a> format with groupsize 128)<a href=#vicuna-13b-quantized-4-bit--8-bit-weights-gptq-format-with-groupsize-128 class=hash-link aria-label="Direct link to vicuna-13b-quantized-4-bit--8-bit-weights-gptq-format-with-groupsize-128" title="Direct link to vicuna-13b-quantized-4-bit--8-bit-weights-gptq-format-with-groupsize-128" translate=no>​</a></h2>
<h5 class="anchor anchorWithStickyNavbar_EG6R" id=2023-04-03-torrent-magnet-2><a href="magnet:?xt=urn:btih:f67d372a01c0b8e0162931623d6c55a5e6f34921&dn=Vicuna-13B-quantized-128g&tr=http%3a%2f%2fbt2.archive.org%3a6969%2fannounce" target=_blank rel="noopener noreferrer">2023-04-03 torrent magnet</a><a href=#2023-04-03-torrent-magnet-2 class=hash-link aria-label="Direct link to 2023-04-03-torrent-magnet-2" title="Direct link to 2023-04-03-torrent-magnet-2" translate=no>​</a></h5>
<p>!!! info <a href=https://github.com/oobabooga/text-generation-webui/wiki/LLaMA-model#4-bit-mode target=_blank rel="noopener noreferrer">Tutorial link for Text generation web UI</a></p>
<p><a href=https://desuarchive.org/g/thread/92531914#92536953 target=_blank rel="noopener noreferrer">Torrent source</a>
<a href=https://rentry.org/544p2#llama-13b target=_blank rel="noopener noreferrer">Extra config files</a></div><footer class="theme-doc-footer docusaurus-mt-lg"><div class="row margin-top--sm theme-doc-footer-edit-meta-row"><div class=col><a href=https://github.com/wenerme/wener/edit/master/notes/../notes/ai/llm/llama.md target=_blank rel="noopener noreferrer" class=theme-edit-this-page><svg fill=currentColor height=20 width=20 viewBox="0 0 40 40" class=iconEdit_A7Jz aria-hidden=true><g><path d="m34.5 11.7l-3 3.1-6.3-6.3 3.1-3q0.5-0.5 1.2-0.5t1.1 0.5l3.9 3.9q0.5 0.4 0.5 1.1t-0.5 1.2z m-29.5 17.1l18.4-18.5 6.3 6.3-18.4 18.4h-6.3v-6.2z"/></g></svg>Edit this page</a></div><div class="col lastUpdated_Hg3U"><span class=theme-last-updated>Last updated<!-- --> on <b><time datetime=2025-10-09T05:44:41.000Z itemprop=dateModified>Oct 9, 2025</time></b> by <b>wener</b></span></div></div></footer></article><nav class="docusaurus-mt-lg pagination-nav" aria-label="Docs pages"><a class="pagination-nav__link pagination-nav__link--prev" href=/notes/ai/llm><div class=pagination-nav__sublabel>Previous</div><div class=pagination-nav__label>LLM</div></a><a class="pagination-nav__link pagination-nav__link--next" href=/notes/ai/llm/awesome><div class=pagination-nav__sublabel>Next</div><div class=pagination-nav__label>LLM Awesome</div></a></nav></div></div><div class="col col--3"><div class="tableOfContents_wpyd thin-scrollbar theme-doc-toc-desktop"><ul class="table-of-contents table-of-contents__left-border"><li><a href=#changelog-mdy class="table-of-contents__link toc-highlight">Changelog (MDY)</a><li><a href=#4-bit-gpu-model-requirements class="table-of-contents__link toc-highlight">4-bit GPU Model Requirements</a><li><a href=#4-bit-cpullamacpp-ram-requirements class="table-of-contents__link toc-highlight">4-bit CPU/llama.cpp RAM Requirements</a><li><a href=#llama-16-bit-weights class="table-of-contents__link toc-highlight">LLaMA 16-bit Weights</a><li><a href=#llama-4-bit-weights class="table-of-contents__link toc-highlight">LLaMA 4-bit Weights</a><li><a href=#wizardlm-13b-uncensored-05102023 class="table-of-contents__link toc-highlight">WizardLM 13B Uncensored (05/10/2023)</a><li><a href=#bluemoonrp-13b-05072023 class="table-of-contents__link toc-highlight">BluemoonRP 13B (05/07/2023)</a><li><a href=#vicuna-13b-cocktail-05072023 class="table-of-contents__link toc-highlight">Vicuna 13B Cocktail (05/07/2023)</a><li><a href=#gpt4-x-alpacadente2-30b-05052023 class="table-of-contents__link toc-highlight">GPT4-x-AlpacaDente2-30B (05/05/2023)</a><li><a href=#vicuna-13b-free-v11-05012023 class="table-of-contents__link toc-highlight">Vicuna 13B Free v1.1 (05/01/2023)</a><li><a href=#pygmalionmetharme-7b-04302023 class="table-of-contents__link toc-highlight">Pygmalion/Metharme 7B (04/30/2023)</a><li><a href=#gpt4-x-alpasta-30b-04292023 class="table-of-contents__link toc-highlight">GPT4-X-Alpasta 30B (04/29/2023)</a><li><a href=#openassistant-llama-30b-sft-6-04232023 class="table-of-contents__link toc-highlight">OpenAssistant LLaMa 30B SFT 6 (04/23/2023)</a><li><a href=#supercot-04222023 class="table-of-contents__link toc-highlight">SuperCOT (04/22/2023)</a><li><a href=#previous-model-list class="table-of-contents__link toc-highlight">Previous Model List</a><li><a href=#llama-quantized-4-bit-weights-ggml-q4_0 class="table-of-contents__link toc-highlight">LLaMA quantized 4-bit weights (ggml q4_0)</a><li><a href=#alpaca-quantized-4-bit-weights-ggml-q4_0 class="table-of-contents__link toc-highlight">Alpaca quantized 4-bit weights (ggml q4_0)</a><li><a href=#gpt4all-7b-quantized-4-bit-weights-ggml-q4_0 class="table-of-contents__link toc-highlight">GPT4All 7B quantized 4-bit weights (ggml q4_0)</a><li><a href=#gpt4-x-alpaca-13b-quantized-4-bit-weights-ggml-q4_0 class="table-of-contents__link toc-highlight">GPT4 x Alpaca 13B quantized 4-bit weights (ggml q4_0)</a><li><a href=#gpt4-x-alpaca-13b-quantized-4-bit-weights-ggml-q4_1-from-gptq-with-groupsize-128 class="table-of-contents__link toc-highlight">GPT4 x Alpaca 13B quantized 4-bit weights (ggml q4_1 from GPTQ with groupsize 128)</a><li><a href=#vicuna-13b-quantized-4-bit-weights-ggml-q4_0 class="table-of-contents__link toc-highlight">Vicuna 13B quantized 4-bit weights (ggml q4_0)</a><li><a href=#openassistant-llama-13b-quantized-4-bit-weights-ggml-q4_0--q4_1 class="table-of-contents__link toc-highlight">OpenAssistant LLaMA 13B quantized 4-bit weights (ggml q4_0 & q4_1)</a><li><a href=#llama-float16-weights class="table-of-contents__link toc-highlight">LLaMA float16 weights</a><li><a href=#vicuna-13b-float16-weights class="table-of-contents__link toc-highlight">Vicuna 13B float16 weights</a><li><a href=#llama-quantized-4-bit-weights-gptq-format-without-groupsize class="table-of-contents__link toc-highlight">LLaMA quantized 4-bit weights (GPTQ format without groupsize)</a><li><a href=#llama-quantized-4-bit-weights-gptq-format-with-groupsize-128 class="table-of-contents__link toc-highlight">LLaMA quantized 4-bit weights (GPTQ format with groupsize 128)</a><li><a href=#alpaca-quantized-4-bit-weights-gptq-format-with-groupsize-128 class="table-of-contents__link toc-highlight">Alpaca quantized 4-bit weights (GPTQ format with groupsize 128)</a><li><a href=#vicuna-13b-quantized-4-bit--8-bit-weights-gptq-format-with-groupsize-128 class="table-of-contents__link toc-highlight">Vicuna 13B quantized 4-bit & 8-bit weights (GPTQ format with groupsize 128)</a></ul></div></div></div></div></main></div></div></div><footer class="theme-layout-footer footer footer--dark"><div class="container container-fluid"><div class="row footer__links"><div class="theme-layout-footer-column col footer__col"><div class=footer__title>笔记</div><ul class="footer__items clean-list"><li class=footer__item><a class=footer__link-item href=/notes/java>Java</a><li class=footer__item><a class=footer__link-item href=/notes/os/alpine>AlpineLinux</a><li class=footer__item><a class=footer__link-item href=/notes/devops/kubernetes>Kubernates</a><li class=footer__item><a class=footer__link-item href=/notes/voip>VoIP</a></ul></div><div class="theme-layout-footer-column col footer__col"><div class=footer__title>Projects</div><ul class="footer__items clean-list"><li class=footer__item>
              <div>
              <a class=footer__link-item href=https://github.com/wenerme/wener>Wener</a>
              -
              <a class=footer__link-item href=https://github.com/wenerme/wener/actions title="wenerme/wener - ci">
              <img style="vertical-align: middle;opacity: .4;" src=https://github.com/wenerme/wener/workflows/Build/badge.svg />
              </a>
              </div>
              <li class=footer__item><a href=https://apis.wener.me target=_blank rel="noopener noreferrer" class=footer__link-item>Wener's Apis<svg width=13.5 height=13.5 aria-label="(opens in new tab)" class=iconExternalLink_kUGX><use href=#theme-svg-external-link /></svg></a></ul></div><div class="theme-layout-footer-column col footer__col"><div class=footer__title>Social</div><ul class="footer__items clean-list"><li class=footer__item><a class=footer__link-item href=/story>Blog</a><li class=footer__item><a href=https://github.com/wenerme target=_blank rel="noopener noreferrer" class=footer__link-item>GitHub</a><li class=footer__item><a href=https://twitter.com/wenerme target=_blank rel="noopener noreferrer" class=footer__link-item>Twitter</a></ul></div></div><div class="footer__bottom text--center"><div class=margin-bottom--sm><img src=/img/wener-logo.svg alt="Wener Site" class="footer__logo themedComponent_j2y5 themedComponent--light_v877"/><img src=/img/wener-logo.svg alt="Wener Site" class="footer__logo themedComponent_j2y5 themedComponent--dark_PUQY"/></div><div class=footer__copyright>Copyright © 1992-2025 Wener - <img alt=cc-by-sa-4.0 src=https://mirrors.creativecommons.org/presskit/buttons/80x15/svg/by-sa.svg /> - Build @2025-10-09 13:46</div></div></div></footer></div></body>