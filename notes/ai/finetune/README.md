---
tags:
  - Topic
  - Training
---

# å¾®è°ƒ

- ä¸ºä»€ä¹ˆè¦å¾®è°ƒå¤§è¯­è¨€æ¨¡å‹ (LLM)ï¼Ÿ
  - è§£å†³ç‰¹å®šé—®é¢˜ï¼šé’ˆå¯¹å…·ä½“ã€å¯è¡¡é‡çš„é—®é¢˜ä¼˜åŒ–æ¨¡å‹è¡¨ç°ã€‚
  - æå‡ä»»åŠ¡è´¨é‡ï¼šæé«˜æ¨¡å‹åœ¨ç‰¹å®šä»»åŠ¡ä¸Šçš„å‡†ç¡®æ€§å’Œæ•ˆæœã€‚
  - é™ä½æˆæœ¬ä¸æå‡é€Ÿåº¦ï¼šä½¿ç”¨å¾®è°ƒåçš„å°æ¨¡å‹å¯èƒ½æ¯”é€šç”¨å¤§æ¨¡å‹æ›´ç»æµã€æ›´å¿«é€Ÿã€‚
  - ä¼˜åŒ–å·¥å…·è°ƒç”¨èƒ½åŠ›ï¼šå¢å¼ºæ¨¡å‹ç†è§£å’Œæ‰§è¡Œç‰¹å®šå·¥å…·æˆ– API è°ƒç”¨çš„èƒ½åŠ›ã€‚
  - å¢å¼ºé€»è¾‘ä¸è§„åˆ™éµå¾ªï¼šä½¿æ¨¡å‹æ›´å¥½åœ°ç†è§£å’Œéµå¾ªå¤æ‚çš„æŒ‡ä»¤ã€é€»è¾‘å’Œç‰¹å®šè§„åˆ™ã€‚
  - æ¨¡å‹è’¸é¦ï¼šå°†å¤§å‹ã€å¤æ‚æ¨¡å‹çš„çŸ¥è¯†å’Œèƒ½åŠ›è¿ç§»åˆ°æ›´å°ã€æ›´é«˜æ•ˆçš„æ¨¡å‹ä¸­ã€‚
  - æ”¹è¿›æ€è€ƒä¸æ¨ç†é“¾ï¼šæå‡æ¨¡å‹è¿›è¡Œå¤æ‚æ€è€ƒã€æ¨ç†å’Œå½¢æˆè¿è´¯æ€ç»´é“¾æ¡çš„èƒ½åŠ›ã€‚
  - å¯¹é½äººç±»ä»·å€¼è§‚ä¸å®‰å…¨è¦æ±‚ï¼šä½¿æ¨¡å‹çš„è¾“å‡ºæ›´ç¬¦åˆäººç±»çš„ä»·å€¼è§‚ã€é“å¾·æ ‡å‡†å’Œå®‰å…¨è§„èŒƒã€‚
- å¦‚ä½•å¼€å§‹å¾®è°ƒ LLMï¼Ÿ
  - æ˜ç¡®ç›®æ ‡ (Pick a goal)ï¼šæ¸…æ™°å®šä¹‰ä½ å¸Œæœ›é€šè¿‡å¾®è°ƒè§£å†³çš„å…·ä½“é—®é¢˜æˆ–è¾¾æˆçš„æ€§èƒ½æŒ‡æ ‡ã€‚
  - å‡†å¤‡è®­ç»ƒæ•°æ® (Generate training data)ï¼šæ”¶é›†æˆ–ç”Ÿæˆé«˜è´¨é‡çš„è®­ç»ƒæ•°æ®ã€‚æ–‡ç« æåˆ°ï¼Œåˆæˆæ•°æ®ï¼ˆSynthetic dataï¼‰åœ¨å¾®è°ƒä¸­æ•ˆæœè‰¯å¥½ã€‚
  - è®­ç»ƒå€™é€‰æ¨¡å‹ (Train a few candidates)ï¼šé€‰æ‹©åˆé€‚çš„åŸºæ¨¡å‹ï¼Œå¹¶å°è¯•è®­ç»ƒå‡ ä¸ªä¸åŒçš„å¾®è°ƒç‰ˆæœ¬ã€‚
  - è¯„ä¼°å…³é”®æŒ‡æ ‡ (Measure what matters)ï¼šæ ¹æ®ä½ è®¾å®šçš„ç›®æ ‡ï¼Œå¯¹å¾®è°ƒåçš„æ¨¡å‹è¿›è¡Œä¸¥æ ¼çš„è¯„ä¼°ï¼Œè¡¡é‡å…¶åœ¨å…³é”®æŒ‡æ ‡ä¸Šçš„è¡¨ç°ã€‚

---

- toolkit
  - [InternLM/xtuner](https://github.com/InternLM/xtuner)
    - Apache-2.0, Python
  - [hiyouga/LLaMA-Factory](https://github.com/hiyouga/LLaMA-Factory)
    - Apache-2.0, Python
    - Unified Efficient Fine-Tuning of 100+ LLMs & VLMs
  - [2U1/Qwen2-VL-Finetune](https://github.com/2U1/Qwen2-VL-Finetune)
    - Apache-2.0, Python
    - Liger-Kernel
  - [axolotl-ai-cloud/axolotl](https://github.com/axolotl-ai-cloud/axolotl)
    - Apache-2.0, Python
  - [unslothai/unsloth](https://github.com/unslothai/unsloth)
    - Apache-2.0, Python
  - [pytorch/torchtune](https://github.com/pytorch/torchtune)
    - Apache-2.0, Python
  - [kiln-ai/kiln](https://github.com/kiln-ai/kiln)
    - Rapid AI Prototyping and Dataset Collaboration Tool
- Eval
  - [open-compass/VLMEvalKit](https://github.com/open-compass/VLMEvalKit)
    - Apache-2.0, Python
    - evaluation toolkit of large multi-modality models (LMMs), support 220+ LMMs, 80+ benchmarks
- Example/LoRA
  - [daniel3303/StoryReasoning](https://github.com/daniel3303/StoryReasoning)
    - å¤šimageï¼Œè¿ç»­æ•…äº‹æ¨ç†ï¼Œäººç‰©è¯†åˆ«
    - æ•°æ®é›† [daniel3303/StoryReasoning](https://huggingface.co/datasets/daniel3303/StoryReasoning)
- Article/æ–‡ç« /å‚è€ƒ
  - [Training a WAN or HunYuan LoRA the right way.](https://civitai.com/articles/11942)

# Glossary

| abbr.    | stand for                                  | meaning          |
| -------- | ------------------------------------------ | ---------------- |
| PEFT     | Parameter-Efficient Fine-Tuning            | ä»…å¾®è°ƒéƒ¨åˆ†å‚æ•°   |
| LoRA     | Low-Rank Adaptation                        | ä½ç§©é€‚é…         |
| QLoRA    | Quantized LoRA                             | é‡åŒ–ä½ç§©é€‚é…     |
| P-Tuning | Prompt Tuning                              | æç¤ºå¾®è°ƒ         |
| DPO      | Direct Preference Optimization             | ç›´æ¥åå¥½ä¼˜åŒ–     |
| GRPO     | Group Relative Policy Optimization         | ç»„ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–   |
| RLHF     | Reinforcement Learning from Human Feedback | äººç±»åé¦ˆå¼ºåŒ–å­¦ä¹  |
| DoRA     | Weight-Decomposed Low-Rank Adaptation      | åŠ¨æ€ç§©é€‚é…       |
| RSLORA   | Rank-Stabilized LoRA                       | ç¨³å®šç§©ä½ç§©é€‚é…   |
| LoftQ    | LoRA-Fine-Tuning-Aware Quantization        | ä½ç§©å¾®è°ƒæ„ŸçŸ¥é‡åŒ– |
| SFT      | Supervised Fine-Tuning                     | ç›‘ç£å¾®è°ƒ         |

| en                | cn       |
| ----------------- | -------- |
| Fine-Tuning       | å¾®è°ƒ     |
| Transfer Learning | è¿ç§»å­¦ä¹  |
| Post-Training     | åè®­ç»ƒ   |

- Full Training
  - Optimizer - 8bit
  - Gradient - 16bit
  - Weight - 16bit
- LoRA (Low-Rank Adaptation) - ä½ç§©é€‚é…
  - é€šè¿‡åœ¨é¢„è®­ç»ƒæ¨¡å‹çš„åŸºç¡€ä¸Šï¼Œæ·»åŠ ä½ç§©çŸ©é˜µæ¥å®ç°å¾®è°ƒã€‚
  - PEFT æŠ€æœ¯
  - ä¸ç›´æ¥ä¿®æ”¹æ¨¡å‹åŸæœ‰çš„å·¨å¤§æƒé‡çŸ©é˜µï¼Œè€Œæ˜¯åœ¨æ¨¡å‹çš„ç‰¹å®šå±‚ï¼ˆé€šå¸¸æ˜¯Transformerçš„æ³¨æ„åŠ›å±‚ï¼‰æ—è¾¹æ³¨å…¥ä¸¤ä¸ªè¾ƒå°çš„ã€å¯è®­ç»ƒçš„â€œä½ç§©â€çŸ©é˜µ (Aå’ŒB)ã€‚
  - ä¼˜ç‚¹ï¼š
    - å¤§å¹…å‡å°‘å¯è®­ç»ƒå‚æ•°ï¼šä½¿å¾—åœ¨æœ‰é™èµ„æºä¸‹å¾®è°ƒå¤§æ¨¡å‹æˆä¸ºå¯èƒ½ã€‚
    - æ›´å°çš„å­˜å‚¨éœ€æ±‚ï¼šå¾®è°ƒååªéœ€è¦ä¿å­˜å¾ˆå°çš„LoRAæƒé‡æ–‡ä»¶ï¼Œè€Œä¸æ˜¯æ•´ä¸ªæ¨¡å‹çš„å‰¯æœ¬ã€‚
    - å¿«é€Ÿåˆ‡æ¢ä»»åŠ¡ï¼šå¯ä»¥ä¸ºä¸åŒä»»åŠ¡è®­ç»ƒä¸åŒçš„LoRAé€‚é…å™¨ï¼ŒåŠ è½½ä¸åŒçš„é€‚é…å™¨å³å¯åˆ‡æ¢æ¨¡å‹è¡Œä¸ºã€‚
    - å‡å°‘ç¾éš¾æ€§é—å¿˜ï¼šç”±äºä¸æ”¹åŠ¨åŸå§‹æƒé‡ï¼Œæ¨¡å‹åœ¨é¢„è®­ç»ƒä»»åŠ¡ä¸Šçš„è¡¨ç°é€šå¸¸èƒ½å¾—åˆ°è¾ƒå¥½ä¿ç•™ã€‚
- QLoRA (Quantized LoRA) - é‡åŒ–ä½ç§©é€‚é…
  - åœ¨LoRAçš„åŸºç¡€ä¸Šï¼Œè¿›ä¸€æ­¥å¯¹ä½ç§©çŸ©é˜µè¿›è¡Œé‡åŒ–ï¼Œä»¥å‡å°‘æ¨¡å‹çš„å†…å­˜å ç”¨å’Œè®¡ç®—å¼€é”€ã€‚
- DPO (Direct Preference Optimization) - ç›´æ¥åå¥½ä¼˜åŒ–
  - RLHF æŠ€æœ¯
  - åˆ©ç”¨ ğŸ‘ğŸ‘ åé¦ˆæ¥ä¼˜åŒ–æ¨¡å‹è¾“å‡ºã€‚
  - ä¼˜ç‚¹
    - å®ç°ç®€å•
    - è®­ç»ƒç¨³å®š
    - æ•ˆæœæœ‰ç«äº‰åŠ›
- GRPO (Group Relative Policy Optimization) - ç»„ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–
  - åŸºäºåå¥½å¯¹é½çš„ç­–ç•¥ä¼˜åŒ–æ–¹æ³•
  - åœ¨ DPO çš„åŸºç¡€ä¸Šï¼Œè¿›ä¸€æ­¥å¼•å…¥äº†ç»„åå¥½ä¿¡æ¯ã€‚
- DeepSpeed ZeRO - Zero Redundancy Optimizer
  - æ˜¾å­˜ä¼˜åŒ–,è®­ç»ƒè¿œè¶…å•ä¸ªGPUæ˜¾å­˜å®¹é‡çš„å·¨å¤§æ¨¡å‹
  - stage
    - å¯¹ä¼˜åŒ–å™¨çŠ¶æ€è¿›è¡Œåˆ‡åˆ†ï¼ˆä¾‹å¦‚Adamä¼˜åŒ–å™¨çš„momentumå’Œvarianceï¼‰
    - å¯¹æ¢¯åº¦ä¹Ÿè¿›è¡Œåˆ‡åˆ†
    - å°†æ¨¡å‹å‚æ•°æœ¬èº«ä¹Ÿè¿›è¡Œåˆ‡åˆ†ã€‚æ¯ä¸ªGPUåªä¿ç•™å½“å‰è®¡ç®—å±‚æ‰€éœ€çš„å‚æ•°ï¼Œå…¶ä»–å‚æ•°åœ¨ä½¿ç”¨æ—¶åŠ¨æ€èšåˆã€‚
  - ZeRO-Offload
    - å°†éƒ¨åˆ†æˆ–å…¨éƒ¨è¢«åˆ‡åˆ†çš„çŠ¶æ€ï¼ˆå‚æ•°ã€æ¢¯åº¦ã€ä¼˜åŒ–å™¨çŠ¶æ€ï¼‰è¿›ä¸€æ­¥å¸è½½åˆ°CPUå†…å­˜ä¸­ï¼Œè¿›ä¸€æ­¥é™ä½GPUæ˜¾å­˜éœ€æ±‚ã€‚

## 2U1/Qwen2-VL-Finetune

- [2U1/Qwen2-VL-Finetune](https://github.com/2U1/Qwen2-VL-Finetune)
  - Apache-2.0, Python, Liger-Kernel, DeepSpeed
  - âš ï¸ Liger-kernel ä¸æ”¯æŒ QLoRA
- dataset
  - --data_path data.json
  - --image_folder

```json
[
  {
    "id": "000000033471",
    "image": ["000000033471.jpg", "000000033472.jpg"],
    "conversations": [
      {
        "from": "human",
        "value": "<image>\n<image>\nIs the perspective of the camera differnt?"
      },
      {
        "from": "gpt",
        "value": "Yes, It the perspective of the camera is different."
      }
    ]
  }
]
```

```bash
git clone https://github.com/2U1/Qwen2-VL-Finetune
cd Qwen2-VL-Finetune
uv venv --python 3.11
uv pip install -r requirements.txt -f https://download.pytorch.org/whl/cu124
uv pip install qwen-vl-utils
uv pip install flash-attn --no-build-isolation

# å¤åˆ¶è„šæœ¬ è‡ªè¡Œä¿®æ”¹åä½¿ç”¨
# ä¿®æ”¹ data-path, image-folder, MODEL_NAME
cp scripts/finetune.sh ft.sh

# å‚è€ƒè„šæœ¬
# Full Finetuning
bash scripts/finetune.sh
# LoRA Finetuning
bash scripts/finetune_lora.sh
# LoRA Finetuning language model & vision model
bash scripts/finetune_lora_vision.sh

# for Video
bash scripts/finetune_video.sh

bash scripts/merge_lora.sh
```

- https://github.com/2U1/Qwen2-VL-Finetune
